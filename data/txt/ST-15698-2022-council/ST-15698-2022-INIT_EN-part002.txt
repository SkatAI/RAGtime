Article 4b 
Requirements for general purpose AI systems and obligations for providers of such  systems 
1. General purpose AI systems which may be used as high risk AI systems or as  components of high risk AI systems in the meaning of Article 6, shall comply with the  requirements established in Title III, Chapter 2 of this Regulation as from the date of  application of the implementing acts adopted by the Commission in accordance with  the examination procedure referred to in Article 74(2) no later than 18 months after the  entry into force of this Regulation. Those implementing acts shall specify and adapt the  application of the requirements established in Title III, Chapter 2 to general purpose AI  systems in the light of their characteristics, technical feasibility, specificities of the AI  value chain and of market and technological developments. When fulfilling those  requirements, the generally acknowledged state of the art shall be taken into account. 
2. Providers of general purpose AI systems referred to in paragraph 1 shall comply, as  from the date of application of the implementing acts referred to in paragraph 1, with  the obligations set out in Articles 16aa, 16e, 16f, 16g, 16i, 16j, 25, 48 and 61. 
3. For the purpose of complying with the obligations set out in Article 16e, providers  shall follow the conformity assessment procedure based on internal control set out in  Annex VI, points 3 and 4. 
4. Providers of such systems shall also keep the technical documentation referred to in  Article 11 at the disposal of the national competent authorities for a period ending ten  years after the general purpose AI system is placed on the Union market or put into  service in the Union.
15698/22 RB/ek 71 TREE.2.B EN 
5. Providers of general purpose AI systems shall cooperate with and provide the  necessary information to other providers intending to put into service or place such  systems on the Union market as high-risk AI systems or as components of high-risk AI  systems, with a view to enabling the latter to comply with their obligations under this  Regulation. Such cooperation between providers shall preserve, as appropriate,  intellectual property rights, and confidential business information or trade secrets in  accordance with Article 70. In order to ensure uniform conditions for the  implementation of this Regulation as regards the information to be shared by the  providers of general purpose AI systems, the Commission may adopt implementing  acts in accordance with the examination procedure referred to in Article 74(2). 
6. In complying with the requirements and obligations referred to in paragraphs 1, 2 and  3: 
- any reference to the intended purpose shall be understood as referring to possible use  of the general purpose AI systems as high risk AI systems or as components of AI high  risk systems in the meaning of Article 6; 
- any reference to the requirements for high-risk AI systems in Chapter II, Title III shall  be understood as referring only to the requirements set out in the present Article. 
Article 4c 
Exceptions to Article 4b 
1. Article 4b shall not apply when the provider has explicitly excluded all high-risk uses  in the instructions of use or information accompanying the general purpose AI system. 
2. Such exclusion shall be made in good faith and shall not be deemed justified if the  provider has sufficient reasons to consider that the system may be misused. 
3. When the provider detects or is informed about market misuse they shall take all  necessary and proportionate measures to prevent such further misuse, in particular  taking into account the scale of the misuse and the seriousness of the associated risks. 
15698/22 RB/ek 72 TREE.2.B EN 
TITLE II 
PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES 
Article 5 
1. The following artificial intelligence practices shall be prohibited: 
(a) the placing on the market, putting into service or use of an AI system that deploys  subliminal techniques beyond a person’s consciousness with the objective to or the  effect of materially distorting a person’s behaviour in a manner that causes or is  reasonably likely to cause that person or another person physical or psychological  harm; 
(b) the placing on the market, putting into service or use of an AI system that exploits  any of the vulnerabilities of a specific group of persons due to their age, disability or  a specific social or economic situation, with the objective to or the effect of  materially distorting the behaviour of a person pertaining to that group in a manner  that causes or is reasonably likely to cause that person or another person physical or  psychological harm; 
(c) the placing on the market, putting into service or use of AI systems for the evaluation  or classification of natural persons over a certain period of time based on their social  behaviour or known or predicted personal or personality characteristics, with the  social score leading to either or both of the following: 
(i) detrimental or unfavourable treatment of certain natural persons or groups  thereof in social contexts which are unrelated to the contexts in which the data  was originally generated or collected; 
15698/22 RB/ek 73 TREE.2.B EN 
(ii) detrimental or unfavourable treatment of certain natural persons or groups  thereof that is unjustified or disproportionate to their social behaviour or its  gravity; 
(d) the use of ‘real-time’ remote biometric identification systems in publicly accessible  spaces by law enforcement authorities or on their behalf for the purpose of law  enforcement, unless and in as far as such use is strictly necessary for one of the  following objectives: 
(i) the targeted search for specific potential victims of crime;  
(ii) the prevention of a specific and substantial threat to the critical infrastructure,  life, health or physical safety of natural persons or the prevention of terrorist  attacks;  
(iii) the localisation or identification of a natural person for the purposes of  conducting a criminal investigation, prosecution or executing a criminal  
penalty for offences, referred to in Article 2(2) of Council Framework Decision  2002/584/JHA32 and punishable in the Member State concerned by a custodial  sentence or a detention order for a maximum period of at least three years, or  other specific offences punishable in the Member State concerned by a  
custodial sentence or a detention order for a maximum period of at least five  years, as determined by the law of that Member State. 
2. The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces  for the purpose of law enforcement for any of the objectives referred to in paragraph 1  point d) shall take into account the following elements: 
(a) the nature of the situation giving rise to the possible use, in particular the seriousness,  probability and scale of the harm caused in the absence of the use of the system;  
  
32 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender  procedures between Member States (OJ L 190, 18.7.2002, p. 1).
15698/22 RB/ek 74 TREE.2.B EN 
(b) the consequences of the use of the system for the rights and freedoms of all persons  concerned, in particular the seriousness, probability and scale of those consequences. 
In addition, the use of ‘real-time’ remote biometric identification systems in publicly  accessible spaces for the purpose of law enforcement for any of the objectives referred to  in paragraph 1 point d) shall comply with necessary and proportionate safeguards and  conditions in relation to the use, in particular as regards the temporal, geographic and  personal limitations. 
3. As regards paragraphs 1, point (d) and 2, each use for the purpose of law enforcement of a  ‘real-time’ remote biometric identification system in publicly accessible spaces shall be  subject to a prior authorisation granted by a judicial authority or by an independent  administrative authority of the Member State in which the use is to take place, issued upon  a reasoned request and in accordance with the detailed rules of national law referred to in  paragraph 4. However, in a duly justified situation of urgency, the use of the system may  be commenced without an authorisation provided that, such authorisation shall be  requested without undue delay during use of the AI system, and if such authorisation is  rejected, its use shall be stopped with immediate effect.  
The competent judicial or administrative authority shall only grant the authorisation where  it is satisfied, based on objective evidence or clear indications presented to it, that the use  of the ‘real-time’ remote biometric identification system at issue is necessary for and  proportionate to achieving one of the objectives specified in paragraph 1, point (d), as  identified in the request. In deciding on the request, the competent judicial or  administrative authority shall take into account the elements referred to in paragraph 2. 
15698/22 RB/ek 75 TREE.2.B EN 
4. A Member State may decide to provide for the possibility to fully or partially authorise the  use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for  the purpose of law enforcement within the limits and under the conditions listed in  paragraphs 1, point (d), 2 and 3. That Member State shall lay down in its national law the  necessary detailed rules for the request, issuance and exercise of, as well as supervision  and reporting relating to, the authorisations referred to in paragraph 3. Those rules shall  also specify in respect of which of the objectives listed in paragraph 1, point (d), including  which of the criminal offences referred to in point (iii) thereof, the competent authorities  may be authorised to use those systems for the purpose of law enforcement. 
TITLE III 
HIGH-RISK AI SYSTEMS 
CHAPTER 1 
CLASSIFICATION OF AI SYSTEMS AS HIGH-RISK 
Article 6 
Classification rules for high-risk AI systems 
1. An AI system that is itself a product covered by the Union harmonisation legislation listed  in Annex II shall be considered as high risk if it is required to undergo a third-party  conformity assessment with a view to the placing on the market or putting into service of  that product pursuant to the above mentioned legislation.
15698/22 RB/ek 76 TREE.2.B EN 
2. An AI system intended to be used as a safety component of a product covered by the  legislation referred to in paragraph 1 shall be considered as high risk if it is required to  undergo a third-party conformity assessment with a view to the placing on the market or  putting into service of that product pursuant to above mentioned legislation. This provision  shall apply irrespective of whether the AI system is placed on the market or put into  service independently from the product.  
3. AI systems referred to in Annex III shall be considered high-risk unless the output of the  system is purely accessory in respect of the relevant action or decision to be taken and is  not therefore likely to lead to a significant risk to the health, safety or fundamental rights. 
In order to ensure uniform conditions for the implementation of this Regulation, the  Commission shall, no later than one year after the entry into force of this Regulation, adopt  implementing acts to specify the circumstances where the output of AI systems referred to  in Annex III would be purely accessory in respect of the relevant action or decision to be  taken. Those implementing acts shall be adopted in accordance with the examination  procedure referred to in Article 74, paragraph 2. 
Article 7 
Amendments to Annex III 
1. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend the list in Annex III by adding high-risk AI systems where both of the following  conditions are fulfilled: 
(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 of  Annex III; 
(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse  impact on fundamental rights, that is, in respect of its severity and probability of  occurrence, equivalent to or greater than the risk of harm or of adverse impact posed  by the high-risk AI systems already referred to in Annex III.
15698/22 RB/ek 77 TREE.2.B EN 
2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm  to the health and safety or a risk of adverse impact on fundamental rights that is equivalent  to or greater than the risk of harm posed by the high-risk AI systems already referred to in  Annex III, the Commission shall take into account the following criteria: 
(a) the intended purpose of the AI system; 
(b) the extent to which an AI system has been used or is likely to be used; 
(c) the extent to which the use of an AI system has already caused harm to the health and  safety or adverse impact on the fundamental rights or has given rise to significant  concerns in relation to the materialisation of such harm or adverse impact, as  demonstrated by reports or documented allegations submitted to national competent  authorities; 
(d) the potential extent of such harm or such adverse impact, in particular in terms of its  intensity and its ability to affect a plurality of persons; 
(e) the extent to which potentially harmed or adversely impacted persons are dependent  on the outcome produced with an AI system, in particular because for practical or  legal reasons it is not reasonably possible to opt-out from that outcome; 
(f) the extent to which potentially harmed or adversely impacted persons are in a  vulnerable position in relation to the user of an AI system, in particular due to an  imbalance of power, knowledge, economic or social circumstances, or age; 
(g) the extent to which the outcome produced with an AI system is not easily reversible,  whereby outcomes having an impact on the health or safety of persons shall not be  considered as easily reversible;
15698/22 RB/ek 78 TREE.2.B EN 
(h) the extent to which existing Union legislation provides for: 
(i) effective measures of redress in relation to the risks posed by an AI system,  with the exclusion of claims for damages; 
(ii) effective measures to prevent or substantially minimise those risks; 
(i) the magnitude and likelihood of benefit of the AI use for individuals, groups, or  society at large. 
3. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend the list in Annex III by removing high-risk AI systems where both of the following  conditions are fulfilled: 
(a) the high-risk AI system(s) concerned no longer pose any significant risks to  fundamental rights, health or safety, taking into account the criteria listed in  paragraph 2;  
(b) the deletion does not decrease the overall level of protection of health, safety and  fundamental rights under Union law. 
CHAPTER 2 
REQUIREMENTS FOR HIGH-RISK AI SYSTEMS 
Article 8 
Compliance with the requirements 
1. High-risk AI systems shall comply with the requirements established in this Chapter,  taking into account the generally acknowledged state of the art.
15698/22 RB/ek 79 TREE.2.B EN 
2. The intended purpose of the high-risk AI system and the risk management system referred  to in Article 9 shall be taken into account when ensuring compliance with those  requirements. 
Article 9 
Risk management system 
1. A risk management system shall be established, implemented, documented and maintained  in relation to high-risk AI systems. 
2. The risk management system shall be understood as a continuous iterative process planned  and run throughout the entire lifecycle of a high-risk AI system, requiring regular  systematic updating. It shall comprise the following steps: 
(a) identification and analysis of the known and foreseeable risks most likely to occur to  health, safety and fundamental rights in view of the intended purpose of the high-risk  AI system; 
(b) [deleted];  
(c) evaluation of other possibly arising risks based on the analysis of data gathered from  the post-market monitoring system referred to in Article 61; 
(d) adoption of suitable risk management measures in accordance with the provisions of  the following paragraphs. 
The risks referred to in this paragraph shall concern only those which may be reasonably  mitigated or eliminated through the development or design of the high-risk AI system, or  the provision of adequate technical information.
15698/22 RB/ek 80 TREE.2.B EN 
3. The risk management measures referred to in paragraph 2, point (d) shall give due  consideration to the effects and possible interaction resulting from the combined  application of the requirements set out in this Chapter 2, with a view to minimising risks  more effectively while achieving an appropriate balance in implementing the measures to  fulfil those requirements.  
4. The risk management measures referred to in paragraph 2, point (d) shall be such that any  residual risk associated with each hazard as well as the overall residual risk of the high-risk  AI systems is judged acceptable. 
In identifying the most appropriate risk management measures, the following shall be  ensured: 
(a) elimination or reduction of risks identified and evaluated pursuant to paragraph 2 as  far as possible through adequate design and development of the high risk AI system; 
(b) where appropriate, implementation of adequate mitigation and control measures in  relation to risks that cannot be eliminated; 
(c) provision of adequate information pursuant to Article 13, in particular as regards the  risks referred to in paragraph 2, point (b) of this Article, and, where appropriate,  training to users. 
With a view to eliminating or reducing risks related to the use of the high-risk AI system,  due consideration shall be given to the technical knowledge, experience, education,  training to be expected by the user and the environment in which the system is intended to  be used. 
5. High-risk AI systems shall be tested in order to ensure that high-risk AI systems perform in  a manner that is consistent with their intended purpose and they are in compliance with the  requirements set out in this Chapter. 
6. Testing procedures may include testing in real world conditions in accordance with Article  54a.
15698/22 RB/ek 81 TREE.2.B EN 
7. The testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the  market or the putting into service. Testing shall be made against preliminarily defined  metrics and probabilistic thresholds that are appropriate to the intended purpose of the  high-risk AI system. 
8. The risk management system described in paragraphs 1 to 7 shall give specific  consideration to whether the high-risk AI system is likely to be accessed by or have an  impact on persons under the age of 18. 
9. For providers of high-risk AI systems that are subject to requirements regarding internal  risk management processes under relevant sectorial Union law, the aspects described in  paragraphs 1 to 8 may be part of the risk management procedures established pursuant to  that law. 
Article 10 
Data and data governance 
1. High-risk AI systems which make use of techniques involving the training of models with  data shall be developed on the basis of training, validation and testing data sets that meet  the quality criteria referred to in paragraphs 2 to 5. 
2. Training, validation and testing data sets shall be subject to appropriate data governance  and management practices. Those practices shall concern in particular,  
(a) the relevant design choices; 
(b) data collection processes; 
(c) relevant data preparation processing operations, such as annotation, labelling,  cleaning, enrichment and aggregation;
15698/22 RB/ek 82 TREE.2.B EN 
(d) the formulation of relevant assumptions, notably with respect to the information that  the data are supposed to measure and represent;  
(e) a prior assessment of the availability, quantity and suitability of the data sets that are  needed;  
(f) examination in view of possible biases that are likely to affect health and safety of  natural persons or lead to discrimination prohibited by Union law;  
(g) the identification of any possible data gaps or shortcomings, and how those gaps and  shortcomings can be addressed. 
3. Training, validation and testing data sets shall be relevant, representative, and to the best  extent possible, free of errors and complete. They shall have the appropriate statistical  properties, including, where applicable, as regards the persons or groups of persons on  which the high-risk AI system is intended to be used. These characteristics of the data sets  may be met at the level of individual data sets or a combination thereof. 
4. Training, validation and testing data sets shall take into account, to the extent required by  the intended purpose, the characteristics or elements that are particular to the specific  geographical, behavioural or functional setting within which the high-risk AI system is  intended to be used.  
5. To the extent that it is strictly necessary for the purposes of ensuring bias monitoring,  detection and correction in relation to the high-risk AI systems, the providers of such  systems may process special categories of personal data referred to in Article 9(1) of  Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of  Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights  and freedoms of natural persons, including technical limitations on the re-use and use of  state-of-the-art security and privacy-preserving measures, such as pseudonymisation, or  encryption where anonymisation may significantly affect the purpose pursued.
15698/22 RB/ek 83 TREE.2.B EN 
6. For the development of high-risk AI systems not using techniques involving the training of  models, paragraphs 2 to 5 shall apply only to the testing data sets.  
Article 11 
Technical documentation  
1. The technical documentation of a high-risk AI system shall be drawn up before that system  is placed on the market or put into service and shall be kept up-to date. 
The technical documentation shall be drawn up in such a way to demonstrate that the high risk AI system complies with the requirements set out in this Chapter and provide national  competent authorities and notified bodies with all the necessary information in a clear and  comprehensive form to assess the compliance of the AI system with those requirements. It  shall contain, at a minimum, the elements set out in Annex IV or, in the case of SMEs,  
including start-ups, any equivalent documentation meeting the same objectives, unless  deemed inappropriate by the competent authority. 
2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II,  section A apply, is placed on the market or put into service one single technical  documentation shall be drawn up containing all the information set out in Annex IV as  well as the information required under those legal acts. 
3. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend Annex IV where necessary to ensure that, in the light of technical progress, the  technical documentation provides all the necessary information to assess the compliance of  the system with the requirements set out in this Chapter.
15698/22 RB/ek 84 TREE.2.B EN 
Article 12 
Record-keeping 
1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’)  over the duration of the life cycle of the system. 
2. In order to ensure a level of traceability of the AI system’s functioning that is appropriate  to the intended purpose of the system, logging capabilities shall enable the recording of  events relevant for  
(i) identification of situations that may result in the AI system presenting a risk within the  meaning of Article 65(1) or in a substantial modification; 
(ii) facilitation of the post-market monitoring referred to in Article 61; and 
(iii) monitoring of the operation of high-risk AI systems referred to in Article 29(4). 
4. For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging  capabilities shall provide, at a minimum:  
(a) recording of the period of each use of the system (start date and time and end date  and time of each use);  
(b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match;  
(d) the identification of the natural persons involved in the verification of the results, as  referred to in Article 14 (5).
15698/22 RB/ek 85 TREE.2.B EN 
Article 13 
Transparency and provision of information to users 
1. High-risk AI systems shall be designed and developed in such a way to ensure that their  operation is sufficiently transparent with a view to achieving compliance with the relevant  obligations of the user and of the provider set out in Chapter 3 of this Title and enabling  users to understand and use the system appropriately. 
2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital  format or otherwise that include concise, complete, correct and clear information that is  relevant, accessible and comprehensible to users. 
3. The information referred to in paragraph 2 shall specify: 
(a) the identity and the contact details of the provider and, where applicable, of its  authorised representative; 
(b) the characteristics, capabilities and limitations of performance of the high-risk AI  system, including: 
(i) its intended purpose, inclusive of the specific geographical, behavioural or  functional setting within which the high-risk AI system is intended to be used; 
(ii) the level of accuracy, including its metrics, robustness and cybersecurity  referred to in Article 15 against which the high-risk AI system has been tested  and validated and which can be expected, and any known and foreseeable  
circumstances that may have an impact on that expected level of accuracy,  
robustness and cybersecurity; 
(iii) any known or foreseeable circumstance, related to the use of the high-risk AI  system in accordance with its intended purpose, which may lead to risks to the  health and safety or fundamental rights referred to in Aricle 9(2);
15698/22 RB/ek 86 TREE.2.B EN 
(iv) when appropriate, its behaviour regarding specific persons or groups of persons  on which the system is intended to be used; 
(v) when appropriate, specifications for the input data, or any other relevant  information in terms of the training, validation and testing data sets used,  
taking into account the intended purpose of the AI system; 
(vi) when appropriate, description of the expected output of the system. 
(c) the changes to the high-risk AI system and its performance which have been pre determined by the provider at the moment of the initial conformity assessment, if  any;  
(d) the human oversight measures referred to in Article 14, including the technical  measures put in place to facilitate the interpretation of the outputs of AI systems by  the users; 
(e) the computational and hardware resources needed, the expected lifetime of the high risk AI system and any necessary maintenance and care measures, including their  frequency, to ensure the proper functioning of that AI system, including as regards  software updates; 
(f) a description of the mechanism included within the AI system that allows users to  properly collect, store and interpret the logs, where relevant. 
Article 14 
Human oversight 
1. High-risk AI systems shall be designed and developed in such a way, including with  appropriate human-machine interface tools, that they can be effectively overseen by natural  persons during the period in which the AI system is in use. 
15698/22 RB/ek 87 TREE.2.B EN 
2. Human oversight shall aim at preventing or minimising the risks to health, safety or  fundamental rights that may emerge when a high-risk AI system is used in accordance with  its intended purpose or under conditions of reasonably foreseeable misuse, in particular  when such risks persist notwithstanding the application of other requirements set out in this  Chapter. 
3. Human oversight shall be ensured through either one or all of the following types of  measures: 
(a) measures identified and built, when technically feasible, into the high-risk AI system  by the provider before it is placed on the market or put into service;  
(b) measures identified by the provider before placing the high-risk AI system on the  market or putting it into service and that are appropriate to be implemented by the  user. 
4. For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be  provided to the user in such a way that natural persons to whom human oversight is  assigned are enabled, as appropriate and proportionate to the circumstances: 
(a) to understand the capacities and limitations of the high-risk AI system and be able to  duly monitor its operation; 
(b) to remain aware of the possible tendency of automatically relying or over-relying on  the output produced by a high-risk AI system (‘automation bias’); 
(c) to correctly interpret the high-risk AI system’s output, taking into account for  example the interpretation tools and methods available; 
(d) to decide, in any particular situation, not to use the high-risk AI system or otherwise  disregard, override or reverse the output of the high-risk AI system; 
(e) to intervene on the operation of the high-risk AI system or interrupt the system  through a “stop” button or a similar procedure.
15698/22 RB/ek 88 TREE.2.B EN 
5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in  paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the  user on the basis of the identification resulting from the system unless this has been  separately verified and confirmed by at least two natural persons. The requirement for a  separate verification by at least two natural persons shall not apply to high risk AI systems  used for the purpose of law enforcement, migration, border control or asylum, in cases  where Union or national law considers the application of this requirement to be  disproportionate.  
Article 15 
Accuracy, robustness and cybersecurity 
1. High-risk AI systems shall be designed and developed in such a way that they achieve, in  the light of their intended purpose, an appropriate level of accuracy, robustness and  cybersecurity, and perform consistently in those respects throughout their lifecycle. 
2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be  declared in the accompanying instructions of use. 
3. High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may  occur within the system or the environment in which the system operates, in particular due  to their interaction with natural persons or other systems. 
The robustness of high-risk AI systems may be achieved through technical redundancy  solutions, which may include backup or fail-safe plans. 
High-risk AI systems that continue to learn after being placed on the market or put into  service shall be developed in such a way to eliminate or reduce as far as possible the risk of  possibly biased outputs influencing input for future operations (‘feedback loops’) are duly  addressed with appropriate mitigation measures.
15698/22 RB/ek 89 TREE.2.B EN 
4. High-risk AI systems shall be resilient as regards attempts by unauthorised third parties to  alter their use or performance by exploiting the system vulnerabilities. 
The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall  be appropriate to the relevant circumstances and the risks. 
The technical solutions to address AI specific vulnerabilities shall include, where  appropriate, measures to prevent and control for attacks trying to manipulate the training  dataset (‘data poisoning’), inputs designed to cause the model to make a mistake  (‘adversarial examples’), or model flaws. 
CHAPTER 3 
OBLIGATIONS OF PROVIDERS AND USERS OF HIGH-RISK AI SYSTEMS AND  OTHER PARTIES 
Article 16 
Obligations of providers of high-risk AI systems  
Providers of high-risk AI systems shall: 
(a) ensure that their high-risk AI systems are compliant with the requirements set out in  Chapter 2 of this Title; 
(aa) indicate their name, registered trade name or registered trade mark, the address at which  they can be contacted on the high-risk AI system or, where that is not possible, on its  packaging or its accompanying documentation, as applicable; 
(b) have a quality management system in place which complies with Article 17; (c) keep the documentation referred to in Article 18;
15698/22 RB/ek 90 TREE.2.B EN 
(d) when under their control, keep the logs automatically generated by their high-risk AI  systems as referred to in Article 20; 
(e) ensure that the high-risk AI system undergoes the relevant conformity assessment  procedure as referred to in Article 43, prior to its placing on the market or putting into  service; 
(f) comply with the registration obligations referred to in Article 51(1); 
(g) take the necessary corrective actions as referred to in Article 21, if the high-risk AI system  is not in conformity with the requirements set out in Chapter 2 of this Title; 
(h) inform the relevant national competent authority of the Member States in which they made  the AI system available or put it into service and, where applicable, the notified body of the  non-compliance and of any corrective actions taken; 
(i) to affix the CE marking to their high-risk AI systems to indicate the conformity with this  Regulation in accordance with Article 49; 
(j) upon request of a national competent authority, demonstrate the conformity of the high risk AI system with the requirements set out in Chapter 2 of this Title. 
Article 17 
Quality management system  
1. Providers of high-risk AI systems shall put a quality management system in place that  ensures compliance with this Regulation. That system shall be documented in a systematic  and orderly manner in the form of written policies, procedures and instructions, and shall  include at least the following aspects: 
(a) a strategy for regulatory compliance, including compliance with conformity  assessment procedures and procedures for the management of modifications to the  high-risk AI system;
15698/22 RB/ek 91 TREE.2.B EN 
(b) techniques, procedures and systematic actions to be used for the design, design  control and design verification of the high-risk AI system; 
(c) techniques, procedures and systematic actions to be used for the development,  quality control and quality assurance of the high-risk AI system; 
(d) examination, test and validation procedures to be carried out before, during and after  the development of the high-risk AI system, and the frequency with which they have  to be carried out; 
(e) technical specifications, including standards, to be applied and, where the relevant  harmonised standards are not applied in full, the means to be used to ensure that the  high-risk AI system complies with the requirements set out in Chapter 2 of this Title; 
(f) systems and procedures for data management, including data collection, data  analysis, data labelling, data storage, data filtration, data mining, data aggregation,  data retention and any other operation regarding the data that is performed before and  for the purposes of the placing on the market or putting into service of high-risk AI  systems; 
(g) the risk management system referred to in Article 9; 
(h) the setting-up, implementation and maintenance of a post-market monitoring system,  in accordance with Article 61; 
(i) procedures related to the reporting of a serious incident in accordance with Article  62; 
(j) the handling of communication with national competent authorities, competent  authorities, including sectoral ones, providing or supporting the access to data,  notified bodies, other operators, customers or other interested parties; 
(k) systems and procedures for record keeping of all relevant documentation and  information;
15698/22 RB/ek 92 TREE.2.B EN 
(l) resource management, including security of supply related measures; 
(m) an accountability framework setting out the responsibilities of the management and  other staff with regard to all aspects listed in this paragraph. 
2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the size  of the provider’s organisation.  
2a. For providers of high-risk AI systems that are subject to obligations regarding quality  management systems under relevant sectorial Union law, the aspects described in  paragraph 1 may be part of the quality management systems pursuant to that law. 
3. For providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation, the  obligation to put in place a quality management system with the exception of paragraph 1,  points (g), (h) and (i) shall be deemed to be fulfilled by complying with the rules on  internal governance arrangements or processes pursuant to the relevant Union financial  services legislation. In that context, any harmonised standards referred to in Article 40 of  this Regulation shall be taken into account. 
Article 18 
Documentation keeping 
1. The provider shall, for a period ending 10 years after the AI system has been placed on the  market or put into service, keep at the disposal of the national competent authorities: 
(a) the technical documentation referred to in Article 11;  
(b) the documentation concerning the quality management system referred to in  Article 17; 
(c) the documentation concerning the changes approved by notified bodies where  applicable; 
15698/22 RB/ek 93 TREE.2.B EN 
(d) the decisions and other documents issued by the notified bodies where applicable;  (e) the EU declaration of conformity referred to in Article 48. 
1a. Each Member State shall determine conditions under which the documentation referred to  in paragraph 1 remains at the disposal of the national competent authorities for the period  indicated in that paragraph for the cases when a provider or its authorised representative  established on its territory goes bankrupt or ceases its activity prior to the end of that  period. 
2. Providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the technical documentation as part of the documentation kept under the relevant  Union financial services legislation. 
Article 19 
Conformity assessment  
1. Providers of high-risk AI systems shall ensure that their systems undergo the relevant  conformity assessment procedure in accordance with Article 43, prior to their placing on  the market or putting into service. Where the compliance of the AI systems with the  requirements set out in Chapter 2 of this Title has been demonstrated following that  conformity assessment, the providers shall draw up an EU declaration of conformity in  accordance with Article 48 and affix the CE marking of conformity in accordance with  Article 49.  
2. [deleted]
15698/22 RB/ek 94 TREE.2.B EN 
Article 20 
Automatically generated logs 
1. Providers of high-risk AI systems shall keep the logs, referred to in Article 12(1),  automatically generated by their high-risk AI systems, to the extent such logs are under  their control by virtue of a contractual arrangement with the user or otherwise by law. They  shall keep them for a period of at least six months, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection of personal data. 
2. Providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the logs automatically generated by their high-risk AI systems as part of the  documentation kept under the relevant financial service legislation. 
Article 21 
Corrective actions 
Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI  system which they have placed on the market or put into service is not in conformity with this  Regulation shall immediately investigate, where applicable, the causes in collaboration with the  reporting user and take the necessary corrective actions to bring that system into conformity, to  withdraw it or to recall it, as appropriate. They shall inform the distributors of the high-risk AI  system in question and, where applicable, the authorised representative and importers accordingly.
15698/22 RB/ek 95 TREE.2.B EN 
Article 22 
Duty of information 
Where the high-risk AI system presents a risk within the meaning of Article 65(1) and that risk is  known to the provider of the system, that provider shall immediately inform the national competent  authorities of the Member States in which it made the system available and, where applicable, the  notified body that issued a certificate for the high-risk AI system, in particular of the non compliance and of any corrective actions taken. 
Article 23 
Cooperation with competent authorities 
Providers of high-risk AI systems shall, upon request by a national competent authority, provide  that authority with all the information and documentation necessary to demonstrate the conformity  of the high-risk AI system with the requirements set out in Chapter 2 of this Title, in a language  which can be easily understood by the authority of the Member State concerned. Upon a reasoned  request from a national competent authority, providers shall also give that authority access to the  logs, referred to in Article 12(1), automatically generated by the high-risk AI system, to the extent  such logs are under their control by virtue of a contractual arrangement with the user or otherwise  by law. 
Article 23a 
Conditions for other persons to be subject to the obligations of a provider  
1. Any natural or legal person shall be considered a provider of a new high-risk AI system for  the purposes of this Regulation and shall be subject to the obligations of the provider under  Article 16, in any of the following circumstances: 
(a) they put their name or trademark on a high-risk AI system already placed on the  market or put into service, without prejudice to contractual arrangements stipulating  that the obligations are allocated otherwise;
15698/22 RB/ek 96 TREE.2.B EN 
(b) [deleted] 
(c) they make a substantial modification to a high-risk AI system already placed on the  market or put into service; 
(d) they modify the intended purpose of an AI system which is not high-risk and is  already placed on the market or put ito service, in a way which makes the modified  system a high-risk AI system; 
(e) they place on the market or put into service a general purpose AI system as a high risk AI system or as a component of a high-risk AI system. 
2. Where the circumstances referred to in paragraph 1, point (a) or (c), occur, the provider that  initially placed the high-risk AI system on the market or put it into service shall no longer be  considered a provider for the purposes of this Regulation. 
3. For high-risk AI systems that are safety components of products to which the legal acts listed  in Annex II, section A apply, the manufacturer of those products shall be considered the  provider of the high-risk AI system and shall be subject to the obligations under Article 16  under either of the following scenarios: 
(i) the high-risk AI system is placed on the market together with the product under the  name or trademark of the product manufacturer;  
(ii) the high-risk AI system is put into service under the the name or trademark of the  product manufacturer after the product has been placed on the market. 
Article 24 
[deleted]
15698/22 RB/ek 97 TREE.2.B EN 
Article 25 
Authorised representatives 
1. Prior to making their systems available on the Union market providers established outside  the Union shall, by written mandate, appoint an authorised representative which is  established in the Union. 
2. The authorised representative shall perform the tasks specified in the mandate received  from the provider. For the purpose of this Regulation, the mandate shall empower the  authorised representative to carry out only the following tasks: 
(-a) verify that the EU declaration of conformity and the technical documentation have  been drawn up and that an appropriate conformity assessment procedure has  been carried out by the provider; 
(a) keep at the disposal of the national competent authorities and national authorities  referred to in Article 63(7), for a period ending 10 years after the high-risk AI system  has been placed on the market or put into service, the contact details of the provider  by which the authorised representative has been appointed, a copy of the EU  declaration of conformity, the technical documentation and, if applicable, the  certificate issued by the notified body; 
(b) provide a national competent authority, upon a reasoned request, with all the  information and documentation, including that kept according to point (b), necessary  to demonstrate the conformity of a high-risk AI system with the requirements set out  in Chapter 2 of this Title, including access to the logs, referred to in Article 12(1),  automatically generated by the high-risk AI system to the extent such logs are under  the control of the provider by virtue of a contractual arrangement with the user or  otherwise by law; 
(c) cooperate with national competent authorities, upon a reasoned request, on any  action the latter takes in relation to the high-risk AI system.
15698/22 RB/ek 98 TREE.2.B EN 
(d) comply with the registration obligations referred to in Article 51(1) and, if the  registration of the system is carried out by the provider itself, verify that the  information referred to in Annex VIII, Part II, 1 to 11, is correct. 
The authorised representative shall terminate the mandate if it has sufficient reasons to consider that  the provider acts contrary to its obligations under this Regulation. In such a case, it shall also  immediately inform the market surveillance authority of the Member State in which it is  established, as well as, where applicable, the relevant notified body, about the termination of the  mandate and the reasons thereof. 
The authorised representative shall be legally liable for defective AI systems on the same basis as,  and jointly and severally with, the provider in respect of its potential liability under Council  Directive 85/374/EEC. 
Article 26 
Obligations of importers 
1. Before placing a high-risk AI system on the market, importers of such system shall ensure  that such a system is in conformity with this Regulation by verifying that: 
(a) the relevant conformity assessment procedure referred to in Article 43 has been  carried out by the provider of that AI system;  
(b) the provider has drawn up the technical documentation in accordance with Annex  IV;  
(c) the system bears the required CE conformity marking and is accompanied by the EU  declaration of conformity and instructions of use;  
(d) the authorised representative referred to in Article 25 has been established by the  provider.
15698/22 RB/ek 99 TREE.2.B EN 
2. Where an importer has sufficient reasons to consider that a high-risk AI system is not in  conformity with this Regulation, or is falsified, or accompanied by falsified  documentation, it shall not place that system on the market until that AI system has been  brought into conformity. Where the high-risk AI system presents a risk within the meaning  of Article 65(1), the importer shall inform the provider of the AI system, the authorised  representatives and the market surveillance authorities to that effect. 
3. Importers shall indicate their name, registered trade name or registered trade mark, and the  address at which they can be contacted on the high-risk AI system or, where that is not  possible, on its packaging or its accompanying documentation, as applicable. 
4. Importers shall ensure that, while a high-risk AI system is under their responsibility, where  applicable, storage or transport conditions do not jeopardise its compliance with the  requirements set out in Chapter 2 of this Title. 
4a. Importers shall keep, for a period ending 10 years after the AI system has been placed on  the market or put into service, a copy of the certificate issued by the notified body, where  applicable, of the instructions for use and of the EU declaration of conformity. 
5. Importers shall provide national competent authorities, upon a reasoned request, with all  necessary information and documentation, including that kept in accordance with  paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements  set out in Chapter 2 of this Title in a language which can be easily understood by that  national competent authority. To this purpose they shall also ensure that the technical  documentation can be made available to those authorities.  
5a. Importers shall cooperate with national competent authorities on any action those  authorities take in relation to an AI system, of which they are the importer.
15698/22 RB/ek 100 TREE.2.B EN 
Article 27 
Obligations of distributors 
1. Before making a high-risk AI system available on the market, distributors shall verify that  the high-risk AI system bears the required CE conformity marking, that it is accompanied  by a copy of EU declaration of conformity and instruction of use, and that the provider and  the importer of the system, as applicable, have complied with their obligations set out  Article 16, point (b) and 26(3) respectively. 
2. Where a distributor considers or has reason to consider that a high-risk AI system is not in  conformity with the requirements set out in Chapter 2 of this Title, it shall not make the  high-risk AI system available on the market until that system has been brought into  conformity with those requirements. Furthermore, where the system presents a risk within  the meaning of Article 65(1), the distributor shall inform the provider or the importer of the  system, as applicable, to that effect. 
3. Distributors shall ensure that, while a high-risk AI system is under their responsibility,  where applicable, storage or transport conditions do not jeopardise the compliance of the  system with the requirements set out in Chapter 2 of this Title. 
4. A distributor that considers or has reason to consider that a high-risk AI system which it  has made available on the market is not in conformity with the requirements set out in  Chapter 2 of this Title shall take the corrective actions necessary to bring that system into  conformity with those requirements, to withdraw it or recall it or shall ensure that the  provider, the importer or any relevant operator, as appropriate, takes those corrective  actions. Where the high-risk AI system presents a risk within the meaning of Article 65(1),  the distributor shall immediately inform the national competent authorities of the Member  States in which it has made the product available to that effect, giving details, in particular,  of the non-compliance and of any corrective actions taken.
15698/22 RB/ek 101 TREE.2.B EN 
5. Upon a reasoned request from a national competent authority, distributors of high-risk AI  systems shall provide that authority with all the information and documentation regarding  its activities as described in paragraph 1 to 4. 
5a. Distributors shall cooperate with national competent authorities on any action those  authorities take in relation to an AI system, of which they are the distributor. 
Article 28 
[deleted] 
Article 29 
Obligations of users of high-risk AI systems 
1. Users of high-risk AI systems shall use such systems in accordance with the instructions of  use accompanying the systems, pursuant to paragraphs 2 and 5 of this Article.  
1a. Users shall assign human oversight to natural persons who have the necessary competence,  training and authority. 
2. The obligations in paragraph 1 and 1a are without prejudice to other user obligations under  Union or national law and to the user’s discretion in organising its own resources and  activities for the purpose of implementing the human oversight measures indicated by the  provider. 
3. Without prejudice to paragraph 1, to the extent the user exercises control over the input  data, that user shall ensure that input data is relevant in view of the intended purpose of the  high-risk AI system. 
15698/22 RB/ek 102 TREE.2.B EN 
4. Users shall implement human oversight and monitor the operation of the high-risk AI  system on the basis of the instructions of use. When they have reasons to consider that the  use in accordance with the instructions of use may result in the AI system presenting a risk  within the meaning of Article 65(1) they shall inform the provider or distributor and  suspend the use of the system. They shall also inform the provider or distributor when they  have identified any serious incident and interrupt the use of the AI system. In case the user  is not able to reach the provider, Article 62 shall apply mutatis mutandis. This obligation  shall not cover sensitive operational data of users of AI systems which are law enforcement  authorities. 
For users that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation, the  monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by  complying with the rules on internal governance arrangements, processes and mechanisms  pursuant to the relevant financial service legislation. 
5. Users of high-risk AI systems shall keep the logs, referred to in Article 12(1),  automatically generated by that high-risk AI system, to the extent such logs are under their  control. They shall keep them for a period of at least six months, unless provided otherwise  in applicable Union or national law, in particular in Union law on the protection of  personal data. 
Users that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the logs as part of the documentation kept pursuant to the relevant Union  financial service legislation. 
5a. Users of high-risk AI systems that are public authorities, agencies or bodies, with the  exception of law enforcement, border control, immigration or asylum authorities, shall  comply with the registration obligations referred to in Article 51. When they find that the  system that they envisage to use has not been registered in the EU database referred to in  Article 60 they shall not use that system and shall inform the provider or the distributor.
15698/22 RB/ek 103 TREE.2.B EN 
6. Users of high-risk AI systems shall use the information provided under Article 13 to  comply with their obligation to carry out a data protection impact assessment under Article  35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, where  applicable. 
6a. Users shall cooperate with national competent authorities on any action those authorities  take in relation to an AI system, of which they are the user. 
CHAPTER 4 
NOTIFIYING AUTHORITIES AND NOTIFIED BODIES 
Article 30 
Notifying authorities 
1. Each Member State shall designate or establish at least one notifying authority responsible  for setting up and carrying out the necessary procedures for the assessment, designation  and notification of conformity assessment bodies and for their monitoring.  
2. Member States may decide that the assessment and monitoring referred to in paragraph 1  shall be carried out by a national accreditation body within the meaning of and in  accordance with Regulation (EC) No 765/2008. 
3. Notifying authorities shall be established, organised and operated in such a way that no  conflict of interest arises with conformity assessment bodies and the objectivity and  impartiality of their activities are safeguarded.
15698/22 RB/ek 104 TREE.2.B EN 
4. Notifying authorities shall be organised in such a way that decisions relating to the  notification of conformity assessment bodies are taken by competent persons different  from those who carried out the assessment of those bodies. 
5. Notifying authorities shall not offer or provide any activities that conformity assessment  bodies perform or any consultancy services on a commercial or competitive basis. 
6. Notifying authorities shall safeguard the confidentiality of the information they obtain in  accordance with Article 70. 
7. Notifying authorities shall have an adequate number of competent personnel at their  disposal for the proper performance of their tasks. 
8. [deleted] 
Article 31 
Application of a conformity assessment body for notification  
1. Conformity assessment bodies shall submit an application for notification to the notifying  authority of the Member State in which they are established. 
2. The application for notification shall be accompanied by a description of the conformity  assessment activities, the conformity assessment module or modules and the AI systems for which the conformity assessment body claims to be competent, as well as by an  accreditation certificate, where one exists, issued by a national accreditation body attesting  that the conformity assessment body fulfils the requirements laid down in Article 33. Any  valid document related to existing designations of the applicant notified body under any  other Union harmonisation legislation shall be added. 
15698/22 RB/ek 105 TREE.2.B EN 
3. Where the conformity assessment body concerned cannot provide an accreditation  certificate, it shall provide the notifying authority with all the documentary evidence  necessary for the verification, recognition and regular monitoring of its compliance with  the requirements laid down in Article 33. For notified bodies which are designated under  any other Union harmonisation legislation, all documents and certificates linked to those  designations may be used to support their designation procedure under this Regulation, as  appropriate. The notified body shall update the documentation referred to in paragraph 2  and paragraph 3 whenever relevant changes occur, in order to enable the authority  responsible for notified bodies to monitor and verify continuous compliance with all the  requirements laid down in Article 33. 
Article 32 
Notification procedure 
1. Notifying authorities may only notify conformity assessment bodies which have satisfied  the requirements laid down in Article 33.  
2. Notifying authorities shall notify those bodies to the Commission and the other Member  States using the electronic notification tool developed and managed by the Commission.  
3. The notification referred to in paragraph 2 shall include full details of the conformity  assessment activities, the conformity assessment module or modules and the AI systems  concerned and the relevant attestation of competence. Where a notification is not based on  an accreditation certificate as referred to in Article 31 (2), the notifying authority shall  provide the Commission and the other Member States with documentary evidence which  attests to the conformity assessment body's competence and the arrangements in place to  ensure that that body will be monitored regularly and will continue to satisfy the  requirements laid down in Article 33. 
15698/22 RB/ek 106 TREE.2.B EN 
4. The conformity assessment body concerned may perform the activities of a notified body  only where no objections are raised by the Commission or the other Member States within  two weeks of a notification by a notifying authority where it includes an accreditation  certificate referred to in Article 31(2), or within two months of a notification by the  notifying authority where it includes documentary evidence referred to in Article 31(3).  
5. [deleted] 
Article 33 
Requirements relating to notified bodies  
1. A notified body shall be established under national law and have legal personality. 
2. Notified bodies shall satisfy the organisational, quality management, resources and process  requirements that are necessary to fulfil their tasks. 
3. The organisational structure, allocation of responsibilities, reporting lines and operation of  notified bodies shall be such as to ensure that there is confidence in the performance by  and in the results of the conformity assessment activities that the notified bodies conduct. 
4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to  which it performs conformity assessment activities. Notified bodies shall also be  independent of any other operator having an economic interest in the high-risk AI system  that is assessed, as well as of any competitors of the provider. 
5. Notified bodies shall be organised and operated so as to safeguard the independence,  objectivity and impartiality of their activities. Notified bodies shall document and  implement a structure and procedures to safeguard impartiality and to promote and apply  the principles of impartiality throughout their organisation, personnel and assessment  activities. 
15698/22 RB/ek 107 TREE.2.B EN 
6. Notified bodies shall have documented procedures in place ensuring that their personnel,  committees, subsidiaries, subcontractors and any associated body or personnel of external  bodies respect the confidentiality of the information in accordance with Article 70 which  comes into their possession during the performance of conformity assessment activities,  except when disclosure is required by law. The staff of notified bodies shall be bound to  observe professional secrecy with regard to all information obtained in carrying out their  tasks under this Regulation, except in relation to the notifying authorities of the Member  State in which their activities are carried out.  
7. Notified bodies shall have procedures for the performance of activities which take due  account of the size of an undertaking, the sector in which it operates, its structure, the  degree of complexity of the AI system in question. 
8. Notified bodies shall take out appropriate liability insurance for their conformity  assessment activities, unless liability is assumed by the Member State in which they are  located in accordance with national law or that Member State is itself directly responsible  for the conformity assessment. 
9. Notified bodies shall be capable of carrying out all the tasks falling to them under this  Regulation with the highest degree of professional integrity and the requisite competence  in the specific field, whether those tasks are carried out by notified bodies themselves or on  their behalf and under their responsibility. 
10. Notified bodies shall have sufficient internal competences to be able to effectively evaluate  the tasks conducted by external parties on their behalf. The notified body shall have  permanent availability of sufficient administrative, technical, legal and scientific personnel  who possess experience and knowledge relating to the relevant artificial intelligence  technologies, data and data computing and to the requirements set out in Chapter 2 of this  Title.
15698/22 RB/ek 108 TREE.2.B EN 
11. Notified bodies shall participate in coordination activities as referred to in Article 38. They  shall also take part directly or be represented in European standardisation organisations, or  ensure that they are aware and up to date in respect of relevant standards. 
12. [deleted] 
Article 33a 
Presumption of conformity with requirements relating to notified bodies  
Where a conformity assessment body demonstrates its conformity with the criteria laid down in the  relevant harmonised standards or parts thereof the references of which have been published in the  Official Journal of the European Union it shall be presumed to comply with the requirements set out  in Article 33 in so far as the applicable harmonised standards cover those requirements. 
Article 34 
Subsidiaries of and subcontracting by notified bodies 
1. Where a notified body subcontracts specific tasks connected with the conformity  assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the  subsidiary meets the requirements laid down in Article 33 and shall inform the notifying  authority accordingly.  
2. Notified bodies shall take full responsibility for the tasks performed by subcontractors or  subsidiaries wherever these are established. 
3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of  the provider.
15698/22 RB/ek 109 TREE.2.B EN 
4. The relevant documents concerning the assessment of the qualifications of the  subcontractor or the subsidiary and the work carried out by them under this Regulation  shall be kept at the disposal of the notifying authority for a period of 5 years from the  termination date of the subcontracting activity. 
Article 34a 
Operational obligations of notified bodies 
1. Notified bodies shall verify the conformity of high-risk AI system in accordance with the  conformity assessment procedures referred to in Article 43.  
2. Notified bodies shall perform their activities while avoiding unnecessary burdens for  providers, and taking due account of the size of an undertaking, the sector in which it  operates, its structure and the degree of complexity of the high risk AI system in question.  In so doing, the notified body shall nevertheless respect the degree of rigour and the level  of protection required for the compliance of the high risk AI system with the requirements  of this Regulation. 
3. Notified bodies shall make available and submit upon request all relevant documentation,  including the providers’ documentation, to the notifying authority referred to in Article 30  to allow that authority to conduct its assessment, designation, notification, monitoring  activities and to facilitate the assessment outlined in this Chapter. 
Article 35 
Identification numbers and lists of notified bodies designated under this Regulation 
1. The Commission shall assign an identification number to notified bodies. It shall assign a  single number, even where a body is notified under several Union acts.
15698/22 RB/ek 110 TREE.2.B EN 
2. The Commission shall make publicly available the list of the bodies notified under this  Regulation, including the identification numbers that have been assigned to them and the  activities for which they have been notified. The Commission shall ensure that the list is  kept up to date. 
Article 36 
Changes to notifications 
1. The notifying authority shall notify the Commission and the other Member States of any  relevant changes to the notification of a notified body via the electronic notification tool  referred to in Article 32(2).  
2. The procedures described in Article 31 and 32 shall apply to extensions of the scope of the  notification. For changes to the notification other than extensions of its scope, the  procedures laid down in the following paragraphs shall apply.  
Where a notified body decides to cease its conformity assessment activities it shall inform  the notifying authority and the providers concerned as soon as possible and in the case of a  planned cessation one year before ceasing its activities. The certificates may remain valid  for a temporary period of nine months after cessation of the notified body's activities on  condition that another notified body has confirmed in writing that it will assume  responsibilities for the AI systems covered by those certificates. The new notified body  shall complete a full assessment of the AI systems affected by the end of that period before  issuing new certificates for those systems. Where the notified body has ceased its activity,  the notifying authority shall withdraw the designation.
15698/22 RB/ek 111 TREE.2.B EN 
3. Where a notifying authority has sufficient reasons to consider that a notified body no  longer meets the requirements laid down in Article 33, or that it is failing to fulfil its  obligations, the notifying authority shall, provided that the the notified body had the  opportunity to make its views known, restrict, suspend or withdraw notification as  
appropriate, depending on the seriousness of the failure to meet those requirements or fulfil  those obligations. It shall immediately inform the Commission and the other Member  States accordingly. 
4. Where its designation has been suspended, restricted, or fully or partially withdrawn, the  notified body shall inform the manufacturers concerned at the latest within 10 days.  
5. In the event of restriction, suspension or withdrawal of a notification, the notifying  authority shall take appropriate steps to ensure that the files of the notified body concerned  are kept and make them available to notifying authorities in other Member States and to  market surveillance authorities at their request.  
6. In the event of restriction, suspension or withdrawal of a designation, the notifying  authority shall:  
a) assess the impact on the certificates issued by the notified body;  
b) submit a report on its findings to the Commission and the other Member States  within three months of having notified the changes to the notification; 
c) require the notified body to suspend or withdraw, within a reasonable period of time  determined by the authority, any certificates which were unduly issued in order to ensure the conformity of AI systems on the market;  
d) inform the Commission and the Member States about certificates of which it has  required their suspension or withdrawal;
15698/22 RB/ek 112 TREE.2.B EN 
e) provide the national competent authorities of the Member State in which the provider  has its registered place of business with all relevant information about the certificates  for which it has required suspension or withdrawal. That competent authority shall  take the appropriate measures, where necessary, to avoid a potential risk to health,  safety or fundamental rights. 
7. With the exception of certificates unduly issued, and where a notification has been  suspended or restricted, the certificates shall remain valid in the following circumstances:  
a) the notifying authority has confirmed, within one month of the suspension or  restriction, that there is no risk to health, safety or fundamental rights in relation to certificates affected by the suspension or restriction, and the notifying authority has  outlined a timeline and actions anticipated to remedy the suspension or restriction; or  
b) the notifying authority has confirmed that no certificates relevant to the suspension  will be issued, amended or re-issued during the course of the suspension or  restriction, and states whether the notified body has the capability of continuing to  monitor and remain responsible for existing certificates issued for the period of the  suspension or restriction. In the event that the authority responsible for notified  bodies determines that the notified body does not have the capability to support  existing certificates issued, the provider shall provide to the national competent  authorities of the Member State in which the provider of the system covered by the  certificate has its registered place of business, within three months of the suspension  or restriction, a written confirmation that another qualified notified body is  
temporarily assuming the functions of the notified body to monitor and remain  responsible for the certificates during the period of suspension or restriction.  
8. With the exception of certificates unduly issued, and where a designation has been  withdrawn, the certificates shall remain valid for a period of nine months in the following  circumstances: 
15698/22 RB/ek 113 TREE.2.B EN 
a) where the national competent authority of the Member State in which the provider of  the AI system covered by the certificate has its registered place of business has  confirmed that there is no risk to health, safety and fundamental rights associated  with the systems in question; and  
b) another notified body has confirmed in writing that it will assume immediate  responsibilities for those systems and will have completed assessment of them within  twelve months of the withdrawal of the designation. 
In the circumstances referred to in the first subparagraph, the national competent authority  of the Member State in which the provider of the system covered by the certificate has its  place of business may extend the provisional validity of the certificates for further periods  of three months, which altogether shall not exceed twelve months.  
The national competent authority or the notified body assuming the functions of the  notified body affected by the change of notification shall immediately inform the  Commission, the other Member States and the other notified bodies thereof. 
Article 37 
Challenge to the competence of notified bodies 
1. The Commission shall, where necessary, investigate all cases where there are reasons to  doubt whether a notified body complies with the requirements laid down in Article 33. 
2. The notifying authority shall provide the Commission, on request, with all relevant  information relating to the notification of the notified body concerned. 
3. The Commission shall ensure that all confidential information obtained in the course of its  investigations pursuant to this Article is treated confidentially in accordance with Article  70.
15698/22 RB/ek 114 TREE.2.B EN 
4. Where the Commission ascertains that a notified body does not meet or no longer meets  the requirements laid down in Article 33, it shall inform the notifying authority of the  reasons of such an ascertainment and request it to take the necessary corrective measures,  including the suspension, restriction or withdrawal of the designation if necessary. Where  the notifying authority fails to take the necessary corrective measures, the Commission  may, by means of implementing acts, suspend, restrict or withdraw the notification. That  implementing act shall be adopted in accordance with the examination procedure referred  to in Article 74(2).  
Article 38 
Coordination of notified bodies 
1. The Commission shall ensure that, with regard to high-risk AI systems, appropriate  coordination and cooperation between notified bodies active in the conformity assessment  procedures pursuant to this Regulation are put in place and properly operated in the form  of a sectoral group of notified bodies. 
2. The notifying authority shall ensure that the bodies notified by them participate in the work  of that group, directly or by means of designated representatives. 
Article 39 
Conformity assessment bodies of third countries 
Conformity assessment bodies established under the law of a third country with which the Union  has concluded an agreement may be authorised to carry out the activities of notified Bodies under  this Regulation, provided that they meet the requirements in Article 33.
15698/22 RB/ek 115 TREE.2.B EN 
CHAPTER 5 
STANDARDS, CONFORMITY ASSESSMENT, CERTIFICATES, REGISTRATION 
Article 40 
Harmonised standards 
1. High-risk AI systems or general purpose AI systems which are in conformity with harmonised  standards or parts thereof the references of which have been published in the Official Journal  of the European Union shall be presumed to be in conformity with the requirements set out in  Chapter 2 of this Title or, as applicable, with requirements set out in Article 4a and Article 4b,  to the extent those standards cover those requirements. 
2. When issuing a standardisation request to European standardisation organisations in accordance with Article 10 of Regulation 1025/2012, the Commission shall specify that  standards are coherent, clear and drafted in such a way that they aim to fulfil in particular the  following objectives: 
a) ensure that AI systems placed on the market or put into service in the Union are safe  and respect Union values and strengthen the Union's open strategic autonomy; 
b) promote investment and innovation in AI, including through increasing legal certainty, as  well as competitiveness and growth of the Union market; 
c) enhance multistakeholder governance, representative of all relevant European  stakeholders (e.g. industry, SMEs, civil society, researchers); 
d) contribute to strengthening global cooperation on standardisation in the field of AI that  is consistent with Union values and interests. 
The Commission shall request the European standardisation organisations to provide  evidence of their best efforts to fulfil the above objectives.
15698/22 RB/ek 116 TREE.2.B EN 
Article 41 
Common specifications 
1. The Commission is empowered to adopt, after consulting the AI Board referred to in  Article 56, implementing acts in accordance with the examination procedure referred to in  Article 74(2) establishing common technical specifications for the requirements set out in  Chapter 2 of this Title, or, as applicable, with requirements set out in Article 4a and Article  4b, where the following conditions have been fulfilled:  
(a) no reference to harmonised standards covering the relevant essential safety or  fundamental right concerns is published in the Official Journal of the European Union  in accordance with Regulation (EU) No 1025/2012; 
(b) the Commission has requested, pursuant to Article 10(1) of Regulation 1025/2012, one  or more European standardisation organisations to draft a harmonised standard for the  requirements set out in Chapter 2 of this Title; 
(c) the request referred to in point (b) has not been accepted by any of the European  standardisation organisations or the harmonised standards addressing that request are  not delivered within the deadline set in accordance with article 10(1) of Regulation  1025/2012 or those standards do not comply with the request. 
1a. Before preparing a draft implementing act, the Commission shall inform the committee  referred to in Article 22 of Regulation EU (No) 1025/2012 that it considers that the  conditions in paragraph 1 are fulfilled. 
2. In the early preparation of the draft implementing act establishing the common  specification, the Commission shall fulfil the objectives referred to in Article 40(2) and  gather the views of relevant bodies or expert groups established under relevant sectorial  Union law. Based on that consultation, the Commission shall prepare the draft  implementing act.
15698/22 RB/ek 117 TREE.2.B EN 
3. High-risk AI systems or general purpose AI systems which are in conformity with the  common specifications referred to in paragraph 1 shall be presumed to be in conformity  with the requirements set out in Chapter 2 of this Title or, as applicable, with requirements  set out in Article 4a and Article 4b, to the extent those common specifications cover those  requirements. 
4. When references of a harmonised standard are published in the Official Journal of the  European Union, implementing acts referred to in paragraph 1, which cover the  requirements set out in Chapter 2 of this Title or requirements set out in Article 4a and  Article 4b, shall be repealed, as applicable. 
5. When a Member State considers that a common specification does not entirely satisfy the  requirements set out in Chapter 2 of this Title or requirements set out in Article 4a and  Article 4b, as applicable, it shall inform the Commission thereof with a detailed  explanation and the Commission shall assess that information and, if appropriate, amend  the implementing act establishing the common specification in question. 
Article 42 
Presumption of conformity with certain requirements 
1. High-risk AI systems that have been trained and tested on data reflecting the specific  geographical, behavioural or functional setting within which they are intended to be used  shall be presumed to be in compliance with the respective requirements set out in Article  10(4). 
15698/22 RB/ek 118 TREE.2.B EN 
2. High-risk AI systems or general purpose AI systems that have been certified or for which a  statement of conformity has been issued under a cybersecurity scheme pursuant to  Regulation (EU) 2019/881 of the European Parliament and of the Council33 and the  references of which have been published in the Official Journal of the European Union  shall be presumed to be in compliance with the cybersecurity requirements set out in  Article 15 of this Regulation in so far as the cybersecurity certificate or statement of  conformity or parts thereof cover those requirements. 
Article 43 
Conformity assessment 
1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the  compliance of a high-risk AI system with the requirements set out in Chapter 2 of this  Title, the provider has applied harmonised standards referred to in Article 40, or, where  applicable, common specifications referred to in Article 41, the provider shall opt for one  of the following procedures: 
(a) the conformity assessment procedure based on internal control referred to in Annex  VI; or 
(b) the conformity assessment procedure based on assessment of the quality  management system and assessment of the technical documentation, with the  involvement of a notified body, referred to in Annex VII. 
Where, in demonstrating the compliance of a high-risk AI system with the requirements set  out in Chapter 2 of this Title, the provider has not applied or has applied only in part  harmonised standards referred to in Article 40, or where such harmonised standards do not  exist and common specifications referred to in Article 41 are not available, the provider  shall follow the conformity assessment procedure set out in Annex VII. 
  
33 Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the  European Union Agency for Cybersecurity) and on information and communications technology cybersecurity  certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 1).
15698/22 RB/ek 119 TREE.2.B EN 
For the purpose of the conformity assessment procedure referred to in Annex VII, the  provider may choose any of the notified bodies. However, when the system is intended to  be put into service by law enforcement, immigration or asylum authorities as well as EU  institutions, bodies or agencies, the market surveillance authority referred to in Article  63(5) or (6), as applicable, shall act as a notified body. 
2. For high-risk AI systems referred to in points 2 to 8 of Annex III and for general purpose  AI systems referred in Title 1a, providers shall follow the conformity assessment  procedure based on internal control as referred to in Annex VI, which does not provide for  the involvement of a notified body.  
3. For high-risk AI systems, to which legal acts listed in Annex II, section A, apply, the  provider shall follow the relevant conformity assessment as required under those legal acts.  The requirements set out in Chapter 2 of this Title shall apply to those high-risk AI systems  and shall be part of that assessment. Points 4.3., 4.4., 4.5. and the fifth paragraph of point  4.6 of Annex VII shall also apply.  
For the purpose of that assessment, notified bodies which have been notified under those  legal acts shall be entitled to control the conformity of the high-risk AI systems with the  requirements set out in Chapter 2 of this Title, provided that the compliance of those  notified bodies with requirements laid down in Article 33(4), (9) and (10) has been  assessed in the context of the notification procedure under those legal acts. 
Where the legal acts listed in Annex II, section A, enable the manufacturer of the product  to opt out from a third-party conformity assessment, provided that that manufacturer has  applied all harmonised standards covering all the relevant requirements, that manufacturer  may make use of that option only if he has also applied harmonised standards or, where  applicable, common specifications referred to in Article 41, covering the requirements set  out in Chapter 2 of this Title.  
4. [deleted]
15698/22 RB/ek 120 TREE.2.B EN 
5. The Commission is empowered to adopt delegated acts in accordance with Article 73 for  the purpose of updating Annexes VI and Annex VII in light of technical progress. 
6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in  order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the  conformity assessment procedure referred to in Annex VII or parts thereof. The  Commission shall adopt such delegated acts taking into account the effectiveness of the  conformity assessment procedure based on internal control referred to in Annex VI in  preventing or minimizing the risks to health and safety and protection of fundamental  rights posed by such systems as well as the availability of adequate capacities and  resources among notified bodies. 
Article 44 
Certificates 
1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a  a language which can be easily undestood by the relevant authorities in the Member State  in which the notified body is established.  
2. Certificates shall be valid for the period they indicate, which shall not exceed five years.  On application by the provider, the validity of a certificate may be extended for further  periods, each not exceeding five years, based on a re-assessment in accordance with the  applicable conformity assessment procedures. Any supplement to a certificate shall remain  valid as long as the certificate which it supplements is valid. 
3. Where a notified body finds that an AI system no longer meets the requirements set out in  Chapter 2 of this Title, it shall, taking account of the principle of proportionality, suspend  or withdraw the certificate issued or impose any restrictions on it, unless compliance with  those requirements is ensured by appropriate corrective action taken by the provider of the  
system within an appropriate deadline set by the notified body. The notified body shall  give reasons for its decision.
15698/22 RB/ek 121 TREE.2.B EN 
Article 45 
Appeal against decisions of notified bodies 
An appeal procedure against decisions of the notified bodies shall be available. 
Article 46 
Information obligations of notified bodies 
1. Notified bodies shall inform the notifying authority of the following:  
(a) any Union technical documentation assessment certificates, any supplements to those  certificates, quality management system approvals issued in accordance with the  requirements of Annex VII; 
(b) any refusal, restriction, suspension or withdrawal of a Union technical  documentation assessment certificate or a quality management system approval  issued in accordance with the requirements of Annex VII;  
(c) any circumstances affecting the scope of or conditions for notification; 
(d) any request for information which they have received from market surveillance  authorities regarding conformity assessment activities; 
(e) on request, conformity assessment activities performed within the scope of their  notification and any other activity performed, including cross-border activities and  subcontracting. 
2. Each notified body shall inform the other notified bodies of: 
(a) quality management system approvals which it has refused, suspended or withdrawn,  and, upon request, of quality system approvals which it has issued;
15698/22 RB/ek 122 TREE.2.B EN 
(b) EU technical documentation assessment certificates or any supplements thereto  which it has refused, withdrawn, suspended or otherwise restricted, and, upon  request, of the certificates and/or supplements thereto which it has issued. 
3. Each notified body shall provide the other notified bodies carrying out similar conformity  assessment activities covering the same AI systems with relevant information on issues  relating to negative and, on request, positive conformity assessment results. 
4. The obligations referred to in paragraphs 1 to 3 shall be complied with in accordance with  Article 70. 
Article 47 
Derogation from conformity assessment procedure 
1. By way of derogation from Article 43 and upon a duly justified request, any market  surveillance authority may authorise the placing on the market or putting into service of  specific high-risk AI systems within the territory of the Member State concerned, for  exceptional reasons of public security or the protection of life and health of persons,  environmental protection and the protection of key industrial and infrastructural assets.  That authorisation shall be for a limited period of time while the necessary conformity  assessment procedures are being carried out, taking into account the exceptional reasons  justifying the derogation. The completion of those procedures shall be undertaken without  undue delay. 
1a. In a duly justified situation of urgency for exceptional reasons of public security or in case  of specific, substantial and imminent threat to the life or physical safety of natural persons,  law enforcement authorities or civil protection authorities may put a specific high-risk AI  system into service without the authorisation referred to in paragraph 1 provided that such  authorisation is requested during or after the use without undue delay, and if such  authorisation is rejected, its use shall be stopped with immediate effect and all the results  and outputs of this use shall be immediately discarded.
15698/22 RB/ek 123 TREE.2.B EN 
2. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance  authority concludes that the high-risk AI system complies with the requirements of Chapter  2 of this Title. The market surveillance authority shall inform the Commission and the  other Member States of any authorisation issued pursuant to paragraph 1. This obligation  shall not cover sensitive operational data in relation to the activities of law enforcement  authorities. 
3. [deleted]  
4. [deleted]  
5. [deleted]  
6. For high-risk AI systems related to products covered by Union harmonisation legislation referred to in Annex II Section A, only the conformity assessment derogation procedures  established in that legislation shall apply. 
Article 48 
EU declaration of conformity 
1. The provider shall draw up a written or electronically signed EU declaration of conformity  for each AI system and keep it at the disposal of the national competent authorities for 10  years after the AI system has been placed on the market or put into service. The EU  declaration of conformity shall identify the AI system for which it has been drawn up. A  copy of the EU declaration of conformity shall be submitted to the relevant national  competent authorities upon request. 
2. The EU declaration of conformity shall state that the high-risk AI system in question meets  the requirements set out in Chapter 2 of this Title. The EU declaration of conformity shall  contain the information set out in Annex V and shall be translated into a language that can  be easily understood by the national competent authorities of the Member State(s) in which  the high-risk AI system is made available. 
15698/22 RB/ek 124 TREE.2.B EN 
3. Where high-risk AI systems are subject to other Union harmonisation legislation which  also requires an EU declaration of conformity, a single EU declaration of conformity shall  be drawn up in respect of all Union legislations applicable to the high-risk AI system. The  declaration shall contain all the information required for identification of the Union  harmonisation legislation to which the declaration relates.  
4. By drawing up the EU declaration of conformity, the provider shall assume responsibility  for compliance with the requirements set out in Chapter 2 of this Title. The provider shall  keep the EU declaration of conformity up-to-date as appropriate. 
5. The Commission shall be empowered to adopt delegated acts in accordance with Article 73  for the purpose of updating the content of the EU declaration of conformity set out in  Annex V in order to introduce elements that become necessary in light of technical  progress. 
Article 49 
CE marking of conformity 
1. The CE marking of conformity shall be subject to the general principles set out in Article  30 of Regulation (EC) No 765/2008.  
2. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems.  Where that is not possible or not warranted on account of the nature of the high-risk AI  system, it shall be affixed to the packaging or to the accompanying documentation, as  appropriate. 
3. Where applicable, the CE marking shall be followed by the identification number of the  notified body responsible for the conformity assessment procedures set out in Article 43.  The identification number shall also be indicated in any promotional material which  mentions that the high-risk AI system fulfils the requirements for CE marking.
15698/22 RB/ek 125 TREE.2.B EN 
Article 50 
[deleted] 
Article 51 
Registration of relevant operators and of high-risk AI systems listed in Annex III 
1. Before placing on the market or putting into service a high-risk AI system listed in Annex  III with the exception of high-risk AI systems referred to in Annex III, points 1, 6 and 7 in  the areas of law enforcement, migration, asylum and border control management, and high  risk AI systems referred to in Annex III point 2, the provider and where applicable, the  authorised representative shall register themselves in the EU database referred to in Article  60. The provider or, where applicable the authorised representative, shall also register their  systems in that database. 
2. Before using a high-risk AI system listed in Annex III, users of high-risk AI systems that  are public authorities, agencies or bodies, or entities acting on their behalf, shall register  themselves in the EU database referred to in Article 60 and select the system that they  envisage to use. 
The obligations laid down in the previous subparagraph shall not apply to law  enforcement, border control, immigration or asylum authorities, agencies or bodies and  authorities, agencies or bodies using high-risk AI systems referred to Annex III point 2, as  well as to entities acting on their behalf.
15698/22 RB/ek 126 TREE.2.B EN 
TITLE IV 
TRANSPARENCY OBLIGATIONS FOR PROVIDERS AND USERS OF CERTAIN AI SYSTEMS 
Article 52 
Transparency obligations for providers and users of certain AI systems 
1. Providers shall ensure that AI systems intended to interact with natural persons are  designed and developed in such a way that natural persons are informed that they are  interacting with an AI system, unless this is obvious from the point of view of a natural  person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. This obligation shall not apply to AI systems  authorised by law to detect, prevent, investigate and prosecute criminal offences, subject to  appropriate safeguards for the rights and freedoms of third parties, unless those systems are  available for the public to report a criminal offence. 
2. Users of a biometric categorisation system shall inform of the operation of the system the  natural persons exposed thereto. This obligation shall not apply to AI systems used for  biometric categorisation, which are permitted by law to detect, prevent and investigate  criminal offences, subject to appropriate safeguards for the rights and freedoms of third  parties. 
2a. Users of an emotion recognition system shall inform of the operation of the system the  natural persons exposed thereto. This obligation shall not apply to AI systems used for  emotion recognition which are permitted by law to detect, prevent and investigate criminal  offences, subject to appropriate safeguards for the rights and freedoms of third parties. 
15698/22 RB/ek 127 TREE.2.B EN 
3. Users of an AI system that generates or manipulates image, audio or video content that  appreciably resembles existing persons, objects, places or other entities or events and  would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose that  the content has been artificially generated or manipulated.  
However, the first subparagraph shall not apply where the use is authorised by law to  detect, prevent, investigate and prosecute criminal offences or where the content is part of  an evidently creative, satirical, artistic or fictional work or programme subject to  appropriate safeguards for the rights and freedoms of third parties. 
3a. The information referred to in paragraphs 1 to 3 shall be provided to natural persons in a  clear and distinguishable manner at the latest at the time of the first interaction or exposure. 
4. Paragraphs 1, 2, 2a and 3 and 3a shall not affect the requirements and obligations set out in  Title III of this Regulation and shall be without prejudice to other transparency obligations  for users of AI systems laid down in Union or national law. 
TITLE V 
MEASURES IN SUPPORT OF INNOVATION 
Article 53 
AI regulatory sandboxes  
-1a. National competent authorities may establish AI regulatory sandboxes for the  development, training, testing and validation of innovative AI systems under the direct  supervision, guidance and support by the national competent authority, before those  systems are placed on the market or put into service. Such regulatory sandboxes may  include testing in real world conditions supervised by the national competent authorities.
15698/22 RB/ek 128 TREE.2.B EN 
-1b. [deleted] 
-1c Where appropriate, national competent authorities shall cooperate with other relevant  authorities and may allow for the involvement of other actors within the AI ecosystem. 
-1d. This Article shall not affect other regulatory sandboxes established under national or Union  law, including in cases where the products or services that are tested in them are linked to  the use of innovative AI systems. Member States shall ensure an appropriate level of  cooperation between the authorities supervising those other sandboxes and the national  competent authorities. 
1. [deleted]  
1a. [deleted]  
1b. The establishment of AI regulatory sandboxes under this Regulation shall aim to contribute  to one or more of the following objectives: 
a) foster innovation and competitiveness and facilitate the development of an AI  ecosystem; 
b) facilitate and accelerate access to the Union market for AI systems, in particular when  provided by small and medium enterprises (SMEs), including start-ups; 
c) improve legal certainty and contribute to the sharing of best practices through  cooperation with the authorities involved in the AI regulatory sandbox with a view to  ensuring future compliance with this Regulation and, where appropriate, with other  Union and Member States legislation; 
d) contribute to evidence-based regulatory learning. 
2. [deleted]
15698/22 RB/ek 129 TREE.2.B EN 
2a. Access to the AI regulatory sandboxes shall be open to any provider or prospective  provider of an AI system who fulfils the eligibility and selection criteria referred to in  paragraph 6(a) and who has been selected by the national competent authorities following  the selection procedure referred to in paragraph 6(b). Providers or prospective providers  may also submit applications in partnership with users or any other relevant third parties. 
Participation in the AI regulatory sandbox shall be limited to a period that is appropriate to  the complexity and scale of the project. This period may be extended by the national  competent authority.  
Participation in the AI regulatory sandbox shall be based on a specific plan referred to in  paragraph 6 of this Article that shall be agreed between the participant(s) and the national  competent authoritie(s), as applicable.  
3. The participation in the AI regulatory sandboxes shall not affect the supervisory and  corrective powers of the authorities supervising the sandbox. Those authorities shall  exercice their supervisory powers in a flexible manner within the limits of the relevant  legislation, using their discretionary powers when implementing legal provisions to a  specific AI sandbox project, with the objective of supporting innovation in AI in the  Union. 
Provided that the participant(s) respect the sandbox plan and the terms and conditions for  their participation as referred to in paragraph 6(c) and follow in good faith the guidance  given by the authorities, no administrative fines shall be imposed by the authorities for  infringement of applicable Union or Member State legislation relating to the AI system  supervised in the sandbox, including the provisions of this Regulation.  
4. The participants remain liable under applicable Union and Member States liability  legislation for any damage caused in the course of their participation in an AI regulatory  sandbox.
15698/22 RB/ek 130 TREE.2.B EN 
4a. Upon request of the provider or prospective provider of the AI system, the national  competent authority shall provide, where applicable, a written proof of the activities  successfully carried out in the sandbox. The national competent authority shall also  provide an exit report detailing the activities carried out in the sandbox and the related  results and learning outcomes. Such written proof and exit report could be taken into  account by market surveillance authorities or notified bodies, as applicable, in the context  of conformity assessment procedures or market surveillance checks.  
Subject to the confidentiality provisions in Article 70 and with the agreement of the  sandbox participants, the European Commission and the AI Board shall be authorised to  access the exit reports and shall take them into account, as appropriate, when exercising  their tasks under this Regulation. If both the participant and the national competent  authority explicitly agree to this, the exit report can be made publicly available through the  single information platform referred to in article 55(3)(b). 
4b. The AI regulatory sandboxes shall be designed and implemented in such a way that, where  relevant, they facilitate cross-border cooperation between the national competent  authorities.  
5. National competent authorities shall make publicly available annual reports on the  implementation of the AI regulatory sandboxes, including good practices, lessons learnt  and recommendations on their setup and, where relevant, on the application of this  Regulation and other Union legislation supervised within the sandbox. Those annual  reports shall be submitted to the AI Board which shall make publicly available a summary  of all good practices, lessons learnt and recommendations. This obligation to make annual  reports publicly available shall not cover sensitive operational data in relation to the  activities of law enforcement, border control, immigration or asylum authorities. The  Commission and the AI Board shall, where appropriate, take the annual reports into  account when exercising their tasks under this Regulation.
15698/22 RB/ek 131 TREE.2.B EN 
5b. The Commission shall ensure that information about AI regulatory sandboxes, including  about those established under this Article, is available through the single information  platform referred to in Article 55(3)(b). 
6. The modalities and the conditions for the establishment and operation of the AI regulatory  sandboxes under this Regulation shall be adopted through implementing acts in accordance  with the examination procedure referred to in Article 74(2). 
The modalities and conditions shall to the best extent possible support flexibility for  national competent authorities to establish and operate their AI regulatory sandboxes,  foster innovation and regulatory learning and shall particularly take into account the  special circumstances and capacities of participating SMEs, including start-ups. 
Those implementing acts shall include common main principles on the following issues: 
a) eligibility and selection for participation in the AI regulatory sandbox;  b) procedure for the application, participation, monitoring, exiting from and  termination of the AI regulatory sandbox, including the sandbox plan and the exit  report;  
c) the terms and conditions applicable to the participants. 
7. When national competent authorities consider authorising testing in real world conditions  supervised within the framework of an AI regulatory sandbox established under this  Article, they shall specifically agree with the participants on the terms and conditions of  such testing and in particular on the appropriate safeguards with the view to protect  fundamental rights, health and safety. Where appropriate, they shall cooperate with other  national competent authorities with a view to ensure consistent practices across the Union.
15698/22 RB/ek 132 TREE.2.B EN 
Article 54 
Further processing of personal data for developing certain AI systems in the public interest in the  AI regulatory sandbox 
1. In the AI regulatory sandbox personal data lawfully collected for other purposes may be  processed for the purposes of developing, testing and training of innovative AI systems in  the sandbox under the following cumulative conditions: 
(a) the innovative AI systems shall be developed for safeguarding substantial public  interest by a public authority or another natural or legal person governed by public  law or by private law and in one or more of the following areas: 
(i) [deleted]  
(ii) public safety and health, including prevention, control and treatment of disease  and improvement of health care systems; 
(iii) protection and improvement of the quality of the environment, including green  transition, climate change mitigation and adaptation;  
(iv) energy sustainability, transport and mobility;  
(v) efficiency and quality of public administration and public services; 
(vi) cybersecurity and resilience of critical infrastructure. 
(b) the data processed are necessary for complying with one or more of the requirements  referred to in Title III, Chapter 2 where those requirements cannot be effectively  fulfilled by processing anonymised, synthetic or other non-personal data;
15698/22 RB/ek 133 TREE.2.B EN 
(c) there are effective monitoring mechanisms to identify if any high risks to the rights  and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU)  2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise during the  sandbox experimentation as well as response mechanism to promptly mitigate those  risks and, where necessary, stop the processing;  
(d) any personal data to be processed in the context of the sandbox are in a functionally  separate, isolated and protected data processing environment under the control of the  participants and only authorised persons have access to that data;  
(e) any personal data processed are not to be transmitted, transferred or otherwise  accessed by other parties that are not participants in the sandbox, unless such  disclosure occurs in compliance with Regulation (EU) 2016/679 or, where  applicable, Regulation 2018/725, and all participants have agreed to it;  
(f) any processing of personal data in the context of the sandbox shall not affect the  application of the rights of the data subjects as provided for under Union law on the  protection of personal data, in particular in Article 22 of Regulation (EU) 2016/679  and Article 24 of Regulation (EU) 2018/1725; 
(g) any personal data processed in the context of the sandbox are protected by means of  appropriate technical and organisational measures and deleted once the participation  in the sandbox has terminated or the personal data has reached the end of its retention  period;  
(h) the logs of the processing of personal data in the context of the sandbox are kept for  the duration of the participation in the sandbox, unless provided otherwise by Union  or national law; 
(i) complete and detailed description of the process and rationale behind the training,  testing and validation of the AI system is kept together with the testing results as part  of the technical documentation in Annex IV;
15698/22 RB/ek 134 TREE.2.B EN 
(j) a short summary of the AI project developed in the sandbox, its objectives and  expected results published on the website of the competent authorities. This  obligation shall not cover sensitive operational data in relation to the activities of law  enforcement, border control, immigration or asylum authorities. 
1a. For the purpose of prevention, investigation, detection or prosecution of criminal offences  or the execution of criminal penalties, including the safeguarding against and the  prevention of threats to public security, under the control and responsibility of law  enforcement authorities, the processing of personal data in AI regulatory sandboxes shall  be based on a specific Member State or Union law and subject to the same cumulative  conditions as referred to in paragraph 1. 
2. Paragraph 1 is without prejudice to Union or Member States laws laying down the basis for  the processing of personal data which is necessary for the purpose of developing, testing  and training of innovative AI systems or any other legal basis, in compliance with Union  law on the protection of personal data. 
Article 54a 
Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes 
1. Testing of AI systems in real world conditions outside AI regulatory sandboxes may be  conducted by providers or prospective providers of high-risk AI systems listed in Annex  III, in accordance with the provisions of this Article and the real-world testing plan  referred to in this Article. 
The detailed elements of the real-world testing plan shall be specified in implementing acts  adopted by the Commission in accordance with the examination procedure referred to in  Article 74(2).
15698/22 RB/ek 135 TREE.2.B EN 
This provision shall be without prejudice to Union or Member State legislation for the  testing in real world conditions of high-risk AI systems related to products covered by  legislation listed in Annex II. 
2. Providers or prospective providers may conduct testing of high-risk AI systems referred to  in Annex III in real world conditions at any time before the placing on the market or  putting into service of the AI system on their own or in partnership with one or more  prospective users.  
3. The testing of high-risk AI systems in real world conditions under this Article shall be  without prejudice to ethical review that may be required by national or Union law.  
4. Providers or prospective providers may conduct the testing in real world conditions only  where all of the following conditions are met:  
(a) the provider or prospective provider has drawn up a real-world testing plan and  submitted it to the market surveillance authority in the Member State(s) where the testing in real world conditions is to be conducted; 
(b) the market surveillance authority in the Member State(s) where the testing in real  world conditions is to be conducted have not objected to the testing within 30 days  after its submission;  
(c) the provider or prospective provider with the exception of high-risk AI systems  referred to in Annex III, points 1, 6 and 7 in the areas of law enforcement, migration,  asylum and border control management, and high risk AI systems referred to in Annex  III point 2, has registered the testing in real world conditions in the EU database  referred to in Article 60(5a) with a Union-wide unique single identification number  and the information specified in Annex VIIIa; 
(d) the provider or prospective provider conducting the testing in real world conditions is  established in the Union or it has appointed a legal representative for the purpose of  the testing in real world conditions who is established in the Union;
15698/22 RB/ek 136 TREE.2.B EN 
(e) data collected and processed for the purpose of the testing in real world conditions  shall not be transferred to countries outside the Union, unless the transfer and the  processing provides equivalent safeguards to those provided under Union law; 
(f) the testing in real world conditions does not last longer than necessary to achieve its  objectives and in any case not longer than 12 months; 
(g) persons belonging to vulnerable groups due to their age, physical or mental disability  are appropriately protected; 
(h) [deleted] 
(i) where a provider or prospective provider organises the testing in real world conditions  in cooperation with one or more prospective users, the latter have been informed of all  aspects of the testing that are relevant to their decision to participate, and given the  relevant instructions on how to use the AI system referred to in Article 13; the  provider or prospective provider and the user(s) shall conclude an agreement  specifying their roles and responsibilities with a view to ensuring compliance with the  provisions for testing in real world conditions under this Regulation and other  applicable Union and Member States legislation; 
(j) the subjects of the testing in real world conditions have given informed consent in  accordance with Article 54b, or in the case of law enforcement, where the seeking of  informed consent would prevent the AI system from being tested, the testing itself and  the outcome of the testing in the real world conditions shall not have a negative effect  on the subject; 
(k) the testing in real world conditions is effectively overseen by the provider or  prospective provider and user(s) with persons who are suitably qualified in the  relevant field and have the necessary capacity, training and authority to perform their  tasks; 
(l) the predictions, recommendations or decisions of the AI system can be effectively  reversed or disregarded.
15698/22 RB/ek 137 TREE.2.B EN 
5. Any subject of the testing in real world conditions, or his or her legally designated  representative, as appropriate, may, without any resulting detriment and without having to  provide any justification, withdraw from the testing at any time by revoking his or her  informed consent. The withdrawal of the informed consent shall not affect the activities  already carried out and the use of data obtained based on the informed consent before its  withdrawal.  
6. Any serious incident identified in the course of the testing in real world conditions shall be  reported to the national market surveillance authority in accordance with Article 62 of this  Regulation. The provider or prospective provider shall adopt immediate mitigation  measures or, failing that, suspend the testing in real world conditions until such mitigation  takes place or otherwise terminate it. The provider or prospective provider shall establish a  procedure for the prompt recall of the AI system upon such termination of the testing in  real world conditions. 
7. Providers or prospective providers shall notify the national market surveillance authority in  the Member State(s) where the testing in real world conditions is to be conducted of the  suspension or termination of the testing in real world conditions and the final outcomes. 
8. The provider and prospective provider shall be liable under applicable Union and Member  States liability legislation for any damage caused in the course of their participation in the  testing in real world conditions. 
Article 54b 
Informed consent to participate in testing in real world conditions outside AI regulatory sandboxes 
1. For the purpose of testing in real world conditions under Article 54a, informed consent  shall be freely given by the subject of testing prior to his or her participation in such testing  and after having been duly informed with concise, clear, relevant, and understandable  information regarding:
15698/22 RB/ek 138 TREE.2.B EN 
(i) the nature and objectives of the testing in real world conditions and the possible  inconvenience that may be linked to his or her participation;  
(ii) the conditions under which the testing in real world conditions is to be conducted,  including the expected duration of the subject's participation; 
(iii) the subject's rights and guarantees regarding participation, in particular his or her  right to refuse to participate in and the right to withdraw from testing in real world  conditions at any time without any resulting detriment and without having to provide  any justification; 
(iv) the modalities for requesting the reversal or the disregard of the predictions,  recommendations or decisions of the AI system;  
(v) the Union-wide unique single identification number of the testing in real world  conditions in accordance with Article 54a(4c) and the contact details of the provider  or its legal representative from whom further information can be obtained. 
2. The informed consent shall be dated and documented and a copy shall be given to the  subject or his or her legal representative. 
Article 55 
Support measures for operators, in particular SMEs, including start-ups  
1. Member States shall undertake the following actions: 
(a) provide SMEs, including start-ups, with priority access to the AI regulatory  sandboxes to the extent that they fulfil the eligibility and selection criteria; 
(b) organise specific awareness raising and training activities about the application of  this Regulation tailored to the needs of the SMEs, including start-ups, and, as  appropriate, local public authorities;
15698/22 RB/ek 139 TREE.2.B EN 
(c) where appropriate, establish a dedicated channel for communication with SMEs,  including start-ups and, as appropriate, local public authorities to provide advice and  respond to queries about the implementation of this Regulation, including as regards  participation in AI regulatory sandboxes. 
2. The specific interests and needs of the SME providers, including start-ups, shall be taken  into account when setting the fees for conformity assessment under Article 43, reducing  those fees proportionately to their size, market size and other relevant indicators. 
3. The Commission shall undertake the following actions: 
(a) upon request of the AI Board, provide standardised templates for the areas covered  by this Regulation; 
(b) develop and maintain a single information platform providing easy to use  information in relation to this Regulation for all operators across the Union; 
(c) organise appropriate communication campaigns to raise awareness about the  obligations arising from this Regulation; 
(d) evaluate and promote the convergence of best practices in public procurement  procedures in relation to AI systems.
15698/22 RB/ek 140 TREE.2.B EN