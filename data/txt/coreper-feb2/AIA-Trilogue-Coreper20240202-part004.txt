prejudice to the procedural rights of the concerned operator in accordance with Article 18  of Regulation (EU) 2019/1020. The period referred to in the first sentence of this  paragraph shall be reduced to thirty days in the event of non-compliance with the  prohibition of the artificial intelligence practices referred to in Article 5. 
9. The market surveillance authorities of all Member States shall ensure that appropriate  restrictive measures are taken in respect of the product or the AI system concerned, such as  withdrawal of the product or the AI system from their market, without undue delay. 
Article 65a 
Procedure for dealing with AI systems classified by the provider as a not high-risk in  application of Annex III 
1. Where a market surveillance authority has sufficient reasons to consider that an AI system  classified by the provider as non-high-risk in application of Annex III is high-risk, they  market surveillance authority shall carry out an evaluation of the AI system concerned in  respect of its classification as a high-risk AI system based on the conditions set out in  Annex III and the Commission guidelines. 
2. Where, in the course of that evaluation, the market surveillance authority finds that the AI  system concerned is high-risk, it shall without undue delay require the relevant provider to  take all necessary actions to bring the AI system into compliance with the requirements  and obligations laid down in this Regulation as well as take appropriate corrective action  within a period it may prescribe. 
3. Where the market surveillance authority considers that the use of the AI system concerned  is not restricted to its national territory, it shall inform the Commission and the other  Member States without undue delay of the results of the evaluation and of the actions  which it has required the provider to take.  
4. The provider shall ensure that all necessary action is taken to bring the AI system into  compliance with the requirements and obligations laid down in this Regulation. Where the  provider of an AI system concerned does not bring the AI system into compliance with the  requirements and obligations of this Regulation within the period referred to in paragraph 2, the provider shall be subject to fines in accordance with Article 71.
 
5. The provider shall ensure that all appropriate corrective action is taken in respect of all the  AI systems concerned that it has made available on the market throughout the Union. 
6. Where the provider of the AI system concerned does not take adequate corrective action  within the period referred to in paragraph 2, then the provisions of Article 65 paragraphs 5  to 9 apply.  
7. Where, in the course of that evaluation pursuant to paragraph 1, the market surveillance  authority establishes that the AI system was misclassified by the provider as not high-risk  to circumvent the application of requirements in Title III, Chapter 2, the provider shall be  subject to fines in accordance with Article 71. 
8. In exercising their power to monitor the application of this article and in accordance with  Article 11 of Regulation (EU) 2019/1020, market surveillance authorities may perform  appropriate checks, taking into account in particular information stored in the EU database  referred to in Article 60. 
Article 66 
Union safeguard procedure 
1. Where, within three months of receipt of the notification referred to in Article 65(5), or 30  days in the case of non-compliance with the prohibition of the artificial intelligence  practices referred to in Article 5, objections are raised by the market surveillance authority  of a Member State against a measure taken by another market surveillance authority, or  where the Commission considers the measure to be contrary to Union law, the Commission  shall without undue delay enter into consultation with the market surveillance authority of  the relevant Member State and operator or operators and shall evaluate the national  measure. On the basis of the results of that evaluation, the Commission shall decide  whether the national measure is justified or not within six months, or 60 days in the case of  non-compliance with the prohibition of the artificial intelligence practices referred to in  Article 5, starting from the notification referred to in Article 65(5) and notify such decision  to the market surveillance authority of the Member State concerned. The Commission shall  also inform all other market surveillance authorities of such decision. 
 
2. If the measure taken by the relevant Member States is considered justified by the  Commission, all Member States shall ensure that appropriate restrictive measures are taken  in respect of the AI system concerned, such as withdrawal of the AI system from their  market without undue delay, and shall inform the Commission accordingly. If the national  measure is considered unjustified by the Commission, the Member State concerned shall  withdraw the measure and inform the Commission accordingly. 
3. Where the national measure is considered justified and the non-compliance of the AI  system is attributed to shortcomings in the harmonised standards or common specifications  referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the  procedure provided for in Article 11 of Regulation (EU) No 1025/2012. 
Article 67 
Compliant AI systems which present a risk 
1. Where, having performed an evaluation under Article 65, after consulting the relevant  national public authority referred to in Article 64(3), the market surveillance authority of a  Member State finds that although a high-risk AI system is in compliance with this  Regulation, it presents a risk to the health or safety of persons, fundamental rights, or to  other aspects of public interest protection, it shall require the relevant operator to take all  appropriate measures to ensure that the AI system concerned, when placed on the market  or put into service, no longer presents that risk without undue delay, within a period it may  prescribe. 
2. The provider or other relevant operators shall ensure that corrective action is taken in  respect of all the AI systems concerned that they have made available on the market  throughout the Union within the timeline prescribed by the market surveillance authority of  the Member State referred to in paragraph 1.  
3. The Member States shall immediately inform the Commission and the other Member  States. That information shall include all available details, in particular the data necessary  for the identification of the AI system concerned, the origin and the supply chain of the AI  system, the nature of the risk involved and the nature and duration of the national measures  taken.
 
4. The Commission shall without undue delay enter into consultation with the Member States  concerned and the relevant operator and shall evaluate the national measures taken. On the  basis of the results of that evaluation, the Commission shall decide whether the measure is  justified or not and, where necessary, propose appropriate measures. 
5. The Commission shall immediately communicate its decision to the Member States  concerned and to the relevant operators. It shall also inform of the decision all other  Member States.  
Article 68 
Formal non-compliance 
1. Where the market surveillance authority of a Member State makes one of the following  findings, it shall require the relevant provider to put an end to the non-compliance  concerned, within a period it may prescribe:  
(a) the CE marking has been affixed in violation of Article 49; 
(b) the CE marking has not been affixed; 
(c) the EU declaration of conformity has not been drawn up; 
(d) the EU declaration of conformity has not been drawn up correctly; 
(ea) the registration in the EU database has not been carried out; 
(eb) where applicable, the authorised representative has not been appointed; (ec) the technical documentation is not available. 
2. Where the non-compliance referred to in paragraph 1 persists, the market surveillance  authority of the Member State concerned shall take appropriate and proportionate measures  to restrict or prohibit the high-risk AI system being made available on the market or ensure  that it is recalled or withdrawn from the market without delay. 
 
Article 68a 
EU AI testing support structures in the area of artificial intelligence 
1. The Commission shall designate one or more EU AI testing support structures to perform  the tasks listed under Article 21(6) of Regulation (EU) 1020/2019 in the area of artificial  intelligence.  
2. Without prejudice to the tasks referred to in paragraph 1, EU AI testing support structure  shall also provide independent technical or scientific advice at the request of the Board, the  Commission, or market surveillance authorities.  
Chapter 3b 
REMEDIES 
Article 68a 
Right to lodge a complaint with a market surveillance authority 
1. Without prejudice to other administrative or judicial remedies, complaints to the relevant  market surveillance authority may be submitted by any natural or legal person having  grounds to consider that there has been an infringement of the provisions of this  Regulation. 
2. In accordance with Regulation (EU) 2019/1020, complaints shall be taken into account for  the purpose of conducting the market surveillance activities and be handled in line with the  dedicated procedures established therefore by the market surveillance authorities 
Article 68 c 
A right to explanation of individual decision-making 
1. Any affected person subject to a decision which is taken by the deployer on the basis of the  output from an high-risk AI system listed in Annex III, with the exception of systems listed  under point 2, and which produces legal effects or similarly significantly affects him or her  in a way that they consider to adversely impact their health, safety and fundamental rights  
shall have the right to request from the deployer clear and meaningful explanations on the 
 
role of the AI system in the decision-making procedure and the main elements of the  decision taken. 
2. Paragraph 1 shall not apply to the use of AI systems for which exceptions from, or  restrictions to, the obligation under paragraph 1 follow from Union or national law in  compliance with Union law. 
3. This Article shall only apply to the extent that the right referred to in paragraph 1 is not  already provided for under Union legislation. 
Article 68d 
 Amendment to Directive (EU) 2020/1828 
In Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council30,  the following point is added: “(67a) Regulation xxxx/xxxx of the European Parliament and  of the Council [laying down harmonised rules on artificial intelligence (Artificial  Intelligence Act) and amending certain Union legislative acts (OJ L ...)]”.  
Article 68 e 
 Reporting of breaches and protection of reporting persons 
Directive (EU) 2019/1937 of the European Parliament and of the Council shall apply to the  reporting of breaches of this Regulation and the protection of persons reporting such  breaches.  
  
30 Directive (EU) 2020/1828 of the European Parliament and of the Council of 25 November 2020 on  representative actions for the protection of the collective interests of consumers and repealing Directive  2009/22/EC (OJ L 409, 4.12.2020, p. 1).
 
Chapter 3c 
SUPERVISION, INVESTIGATION, ENFORCEMENT AND  MONITORING IN RESPECT OF PROVIDERS OF GENERAL  PURPOSE AI MODELS 
Article 68f  
Enforcement of obligations on providers of general purpose AI models  
1. The Commission shall have exclusive powers to supervise and enforce Chapter/Title  [general purpose AI models] taking into account the procedural guarantees by virtue of  Article 68m. The Commission shall entrust the implementation of these tasks to the  European AI Office, without prejudice to the powers of organisation of the Commission  and the division of competences between Member States and the Union based on the  Treaties. 
2. Without prejudice to Article 63a paragraph 3, market surveillance authorities may request  to the Commission to exercise the powers laid down in this Chapter, where this is  necessary and proportionate to assist with the fulfilment of their tasks under this  Regulation. 
Article 68g 
Monitoring actions  
1. For the purposes of carrying out the tasks assigned to it under this Chapter, the AI Office  may take the necessary actions to monitor the effective implementation and compliance  with this Regulation by providers of general purpose AI models, including adherence to  approved codes of practice.  
2. Downstream providers shall have the right to lodge a complaint alleging an infringement  of this Regulation. A complaint shall be duly reasoned and at least indicate:  
(a) the point of contact of the provider of the general purpose AI model concerned;
 
(b) description of the relevant facts, the provisions of this Regulation concerned and the  reason why the downstream provider considers that the provider of the general  purpose AI model concerned infringed this Regulation; 
(c) any other information that the downstream provider that sent the request considers  relevant, including, where appropriate, information gathered on its own initiative. 
Article 68h 
Alerts of systemic risks by the scientific panel 
1. The scientific panel may provide a qualified alert to the AI Office where it has reason to  suspect that 
(a) a general purpose AI model poses concrete identifiable risk at Union level; or 
(b) a general purpose AI model meets the requirements referred to in Article 52a  [Classification of general purpose AI models with systemic risk]. 
2. Upon such qualified alert, the Commission, through the AI Office and after having  informed the AI Board, may exercise the powers laid down in this Chapter for the purpose  of assessing the matter. The AI Office shall inform the Board of any measure according to  Articles 68i-68m. 
3. A qualified alert shall be duly reasoned and at least indicate:  
(a) the point of contact of the provider of the general purpose AI model with systemic  risk concerned;  
(b) a description of the relevant facts and reasons for the suspicion of the scientific panel; 
(c) any other information that the scientific panel considers relevant, including, where  appropriate, information gathered on its own initiative. 
Article 68i
 
Power to request documentation and information  
1. The Commission may request the provider of the general purpose AI model concerned to  provide the documentation drawn up by the provider according to Article 52c [Obligations  for providers of general purpose AI models] and 52d [Obligations on providers of general  purpose AI models with systemic risk] or any additional information that is necessary for  the purpose of assessing compliance of the provider with this Regulation.  
2. Before the request for information is sent, the AI Office may initiate a structured dialogue  with the provider of the general purpose AI model. 
3. Upon a duly substantiated request from the scientific panel, the Commission may issue a  request for information to a provider of a general purpose AI model, where the access to  information is necessary and proportionate for the fulfilment of the tasks of the scientific  panel according to Article 58b [Scientific panel](2). 
4. The request for information shall state the legal basis and the purpose of the request,  specifying what information is required and set the period within which the information is  to be provided, and the fines provided for in Article 72a [fines] for supplying incorrect,  incomplete or misleading information. 
5. The provider of the general purpose AI model concerned or their representatives and, in  the case of legal persons, companies or firms, or where they have no legal personality, the  persons authorised to represent them by law or by their constitution shall supply the  information requested on behalf of the provider of the general purpose AI model  concerned. Lawyers duly authorised to act may supply the information on behalf of their  clients. The latter shall remain fully responsible if the information supplied is incomplete,  incorrect or misleading. 
Article 68j  
Power to conduct evaluations  
1. The AI Office, after consulting the Board, may conduct evaluations of the general purpose  AI model concerned
 
(a) to assess compliance of the provider with the obligations under this Regulation,  where the information gathered pursuant to Article 68i [Power to request  
information] is insufficient; or 
(b) to investigate systemic risks at Union level of general purpose AI models with  systemic risk, in particular following a qualified report from the scientific panel in  accordance with point (c) of Article 68f [Enforcement of obligations on providers of  general purpose AI models and general purpose AI models with systemic risk](3).  
2. The Commission may decide to appoint independent experts to carry out evaluations on its  behalf, including from the scientific panel pursuant to Article [scientific panel of  independent experts]. All independent experts appointed for this task shall meet the criteria  outlined in Article 58b, paragraph 2. 
3. For the purpose of paragraph 1, the Commission may request access to the general purpose  AI model concerned through application programming interfaces (‘API’) or further  appropriate technical means and tools, including through source code. 
4. The request for access shall state the legal basis, the purpose and reasons of the request and  set the period within which the access is to be provided, and the fines provided for in  Article 72a [fines] for failure to provide access. 
5. The providers of the general purpose AI model concerned and, in the case of legal persons,  companies or firms, or where they have no legal personality, the persons authorised to  represent them by law or by their constitution shall provide the access requested on behalf  of the provider of the general purpose AI model concerned. 
6. The modalities and the conditions of the evaluations, including the modalities for involving  independent experts and the procedure for the selection of the latter, shall be set out in  implementing acts. Those implementing acts shall be adopted in accordance with the  examination procedure referred to in Article 74(2). 
7. Prior to requesting access to the general purpose AI model concerned, the AI Office may  initiate a structured dialogue with the provider of the general purpose AI model to gather  more information on the internal testing of the model, internal safeguards for preventing  systemic risks, and other internal procedures and measures the provider has taken to  mitigate such risks.
 
Article 68k 
Power to request measures 
1. Where necessary and appropriate, the Commission may request providers to 
(a) take appropriate measures to comply with the obligations set out in Title VIIIa,  Chapter 2 [Obligations for provider of general purpose AI models];  
(b) require a provider to implement mitigation measures, where the evaluation carried out  in accordance with Article 68j [Power to conduct evaluations] has given rise to  serious and substantiated concern of a systemic risk at Union level; 
(c) restrict the making available on the market, withdraw or recall the model. 
2. Before a measure is requested, the AI Office may initiate a structured dialogue with the  provider of the general purpose AI model. 
3. If, during the structured dialogue under paragraph 2, the provider of the general purpose AI  model with systemic risk offers commitments to implement mitigation measures to address  a systemic risk at Union level, the Commission may by decision make these commitments  binding and declare that there are no further grounds for action. 
Article 68m 
Procedural rights of economic operators of the general purpose AI model 
Article 18 of the Regulation (EU) 2019/1020 apply by analogy to the providers of the  general purpose AI model without prejudice to more specific procedural rights provided  for in this Regulation. 
 
TITLE IX 
CODES OF CONDUCT 
Article 69 
Codes of conduct for voluntary application of specific requirements 
1. The AI Office, and the Member States shall encourage and facilitate the drawing up of  codes of conduct, including related governance mechanisms, intended to foster the  voluntary application to AI systems other than high-risk AI systems of some or all of the  requirements set out in Title III, Chapter 2 of this Regulation taking into account the  available technical solutions and industry best practices allowing for the application of  such requirements. 
2. The AI Office and the Member States shall facilitate the drawing up of codes of conduct  concerning the voluntary application, including by deployers, of specific requirements to  all AI systems, on the basis of clear objectives and key performance indicators to measure  the achievement of those objectives, including elements such as, but not limited to:  (a) applicable elements foreseen in European ethic guidelines for trustworthy AI;  
(b) assessing and minimizing the impact of AI systems on environmental sustainability, including as regards energy-efficient programming and  
techniques for efficient design, training and use of AI;  
(c) promoting AI literacy, in particular of persons dealing with the development,  operation and use of AI;  
(d) facilitating an inclusive and diverse design of AI systems, including through the  establishment of inclusive and diverse development teams and the promotion of  stakeholders’ participation in that process; 
 
(e) assessing and preventing the negative impact of AI systems on vulnerable persons  or groups of persons, including as regards accessibility for persons with a  
disability, as well as on gender equality.  
3. Codes of conduct may be drawn up by individual providers or deployers of AI systems or by organisations representing them or by both, including with the involvement of  deployers and any interested stakeholders and their representative organisations, including  civil society organisations and academia. Codes of conduct may cover one or more AI  systems taking into account the similarity of the intended purpose of the relevant systems. 
4. The AI Office, and the Member States shall take into account the specific interests and  needs of SMEs, including start-ups, when encouraging and facilitating the drawing up of  codes of conduct. 
TITLE X 
CONFIDENTIALITY AND PENALTIES  
Article 70 
Confidentiality 
1. The Commission, market surveillance authorities and notified bodies and any other natural  or legal person involved in the application of this Regulation shall, in accordance with  Union or national law, respect the confidentiality of information and data obtained in  carrying out their tasks and activities in such a manner as to protect, in particular: 
(a) intellectual property rights, and confidential business information or trade secrets of  a natural or legal person, including source code, except the cases referred to in  Article 5 of Directive 2016/943 on the protection of undisclosed know-how and  business information (trade secrets) against their unlawful acquisition, use and  disclosure apply; 
(b) the effective implementation of this Regulation, in particular for the purpose of  inspections, investigations or audits; 
(ba) public and national security interests;
 
(c) integrity of criminal or administrative proceedings; 
(da) the integrity of information classified in accordance with Union or national law. 
1a. The authorities involved in the application of this Regulation pursuant to paragraph 1 shall  only request data that is strictly necessary for the assessment of the risk posed by the AI  system and for the exercise of their powers in compliance with this Regulation and  Regulation 2019/1020. They shall put in place adequate and effective cybersecurity  measures to protect the security and confidentiality of the information and data obtained  and shall delete the data collected as soon as it is no longer needed for the purpose it was  requested for, in accordance with applicable national or European legislation.  
2. Without prejudice to paragraph 1 and 1a, information exchanged on a confidential basis  between the national competent authorities and between national competent authorities and  the Commission shall not be disclosed without the prior consultation of the originating  national competent authority and the deployer when high-risk AI systems referred to in  points 1, 6 and 7 of Annex III are used by law enforcement, border control, immigration or  asylum authorities, when such disclosure would jeopardise public and national security  interests. This exchange of information shall not cover sensitive operational data in relation  to the activities of law enforcement, border control, immigration or asylum authorities. 
When the law enforcement, immigration or asylum authorities are providers of high-risk  AI systems referred to in points 1, 6 and 7 of Annex III, the technical documentation  referred to in Annex IV shall remain within the premises of those authorities. Those  authorities shall ensure that the market surveillance authorities referred to in Article 63(5) and (6), as applicable, can, upon request, immediately access the documentation or obtain a  copy thereof. Only staff of the market surveillance authority holding the appropriate level  of security clearance shall be allowed to access that documentation or any copy thereof. 
3. Paragraphs 1, [1a] and 2 shall not affect the rights and obligations of the Commission,  Member States and their relevant authorities, as well as notified bodies, with regard to the  exchange of information and the dissemination of warnings, including in the context of  cross-border cooperation, nor the obligations of the parties concerned to provide  information under criminal law of the Member States. 
4. The Commission and Member States may exchange, where necessary and in accordance  with relevant provisions of international and trade agreements, confidential information 
 
with regulatory authorities of third countries with which they have concluded bilateral or  multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality. 
Article 71 
Penalties 
1. In compliance with the terms and conditions laid down in this Regulation, Member States  shall lay down the rules on penalties and other enforcement measures, which may also  include warnings and non-monetary measures, applicable to infringements of this  Regulation by operators, and shall take all measures necessary to ensure that they are  properly and effectively implemented and taking into account the guidelines issued by the  Commission pursuant to Article 82b. The penalties provided for shall be effective,  proportionate, and dissuasive. They shall take into account the interests of SMEs including  start-ups and their economic viability.  
2. The Member States shall without delay notify the Commission and at the latest by the date  of entry into application of those respective rules and of those respective measures and  shall notify them, without delay, of any subsequent amendment affecting them. 
3. Non-compliance with the prohibition of the artificial intelligence practices referred to in  Article 5 shall be subject to administrative fines of up to 35 000 000 EUR or, if the  offender is a company, up to 7 % of its total worldwide annual turnover for the preceding  financial year, whichever is higher. 
4. Non-compliance of an AI system with any of the following provisions related to operators  or notified bodies, other than those laid down in Articles 5, shall be subject to  administrative fines of up to 15 000 000 EUR or, if the offender is a company, up to 3% of  its total worldwide annual turnover for the preceding financial year, whichever is higher:  
(b) obligations of providers pursuant to Article 16; 
(d) obligations of authorised representatives pursuant to Article 25; 
(e) obligations of importers pursuant to Article 26; 
(f) obligations of distributors pursuant to Article 27;
 
(g) obligations of deployers pursuant to Article 29, paragraphs 1 to 6a; 
(h) requirements and obligations of notified bodies pursuant to Article 33, 34(1), 34(3),  34(4), 34a; 
(i) transparency obligations for providers and users pursuant to Article 52. 
5. The supply of incorrect, incomplete or misleading information to notified bodies and  national competent authorities in reply to a request shall be subject to administrative fines  of up to 7 500 000 EUR or, if the offender is a company, up to 1 % of its total worldwide  annual turnover for the preceding financial year, whichever is higher. 
5a. In case of SMEs, including start-ups, each fine referred to in this Article shall be up to the  percentages or amount referred to paragraphs 3, 4 and 5, whichever of the two is lower. 
6. When deciding whether to impose an administrative fine and on the amount of the  administrative fine in each individual case, all relevant circumstances of the specific  situation shall be taken into account and, as appropriate, regard shall be given to the  following: 
(a) the nature, gravity and duration of the infringement and of its consequences, taking  into account the purpose of the AI system, as well as, where appropriate, the number  of affected persons and the level of damage suffered by them; 
(b) whether administrative fines have been already applied by other market surveillance  authorities of one or more Member States to the same operator for the same  infringement; 
(ba) whether administrative fines have been already applied by other authorities to the  same operator for infringements of other Union or national law, when such  infringements result from the same activity or omission constituting a relevant  infringement of this Act; 
(c) the size, the annual turnover and market share of the operator committing the  infringement;
 
(ca) any other aggravating or mitigating factor applicable to the circumstances of the  case, such as financial benefits gained, or losses avoided, directly or indirectly, from  the infringement; 
(ca) the degree of cooperation with the national competent authorities, in order to remedy  the infringement and mitigate the possible adverse effects of the infringement; 
(cb) the degree of responsibility of the operator taking into account the technical and  organisational measures implemented by them; 
(ce) the manner in which the infringement became known to the national competent  authorities, in particular whether, and if so to what extent, the operator notified the  infringement; 
(cf) the intentional or negligent character of the infringement; 
(cg) any action taken by the operator to mitigate the harm of damage suffered by the  affected persons. 
7. Each Member State shall lay down rules on to what extent administrative fines may be imposed on public authorities and bodies established in that Member State. 
8. Depending on the legal system of the Member States, the rules on administrative fines may  be applied in such a manner that the fines are imposed by competent national courts or  other bodies as applicable in those Member States. The application of such rules in those  Member States shall have an equivalent effect. 
8a. The exercise by the market surveillance authority of its powers under this Article shall be  subject to appropriate procedural safeguards in accordance with Union and Member State  law, including effective judicial remedy and due process. 
8b. Member States shall, on an annual basis, report to the Commission about the  administrative fines they have issued during that year, in accordance with this Article, and  any related litigation or judicial proceedings;
 
Article 72 
Administrative fines on Union institutions, agencies and bodies 
1. The European Data Protection Supervisor may impose administrative fines on Union  institutions, agencies and bodies falling within the scope of this Regulation. When deciding  whether to impose an administrative fine and deciding on the amount of the administrative  fine in each individual case, all relevant circumstances of the specific situation shall be  taken into account and due regard shall be given to the following: 
(a) the nature, gravity and duration of the infringement and of its consequences, taking  into account the purpose of the AI system concerned as well as the number of  affected persons and the level of damage suffered by them, and any relevant previous  infringement; 
(aa) the degree of responsibility of the Union institution, agency or body, taking into  account technical and organisational measures implemented by them; 
(ab) any action taken by the Union institution, agency or body to mitigate the damage  suffered by affected persons; 
(b) the degree of cooperation with the European Data Protection Supervisor in order to  remedy the infringement and mitigate the possible adverse effects of the  
infringement, including compliance with any of the measures previously ordered by  the European Data Protection Supervisor against the Union institution or agency or  body concerned with regard to the same subject matter; 
(c) any similar previous infringements by the Union institution, agency or body; 
(ca) the manner in which the infringement became known to the European Data  Protection Supervisor, in particular whether, and if so to what extent, the Union  institution or body notified the infringement; 
(cb) the annual budget of the body. 
2. Non-compliance with the prohibition of the artificial intelligence practices referred to in  Article 5 shall be subject to administrative fines of up to EUR 1 500 000.
 
3. Non-compliance of the AI system with any requirements or obligations under this  Regulation, other than those laid down in Articles 5, shall be subject to administrative fines  of up to EUR 750 000. 
4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor  shall give the Union institution, agency or body which is the subject of the proceedings  conducted by the European Data Protection Supervisor the opportunity of being heard on  the matter regarding the possible infringement. The European Data Protection Supervisor  shall base his or her decisions only on elements and circumstances on which the parties  concerned have been able to comment. Complainants, if any, shall be associated closely  with the proceedings. 
5. The rights of defence of the parties concerned shall be fully respected in the proceedings.  They shall be entitled to have access to the European Data Protection Supervisor’s file,  subject to the legitimate interest of individuals or undertakings in the protection of their  personal data or business secrets. 
6. Funds collected by imposition of fines in this Article shall contribute to the general budget  of the Union. The fines shall not affect the effective operation of the Union institution,  body or agency fined. 
6a. The European Data Protection Supervisor shall, on an annual basis, notify the Commission  of the administrative fines it has imposed pursuant to this Article and any litigation or  judicial proceedings.  
Article 72a 
Fines for providers of general purpose AI models 
1. The Commission may impose on providers of general purpose AI models fines not  exceeding 3% of its total worldwide turnover in the preceding financial year or 15  million EUR whichever is higher. Fines should be imposed one year after the entry into  application of the relevant provisions in this Regulation in order to allow providers  sufficient time to adapt when the Commission finds that the provider intentionally or  negligently: 
(a) infringes the relevant provisions of this Regulation; 
(b) fails to comply with a request for document or information pursuant to 
 
Article 68i [Power to request documentation and information], or supply  
of incorrect, incomplete or misleading information; 
(b) fails to comply with a measure requested under Article 68k [Power to request  measures]; 
(c) fails to make available to the Commission access to the general purpose AI  model or general purpose AI model with systemic risk with a view to  
conduct an evaluation pursuant to Article 68j [Power to conduct  
evaluations]. 
In fixing the amount of the fine or periodic penalty payment, regard shall be had to the  nature, gravity and duration of the infringement, taking due account of the principles of  proportionality and appropriateness. The Commission shall also into account commitments  made in accordance with Article 68k(3) or in relevant codes of practice in accordance with  Article 52e [Codes of practice]. 
2. Before adopting the decision pursuant to paragraph 1 of this Article, the Commission shall  communicate its preliminary findings to the provider of the general purpose AI model or  general purpose AI model with systemic risk and give opportunity to be heard.  
2a. Fines imposed in accordance with this article shall be proportionate, dissuasive and  effective. 
2b. The information on the fines shall be also communicated to the Board as appropriate. 
3. The Court of Justice of the European Union shall have unlimited jurisdiction to review  decisions whereby the Commission has fixed a fine. It may cancel, reduce or increase the  fine imposed. 
4. The Commission shall adopt implementing acts concerning the modalities and practical  arrangements for the proceedings in view of possible adoptions of decisions pursuant to  paragraph 1. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2).
 
TITLE XI 
DELEGATION OF POWER AND COMMITTEE PROCEDURE  
Article 73 
Exercise of the delegation 
1. The power to adopt delegated acts is conferred on the Commission subject to the  conditions laid down in this Article. 
2. The power to adopt delegated acts referred to in [Article 4, Article 7(1), Article 11(3),  Article 43(5) and (6) and Article 48(5)] shall be conferred on the Commission for a period  of five years from … [the date of entry into force of the Regulation].The Commission shall  draw up a report in respect of the delegation of power not later than 9 months before the  end of the five-year period. The delegation of power shall be tacitly extended for periods of  an identical duration, unless the European Parliament or the Council opposes such  extension not later than three months before the end of each period. 
3. The delegation of power referred to in {Article 7(1), Article 7(3), Article 11(3), Article  43(5) and (6) and Article 48(5)] may be revoked at any time by the European Parliament or  by the Council. A decision of revocation shall put an end to the delegation of power  specified in that decision. It shall take effect the day following that of its publication in the  Official Journal of the European Union or at a later date specified therein. It shall not  affect the validity of any delegated acts already in force. 
4. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the  European Parliament and to the Council. 
5. Any delegated act adopted pursuant to [Article 4], Article 7(1), Article 11(3), Article 43(5)  and (6) and Article 48(5) shall enter into force only if no objection has been expressed by  either the European Parliament or the Council within a period of three months of  notification of that act to the European Parliament and the Council or if, before the expiry  of that period, the European Parliament and the Council have both informed the  Commission that they will not object. That period shall be extended by three months at the initiative of the European Parliament or of the Council.
 
Article 74 
Committee procedure 
1. The Commission shall be assisted by a committee. That committee shall be a committee  within the meaning of Regulation (EU) No 182/2011. 
2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No 182/2011 shall  apply. 
TITLE XII 
FINAL PROVISIONS  
Article 75 
Amendment to Regulation (EC) No 300/2008 
In Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added: 
“ When adopting detailed measures related to technical specifications and procedures  for approval and use of security equipment concerning Artificial Intelligence systems  in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the  European Parliament and of the Council*, the requirements set out in Chapter 2, Title  III of that Regulation shall be taken into account.” 
__________ 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”“ 
Article 76 
Amendment to Regulation (EU) No 167/2013 
In Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added: 
“ When adopting delegated acts pursuant to the first subparagraph concerning artificial  intelligence systems which are safety components in the meaning of Regulation (EU) 
 
YYY/XX [on Artificial Intelligence] of the European Parliament and of the  Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be  taken into account. 
__________ 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).” 
Article 77 
Amendment to Regulation (EU) No 168/2013 
In Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added: 
“ When adopting delegated acts pursuant to the first subparagraph concerning  Artificial Intelligence systems which are safety components in the meaning of  Regulation (EU) YYY/XX on [Artificial Intelligence] of the European Parliament  and of the Council*, the requirements set out in Title III, Chapter 2 of that  Regulation shall be taken into account. 
__________ 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).” 
Article 78 
Amendment to Directive 2014/90/EU 
In Article 8 of Directive 2014/90/EU, the following paragraph is added: “4. 
“For Artificial Intelligence systems which are safety components in the meaning of  Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament and of the  Council*, when carrying out its activities pursuant to paragraph 1 and when adopting  technical specifications and testing standards in accordance with paragraphs 2 and 3, the  Commission shall take into account the requirements set out in Title III, Chapter 2 of that  Regulation.
 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”. 
__________” 
Article 79 
Amendment to Directive (EU) 2016/797 
In Article 5 of Directive (EU) 2016/797, the following paragraph is added: “12. 
“When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to  paragraph 11 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European Parliament  and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be  taken into account. 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”. 
__________” 
Article 80 
Amendment to Regulation (EU) 2018/858 
In Article 5 of Regulation (EU) 2018/858 the following paragraph is added: “4. 
“When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence  systems which are safety components in the meaning of Regulation (EU) YYY/XX [on  Artificial Intelligence] of the European Parliament and of the Council *, the requirements set  out in Title III, Chapter 2 of that Regulation shall be taken into account. 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).”. 
__________”
 
Article 81 
Amendment to Regulation (EU) 2018/1139 
Regulation (EU) 2018/1139 is amended as follows: 
(1) In Article 17, the following paragraph is added: 
“3. 
“Without prejudice to paragraph 2, when adopting implementing acts pursuant to  paragraph 1 concerning Artificial Intelligence systems which are safety components in  the meaning of Regulation (EU) YYY/XX [on Artificial Intelligence] of the European  Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that  Regulation shall be taken into account. 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).” 
__________” 
(2) In Article 19, the following paragraph is added: 
“4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning  Artificial Intelligence systems which are safety components in the meaning of  Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out  in Title III, Chapter 2 of that Regulation shall be taken into account.” 
(3) In Article 43, the following paragraph is added: 
“4. When adopting implementing acts pursuant to paragraph 1 concerning  Artificial Intelligence systems which are safety components in the meaning of  Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out  in Title III, Chapter 2 of that Regulation shall be taken into account.” 
(4) In Article 47, the following paragraph is added: 
“3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning  Artificial Intelligence systems which are safety components in the meaning of 
 
Regulation (EU) YYY/XX [on Artificial Intelligence], the requirements set out  in Title III, Chapter 2 of that Regulation shall be taken into account.” 
(5) In Article 57, the following paragraph is added: 
“ When adopting those implementing acts concerning Artificial Intelligence  systems which are safety components in the meaning of Regulation (EU)  YYY/XX [on Artificial Intelligence], the requirements set out in Title III,  Chapter 2 of that Regulation shall be taken into account.” 
(6) In Article 58, the following paragraph is added: 
“3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning  Artificial Intelligence systems which are safety components in the meaning of  Regulation (EU) YYY/XX [on Artificial Intelligence] , the requirements set  out in Title III, Chapter 2 of that Regulation shall be taken into account..” 
Article 82 
Amendment to Regulation (EU) 2019/2144 
In Article 11 of Regulation (EU) 2019/2144, the following paragraph is added: ”3. 
“When adopting the implementing acts pursuant to paragraph 2, concerning artificial  intelligence systems which are safety components in the meaning of Regulation (EU)  YYY/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the  requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 
* Regulation (EU) YYY/XX [on Artificial Intelligence] (OJ …).. 
__________”
 
Article 82a 
Guidelines from the Commission on the implementation of this Regulation 
1. The Commission shall develop guidelines on the practical implementation of this  Regulation, and in particular on:  
(a) the application of the requirements and obligations referred to in Articles 8 - 15 and  Article 28; 
(b) the prohibited practices referred to in Article 5; 
(c) the practical implementation of the provisions related to substantial modification; (d) the practical implementation of transparency obligations laid down in Article 52;  
(e) detailed information on the relationship of this Regulation with the legislation  referred to in Annex II of this Regulation as well as other relevant Union law,  including as regards consistency in their enforcement; 
(f) the application of the definition of an AI system as set out in Article 3(1). 
When issuing such guidelines, the Commission shall pay particular attention to the needs  of SMEs including start-ups, local public authorities and sectors most likely to be affected  by this Regulation.  
The guidelines referred to in the first subparagraph shall take due account of the generally  acknowledged state of the art on AI, as well as of relevant harmonised standards and  common specifications that are referred to in Articles 40 and 41, or of those harmonised  standards or technical specifications that are set out pursuant to Union harmonisation law. 
2. Upon request of the Member States or the AI Office, or on its own initiative, the  Commission shall update already adopted guidelines when deemed necessary. 
 
Article 83 
AI systems already placed on the market or put into service 
1. Without prejudice to the application of Article 5 as referred in Article 85 (3) (-aa) AI  systems which are components of the large-scale IT systems established by the legal acts  listed in Annex IX that have been placed on the market or put into service before 12  months after the date of application of this Regulation referred to in Article 85(2) shall be  brought into compliance with this Regulation by end of 2030. 
The requirements laid down in this Regulation shall be taken into account in the evaluation  of each large-scale IT systems established by the legal acts listed in Annex IX to be  undertaken as provided for in those respective acts and whenever those legal acts are  replaced or amended. 
2. Without prejudice to the application of Article 5 as referred in Article 85 (3) (-aa ) this  Regulation shall apply to operators of high-risk AI systems, other than the ones referred to  in paragraph 1, that have been placed on the market or put into service before [date of  application of this Regulation referred to in Article 85(2)], only if, from that date, those  systems are subject to significant changes in their designs. In the case of high-risk AI  systems intended to be used by public authorities, providers and deployers of such systems  shall take the necessary steps to comply with the requirements of the present Regulation  four years after the date of entry into application of this Regulation. 
3. Providers of general purpose AI models that have been placed on the market before [date  of application of this Regulation referred to in point a) Article 85(3)] shall take the  necessary steps in order to comply with the obligations laid down in this Regulation by [2  years after the date of entry into application of this Regulation referred to in point a) of  85(3)]. 
Article 84 
Evaluation and review 
1. The Commission shall assess the need for amendment of the list in Annex III, the list of  prohibited AI practices in Article 5, once a year following the entry into force of this 
 
Regulation, and until the end of the period of the delegation of power. The Commission  shall submit the findings of that assessment to the European Parliament and the Council.  
2. By two years after the date of application of this Regulation referred to in Article 85(2) and  every four years thereafter, the Commission shall evaluate and report to the European  Parliament and to the Council on the need for amendment of the following:  
- the need for extension of existing area headings or addition of new area headings in  Annex III;  
- the list of AI systems requiring additional transparency measures in Article 52;  - the effectiveness of the supervision and governance system. 
2a. By three years after the date of application of this Regulation referred to in Article 85(3)  and every four years thereafter, the Commission shall submit a report on the evaluation and  review of this Regulation to the European Parliament and to the Council. This report shall  include an assessment with regard to the structure of enforcement and the possible need for  an Union agency to resolve any identified shortcomings. On the basis of the findings that  report shall, where appropriate, be accompanied by a proposal for amendment of this  Regulation. The reports shall be made public. 
3. The reports referred to in paragraph 2 shall devote specific attention to the following: 
(a) the status of the financial, technical and human resources of the national competent  authorities in order to effectively perform the tasks assigned to them under this  Regulation; 
(b) the state of penalties, and notably administrative fines as referred to in Article 71(1),  applied by Member States to infringements of the provisions of this Regulation; 
(ba) adopted harmonised standards and common specifications developed to support this  Regulation; 
(bb) the number of companies that enter the market after the enter into application of the  regulation and how many of them are SMEs. 
3a. By ... [two years after the date of entry into application of this Regulation referred to in  Article 85(2)] the Commission shall evaluate the functioning of the AI office, whether the 
 
office has been given sufficient powers and competences to fulfil its tasks and whether it  would be relevant and needed for the proper implementation and enforcement of this  Regulation to upgrade the Office and its enforcement competences and to increase its  resources. The Commission shall submit this evaluation report to the European Parliament  and to the Council. 
3a. By two years [after the date of application of this Regulation referred to in Article 85(2)]  and every four years thereafter, the Commission shall submit a report on the review of the  progress on the development of standardization deliverables on energy efficient  development of general-purpose models and asses the need for further measures or actions,  including binding measures or actions. The report shall be submitted to the European  Parliament and to the Council and it shall be made public. 
4. Within … [two years after the date of application of this Regulation referred to in Article  85(2)] and every three years thereafter, the Commission shall evaluate the impact and  effectiveness of voluntary codes of conduct to foster the application of the requirements set  out in Title III, Chapter 2 for AI systems other than high-risk AI systems and possibly  other additional requirements for AI systems other than high-risk AI systems, including as  regards environmental sustainability. 
5. For the purpose of paragraphs 1 to 4 the Board, the Member States and national competent  authorities shall provide the Commission with information on its request, without undue  delay. 
6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the  Commission shall take into account the positions and findings of the Board, of the  European Parliament, of the Council, and of other relevant bodies or sources. 
7. The Commission shall, if necessary, submit appropriate proposals to amend this  Regulation, in particular taking into account developments in technology, the effect of AI  systems on health and safety, fundamental rights and in the light of the state of progress in  the information society. 
7a. To guide the evaluations and reviews referred to in paragraphs 1 to 4 of this Article, the  Office shall undertake to develop an objective and participative methodology for the  evaluation of risk level based on the criteria outlined in the relevant articles and inclusion  of new systems in: the list in Annex III, including the extension of existing area headings 
 
or addition of new area headings in that Annex; the list of prohibited practices laid down in  Article 5; and the list of AI systems requiring additional transparency measures pursuant to  Article 52. 
7b. Any amendment to this Regulation pursuant to paragraph 7 of this Article, or relevant  future delegated or implementing acts, which concern sectoral legislation listed in Annex II  Section B, shall take into account the regulatory specificities of each sector, and existing  governance, conformity assessment and enforcement mechanisms and authorities  established therein. 
7c. By … [five years from the date of application of this Regulation], the Commission shall  carry out an assessment of the enforcement of this Regulation and shall report it to the  European Parliament, the Council and the European Economic and Social Committee,  taking into account the first years of application of the Regulation. On the basis of the  findings that report shall, where appropriate, be accompanied by a proposal for amendment  of this Regulation with regard to the structure of enforcement and the need for an Union  agency to resolve any identified shortcomings. 
Article 85 
Entry into force and application 
1. This Regulation shall enter into force on the twentieth day following that of its publication  in the Official Journal of the European Union. 
2. This Regulation shall apply from [24 months following the entering into force of the  Regulation]. With regard to the obligation referred to in Article 53(1), this obligation shall  include either that at least one regulatory sandbox per Member State shall be operational  on this day or that the Member State participates in the sandbox of another Member State * 
3. By way of derogation from paragraph 2:  
(-a) Title I and II [Prohibitions] shall apply from [six months following the entry  into force of this Regulation];  
(a) Title III Chapter 4, Title VI, Title VIIIa [GPAI], Title X [Penalties] shall  apply from [twelve months following the entry into force of this  
Regulation]; 
 
(b) Article 6(1) and the corresponding obligations in this Regulation shall apply  from [36 months following the entry into force of this Regulation].  
Codes of practices shall be ready at the latest nine months after the entry into force of this  Regulation. The AI Office shall take the necessary steps, including inviting providers  pursuant to Article 52e paragraph 5.  
This Regulation shall be binding in its entirety and directly applicable in all Member  States. 
Done at Brussels, 
For the European Parliament For the Council 
The President The President
 
ANNEX II 
List of Union harmonisation legislation 
Part I 
Section A. List of Union harmonisation legislation based on the New Legislative Framework 
1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on  machinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24) [as repealed by  the Machinery Regulation]; 
2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on  the safety of toys (OJ L 170, 30.6.2009, p. 1); 
3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November  2013 on recreational craft and personal watercraft and repealing Directive 94/25/EC (OJ L  354, 28.12.2013, p. 90); 
4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014  on the harmonisation of the laws of the Member States relating to lifts and safety  components for lifts (OJ L 96, 29.3.2014, p. 251); 
5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014  on the harmonisation of the laws of the Member States relating to equipment and  protective systems intended for use in potentially explosive atmospheres (OJ L 96,  29.3.2014, p. 309); 
6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on  the harmonisation of the laws of the Member States relating to the making available on the  market of radio equipment and repealing Directive 1999/5/EC (OJ L 153, 22.5.2014, p.  62); 
7. Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on  the harmonisation of the laws of the Member States relating to the making available on the  market of pressure equipment (OJ L 189, 27.6.2014, p. 164);
 
8. Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016  on cableway installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1); 
9. Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016  on personal protective equipment and repealing Council Directive 89/686/EEC (OJ L 81,  31.3.2016, p. 51); 
10. Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016  on appliances burning gaseous fuels and repealing Directive 2009/142/EC (OJ L 81,  31.3.2016, p. 99); 
11. Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017  on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and  Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and  93/42/EEC (OJ L 117, 5.5.2017, p. 1; 
12. Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017  on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission  Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176). 
Part II 
Section B. List of other Union harmonisation legislation 
13. Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March  2008 on common rules in the field of civil aviation security and repealing Regulation (EC)  No 2320/2002 (OJ L 97, 9.4.2008, p. 72). 
14. Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15  January 2013 on the approval and market surveillance of two- or three-wheel vehicles and  quadricycles (OJ L 60, 2.3.2013, p. 52); 
15. Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5  February 2013 on the approval and market surveillance of agricultural and forestry  vehicles (OJ L 60, 2.3.2013, p. 1);
 
16. Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on  marine equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p.  146); 
17. Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016  on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016,  p. 44). 
18. Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018  on the approval and market surveillance of motor vehicles and their trailers, and of  systems, components and separate technical units intended for such vehicles, amending  Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC  (OJ L 151, 14.6.2018, p. 1);  
18a. Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27  November 2019 on type-approval requirements for motor vehicles and their trailers, and  systems, components and separate technical units intended for such vehicles, as regards  their general safety and the protection of vehicle occupants and vulnerable road users,  amending Regulation (EU) 2018/858 of the European Parliament and of the Council and  repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the  European Parliament and of the Council and Commission Regulations (EC) No 631/2009,  (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No  1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011,  (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No  1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1); 
19. Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018  on common rules in the field of civil aviation and establishing a European Union Aviation  Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU)  No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the  
European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and  (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation  (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1), in so far as the design, production and  placing on the market of aircrafts referred to in points (a) and (b) of Article 2(1) thereof, 
 
where it concerns unmanned aircraft and their engines, propellers, parts and equipment to  control them remotely, are concerned.
 
ANNEX IIa 
List of criminal offences referred to in Article 5 (1)(iii) 
- terrorism; 
- trafficking in human beings;  
- sexual exploitation of children and child pornography;  
- illicit trafficking in narcotic drugs and psychotropic substances;  - illicit trafficking in weapons, munitions and explosives;  
- murder, grievous bodily injury;  
- illicit trade in human organs and tissue;  
- illicit trafficking in nuclear or radioactive materials;  
- kidnapping, illegal restraint and hostage-taking;  
- crimes within the jurisdiction of the International Criminal Court;  - unlawful seizure of aircraft/ships;  
- rape;  
- environmental crime;  
- organised or armed robbery;  
- sabotage;  
- participation in a criminal organisation involved in one or more offences listed above.
 
ANNEX III 
High-risk AI systems referred to in article 6(2) 
High-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the  following areas: 
1. Biometrics, insofar as their use is permitted under relevant Union or national law: 
(a) Remote biometric identification systems. 
This shall not include AI systems intended to be used for biometric verification  whose sole purpose is to confirm that a specific natural person is the person he or she  claims to be; 
(aa) AI systems intended to be used for biometric categorisation, according to sensitive or  protected attributes or characteristics based on the inference of those attributes or  characteristics; 
(ab) AI systems intended to be used for emotion recognition. 
2. Critical infrastructure: 
(a) AI systems intended to be used as safety components in the management and  operation of critical digital infrastructure, road traffic and the supply of water, gas,  heating and electricity. 
3. Education and vocational training: 
(a) AI systems intended to be used to determine access or admission or to assign natural  persons to educational and vocational training institutions at all levels; 
(b) AI systems intended to be used to evaluate learning outcomes, including when those  outcomes are used to steer the learning process of natural persons in educational and  vocational training institutions at all levels; 
(ba) AI systems intended to be used for the purpose of assessing the appropriate level of  education that individual will receive or will be able to access, in the context  of/within education and vocational training institution;
 
(bb) AI systems intended to be used for monitoring and detecting prohibited behaviour of  students during tests in the context of/within education and vocational training  institutions.  
4. Employment, workers management and access to self-employment: 
(a) AI systems intended to be used for recruitment or selection of natural persons,  notably to place targeted job advertisements, to analyse and filter job applications,  and to evaluate candidates; 
(b) AI intended to be used to make decisions affecting terms of the work related  relationships, promotion and termination of work-related contractual relationships, to  allocate tasks based on individual behaviour or personal traits or characteristics and  to monitor and evaluate performance and behaviour of persons in such relationships. 
5. Access to and enjoyment of essential private services and essential public services and  benefits: 
(a) AI systems intended to be used by public authorities or on behalf of public  authorities to evaluate the eligibility of natural persons for essential public assistance  benefits and services, including healthcare services, as well as to grant, reduce,  revoke, or reclaim such benefits and services; 
(b) AI systems intended to be used to evaluate the creditworthiness of natural persons or  establish their credit score, with the exception of AI systems used for the purpose of  detecting financial fraud; 
(c) AI systems intended to evaluate and classify emergency calls by natural persons or to  be used to dispatch, or to establish priority in the dispatching of emergency first  response services, including by police, firefighters and medical aid, as well as of  emergency healthcare patient triage systems; 
(ca) AI systems intended to be used for risk assessment and pricing in relation to natural  persons in the case of life and health insurance. 
6. Law enforcement, insofar as their use is permitted under relevant Union or national law:
 
(a) AI systems intended to be used by or on behalf of law enforcement authorities, or by  Union institutions, agencies, offices or bodies in support of law enforcement  authorities or on their behalf to assess the risk of a natural person to become a victim  of criminal offences; 
(b) AI systems intended to be used by or on behalf of law enforcement authorities or by  Union institutions, bodies and agencies in support of Law enforcement authorities as  polygraphs and similar tools;  
(d) AI systems intended to be used by or on behalf of law enforcement authorities, or by  Union institutions, agencies, offices or bodies in support of law enforcement  authorities to evaluate the reliability of evidence in the course of investigation or  prosecution of criminal offences; 
(e) AI systems intended to be used by law enforcement authorities or on their behalf or  by Union institutions, agencies, offices or bodies in support of law enforcement  authorities for assessing the risk of a natural person of offending or re-offending not  solely based on profiling of natural persons as referred to in Article 3(4) of Directive  (EU) 2016/680 or to assess personality traits and characteristics or past criminal  behaviour of natural persons or groups; 
(f) AI systems intended to be used by or on behalf of law enforcement authorities or by  Union agencies institutions, agencies, offices or bodies in support of law  
enforcement authorities for profiling of natural persons as referred to in Article 3(4)  of Directive (EU) 2016/680 in the course of detection, investigation or prosecution of  criminal offences. 
7. Migration, asylum and border control management, insofar as their use is permitted under  relevant Union or national law: 
(a) AI systems intended to be used by competent public authorities as polygraphs and  similar tools;  
(b) AI systems intended to be used by or on behalf of competent public authorities or by  Union agencies, offices or bodies to assess a risk, including a security risk, a risk of  irregular migration, or a health risk, posed by a natural person who intends to enter or  has entered into the territory of a Member State;
 
(d) AI systems intended to be used by or on behalf of competent public authorities or by  Union agencies, offices or bodies to assist competent public authorities for the  examination of applications for asylum, visa and residence permits and associated  complaints with regard to the eligibility of the natural persons applying for a status,  including related assessment of the reliability of evidence; 
(da) AI systems intended to be used by or on behalf of competent public authorities,  including Union agencies, offices or bodies, in the context of migration, asylum and  border control management, for the purpose of detecting, recognising or identifying  natural persons with the exception of verification of travel documents.  
8. Administration of justice and democratic processes: 
(a) AI systems intended to be used by a judicial authority or on their behalf to assist a  judicial authority in researching and interpreting facts and the law and in applying  the law to a concrete set of facts or used in a similar way in alternative dispute  resolution; 
(aa) AI systems intended to be used for influencing the outcome of an election or  referendum or the voting behaviour of natural persons in the exercise of their vote in  elections or referenda. This does not include AI systems whose output natural  persons are not directly exposed to, such as tools used to organise, optimise and  structure political campaigns from an administrative and logistic point of view.
 
ANNEX IV 
Technical documentation referred to in article 11(1) 
The technical documentation referred to in Article 11(1) shall contain at least the following  information, as applicable to the relevant AI system: 
1. A general description of the AI system including: 
(a) its intended purpose, the name of the provider and the version of the system  reflecting its relation to previous versions; 
(b) how the AI system interacts or can be used to interact with hardware or software,  including other AI systems, that are not part of the AI system itself, where  
applicable; 
(c) the versions of relevant software or firmware and any requirement related to version  update; 
(d) the description of all forms in which the AI system is placed on the market or put  into service (e.g. software package embedded into hardware, downloadable, API  etc.);  
(e) the description of hardware on which the AI system is intended to run; 
(f) where the AI system is a component of products, photographs or illustrations  showing external features, marking and internal layout of those products; 
(fa) a basic description of the user-interface provided to the deployer; 
(g) instructions of use for the deployer and a basic description of the user-interface  provided to the deployer where applicable. 
2. A detailed description of the elements of the AI system and of the process for its  development, including: 
(a) the methods and steps performed for the development of the AI system, including,  where relevant, recourse to pre-trained systems or tools provided by third parties and  how these have been used, integrated or modified by the provider;
 
(b) the design specifications of the system, namely the general logic of the AI system  and of the algorithms; the key design choices including the rationale and assumptions  made, also with regard to persons or groups of persons on which the system is  intended to be used; the main classification choices; what the system is designed to  optimise for and the relevance of the different parameters; the description of the  expected output and output quality of the system; the decisions about any possible  trade-off made regarding the technical solutions adopted to comply with the  requirements set out in Title III, Chapter 2; 
(c) the description of the system architecture explaining how software components build  on or feed into each other and integrate into the overall processing; the computational  resources used to develop, train, test and validate the AI system; 
(d) where relevant, the data requirements in terms of datasheets describing the training  methodologies and techniques and the training data sets used, including a general  description of these data sets, information about their provenance, scope and main  characteristics; how the data was obtained and selected; labelling procedures (e.g. for  supervised learning), data cleaning methodologies (e.g. outliers detection); 
(e) assessment of the human oversight measures needed in accordance with Article 14,  including an assessment of the technical measures needed to facilitate the  interpretation of the outputs of AI systems by the deployers, in accordance with  Articles 13(3)(d); 
(f) where applicable, a detailed description of pre-determined changes to the AI system  and its performance, together with all the relevant information related to the technical  solutions adopted to ensure continuous compliance of the AI system with the  relevant requirements set out in Title III, Chapter 2; 
(g) the validation and testing procedures used, including information about the validation  and testing data used and their main characteristics; metrics used to measure  accuracy, robustness and compliance with other relevant requirements set out in Title  III, Chapter 2 as well as potentially discriminatory impacts; test logs and all test  reports dated and signed by the responsible persons, including with regard to pre determined changes as referred to under point (f);
 
(ga) cybersecurity measures put in place.  
3. Detailed information about the monitoring, functioning and control of the AI system, in  particular with regard to: its capabilities and limitations in performance, including the  degrees of accuracy for specific persons or groups of persons on which the system is  intended to be used and the overall expected level of accuracy in relation to its intended  purpose; the foreseeable unintended outcomes and sources of risks to health and safety,  fundamental rights and discrimination in view of the intended purpose of the AI system;  the human oversight measures needed in accordance with Article 14, including the  technical measures put in place to facilitate the interpretation of the outputs of AI systems  by the deployers; specifications on input data, as appropriate; 
3. A description of the appropriateness of the performance metrics for the specific AI system; 4. A detailed description of the risk management system in accordance with Article 9; 5. A description of relevant changes made by the provider to the system through its lifecycle; 
6. A list of the harmonised standards applied in full or in part the references of which have  been published in the Official Journal of the European Union; where no such harmonised  standards have been applied, a detailed description of the solutions adopted to meet the  requirements set out in Title III, Chapter 2, including a list of other relevant standards and  technical specifications applied; 
7. A copy of the EU declaration of conformity; 
8. A detailed description of the system in place to evaluate the AI system performance in the  post-market phase in accordance with Article 61, including the post-market monitoring  plan referred to in Article 61(3).
 
ANNEX V 
EU declaration of conformity 
The EU declaration of conformity referred to in Article 48, shall contain all of the  following information: 
1. AI system name and type and any additional unambiguous reference allowing  identification and traceability of the AI system; 
2. Name and address of the provider or, where applicable, their authorised representative; 
3. A statement that the EU declaration of conformity is issued under the sole responsibility of  the provider; 
4. A statement that the AI system in question is in conformity with this Regulation and, if  applicable, with any other relevant Union legislation that provides for the issuing of an EU  declaration of conformity; 
4a. Where an AI system involves the processing of personal data, a statement that that AI  system complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU)  2016/680;  
5. References to any relevant harmonised standards used or any other common specification  in relation to which conformity is declared; 
6. Where applicable, the name and identification number of the notified body, a description  of the conformity assessment procedure performed and identification of the certificate  issued; 
7. Place and date of issue of the declaration, name and function of the person who signed it as  well as an indication for, and on behalf of whom, that person signed, signature.
 
ANNEX VI 
Conformity assessment procedure based on internal control 
1. The conformity assessment procedure based on internal control is the conformity  assessment procedure based on points 2 to 4. 
2. The provider verifies that the established quality management system is in compliance with  the requirements of Article 17. 
3. The provider examines the information contained in the technical documentation in order  to assess the compliance of the AI system with the relevant essential requirements set out  in Title III, Chapter 2. 
4. The provider also verifies that the design and development process of the AI system and its  post-market monitoring as referred to in Article 61 is consistent with the technical  documentation.
 
ANNEX VII 
Conformity based on assessment of quality management system and assessment of technical  documentation 
1. Introduction 
Conformity based on assessment of quality management system and assessment of the  technical documentation is the conformity assessment procedure based on points 2 to 5. 
2. Overview 
The approved quality management system for the design, development and testing of AI  systems pursuant to Article 17 shall be examined in accordance with point 3 and shall be  subject to surveillance as specified in point 5. The technical documentation of the AI system  shall be examined in accordance with point 4. 
3. Quality management system 
3.1. The application of the provider shall include: 
(a) the name and address of the provider and, if the application is lodged by the  authorised representative, their name and address as well; 
(b) the list of AI systems covered under the same quality management system; 
(c) the technical documentation for each AI system covered under the same quality  management system; 
(d) the documentation concerning the quality management system which shall cover all  the aspects listed under Article 17; 
(e) a description of the procedures in place to ensure that the quality management system  remains adequate and effective; 
(f) a written declaration that the same application has not been lodged with any other  notified body.
 
3.2. The quality management system shall be assessed by the notified body, which shall  determine whether it satisfies the requirements referred to in Article 17. 
The decision shall be notified to the provider or its authorised representative. 
The notification shall contain the conclusions of the assessment of the quality management  system and the reasoned assessment decision. 
3.3. The quality management system as approved shall continue to be implemented and  maintained by the provider so that it remains adequate and efficient. 
3.4. Any intended change to the approved quality management system or the list of AI systems  covered by the latter shall be brought to the attention of the notified body by the provider. 
The proposed changes shall be examined by the notified body, which shall decide whether  the modified quality management system continues to satisfy the requirements referred to  in point 3.2 or whether a reassessment is necessary. 
The notified body shall notify the provider of its decision. The notification shall contain  the conclusions of the examination of the changes and the reasoned assessment decision. 
4. Control of the technical documentation 
4.1. In addition to the application referred to in point 3, an application with a notified body of  their choice shall be lodged by the provider for the assessment of the technical  documentation relating to the AI system which the provider intends to place on the market  or put into service and which is covered by the quality management system referred to  under point 3. 
4.2. The application shall include: 
(a) the name and address of the provider; 
(b) a written declaration that the same application has not been lodged with any other  notified body; 
(c) the technical documentation referred to in Annex IV.
 
4.3. The technical documentation shall be examined by the notified body. Where relevant and  limited to what is necessary to fulfil their tasks, the notified body shall be granted full  access to the training, validation, and testing datasets used, including, where appropriate  and subject to security safeguards, through application programming interfaces (API) or  other relevant technical means and tools enabling remote access. 
4.4. In examining the technical documentation, the notified body may require that the provider  supplies further evidence or carries out further tests so as to enable a proper assessment of  conformity of the AI system with the requirements set out in Title III, Chapter 2.  Whenever the notified body is not satisfied with the tests carried out by the provider, the  notified body shall directly carry out adequate tests, as appropriate. 
4.5. Where necessary to assess the conformity of the high-risk AI system with the requirements  set out in Title III, Chapter 2, after all other reasonable ways to verify conformity have  been exhausted and have proven to be insufficient, and upon a reasoned request, the  notified body shall also be granted access to the training and trained models of the AI  system, including its relevant parameters. Such access shall be subject to existing Union  law on the protection of intellectual property and trade secrets.  
4.6. The decision shall be notified to the provider or its authorised representative. The  notification shall contain the conclusions of the assessment of the technical documentation  and the reasoned assessment decision. 
Where the AI system is in conformity with the requirements set out in Title III, Chapter 2,  an EU technical documentation assessment certificate shall be issued by the notified body.  The certificate shall indicate the name and address of the provider, the conclusions of the  examination, the conditions (if any) for its validity and the data necessary for the  identification of the AI system. 
The certificate and its annexes shall contain all relevant information to allow the  conformity of the AI system to be evaluated, and to allow for control of the AI system  while in use, where applicable. 
Where the AI system is not in conformity with the requirements set out in Title III, Chapter  2, the notified body shall refuse to issue an EU technical documentation assessment 
 
certificate and shall inform the applicant accordingly, giving detailed reasons for its  refusal. 
Where the AI system does not meet the requirement relating to the data used to train it, re training of the AI system will be needed prior to the application for a new conformity  assessment. In this case, the reasoned assessment decision of the notified body refusing to  issue the EU technical documentation assessment certificate shall contain specific  considerations on the quality data used to train the AI system, notably on the reasons for  non-compliance. 
4.7. Any change to the AI system that could affect the compliance of the AI system with the  requirements or its intended purpose shall be approved by the notified body which issued  the EU technical documentation assessment certificate. The provider shall inform such  notified body of its intention to introduce any of the above-mentioned changes or if it  becomes otherwise aware of the occurrence of such changes. The intended changes shall  be assessed by the notified body which shall decide whether those changes require a new  conformity assessment in accordance with Article 43(4) or whether they could be  addressed by means of a supplement to the EU technical documentation assessment  certificate. In the latter case, the notified body shall assess the changes, notify the provider  of its decision and, where the changes are approved, issue to the provider a supplement to  the EU technical documentation assessment certificate. 
5. Surveillance of the approved quality management system 
5.1. The purpose of the surveillance carried out by the notified body referred to in Point 3 is to  make sure that the provider duly fulfils the terms and conditions of the approved quality  management system. 
5.2. For assessment purposes, the provider shall allow the notified body to access the premises  where the design, development, testing of the AI systems is taking place. The provider  shall further share with the notified body all necessary information. 
5.3. The notified body shall carry out periodic audits to make sure that the provider maintains  and applies the quality management system and shall provide the provider with an audit  report. In the context of those audits, the notified body may carry out additional tests of the  AI systems for which an EU technical documentation assessment certificate was issued.
 
ANNEX VIII 
Information to be submitted upon the registration of high-risk AI systems in accordance with  Article 51 
SECTION A - Information to be submitted by providers of high-risk AI systems in  accordance with Article 51(1) 
The following information shall be provided and thereafter kept up to date with regard to  high-risk AI systems to be registered in accordance with Article 51(1): 
1. Name, address and contact details of the provider; 
2. Where submission of information is carried out by another person on behalf of the  provider, the name, address and contact details of that person; 
3. Name, address and contact details of the authorised representative, where applicable; 
4. AI system trade name and any additional unambiguous reference allowing identification  and traceability of the AI system; 
5. Description of the intended purpose of the AI system and of the components and functions  supported through this AI system; 
5a. A basic and concise description of the information used by the system (data, inputs) and its  operating logic; 
6. Status of the AI system (on the market, or in service; no longer placed on the market/in  service, recalled); 
7. Type, number and expiry date of the certificate issued by the notified body and the name or  identification number of that notified body, when applicable; 
8. A scanned copy of the certificate referred to in point 7, when applicable; 
9. Member States in which the AI system is or has been placed on the market, put into service  or made available in the Union; 
10. A copy of the EU declaration of conformity referred to in Article 48;
 
11. Electronic instructions for use; this information shall not be provided for high-risk AI  systems in the areas of law enforcement and migration, asylum and border control  management referred to in Annex III, points 1, 6 and 7. 
12. URL for additional information (optional). 
SECTION B - Information to be submitted by deployers of high-risk AI systems in  accordance with Article 51(1b) 
The following information shall be provided and thereafter kept up to date with regard to  high-risk AI systems to be registered in accordance with Article 51:  
1. The name, address and contact details of the deployer;  
2. The name, address and contact details of the person submitting information on behalf of  the deployer;  
5. A summary of the findings of the fundamental rights impact assessment conducted in  accordance with Article 29a;  
6. The URL of the entry of the AI system in the EU database by its provider;  
7. A summary of the data protection impact assessment carried out in accordance with Article  35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680 as specified in  paragraph 6 of Article 29 of this Regulation, where applicable. 
SECTION C - Information to be submitted by providers of high-risk AI systems in  accordance with Article 51(1a) 
The following information shall be provided and thereafter kept up to date with regard to AI  systems to be registered in accordance with Article 51(1a). 
1. Name, address and contact details of the provider; 
 
1. Where submission of information is carried out by another person on behalf of the  provider, the name, address and contact details of that person;  
2. Name, address and contact details of the authorised representative, where applicable;  
3. AI system trade name and any additional unambiguous reference allowing identification  and traceability of the AI system;  
4. Description of the intended purpose of the AI system;  
5. Based on which criterion or criteria provided in Article 6(2a) the AI system is considered  as not high-risk;  
6. Short summary of the grounds for considering the AI system as not high-risk in application  of the procedure under Article 6(2a);  
7. Status of the AI system (on the market, or in service; no longer placed on the market/in  service, recalled); Member States in which the AI system is or has been placed on the  market, put into service or made available in the Union.
 
ANNEX VIIIa 
Information to be submitted upon the registration of high-risk ai systems listed in annex iii in  relation to testing in real world conditions in accordance with Article 54a 
The following information shall be provided and thereafter kept up to date with regard to  testing in real world conditions to be registered in accordance with Article 54a: 
1. Union-wide unique single identification number of the testing in real world conditions; 
2. Name and contact details of the provider or prospective provider and users involved in the  testing in real world conditions;  
3. A brief description of the AI system, its intended purpose and other information necessary  for the identification of the system;  
4. A summary of the main characteristics of the plan for testing in real world conditions;  5. Information on the suspension or termination of the testing in real world conditions.
 
ANNEX IX 
Union legislation on large-scale IT systems in the area of Freedom, Security and Justice 1. Schengen Information System 
(a) Regulation (EU) 2018/1860 of the European Parliament and of the Council of 28  November 2018 on the use of the Schengen Information System for the return of  illegally staying third-country nationals (OJ L 312, 7.12.2018, p. 1). 
(b) Regulation (EU) 2018/1861 of the European Parliament and of the Council of 28  November 2018 on the establishment, operation and use of the Schengen Information  System (SIS) in the field of border checks, and amending the Convention  
implementing the Schengen Agreement, and amending and repealing Regulation  (EC) No 1987/2006 (OJ L 312, 7.12.2018, p. 14). 
(c) Regulation (EU) 2018/1862 of the European Parliament and of the Council of 28  November 2018 on the establishment, operation and use of the Schengen Information  System (SIS) in the field of police cooperation and judicial cooperation in criminal  matters, amending and repealing Council Decision 2007/533/JHA, and repealing  Regulation (EC) No 1986/2006 of the European Parliament and of the Council and  Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56). 
2. Visa Information System 
(a) Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF  THE COUNCIL amending Regulation (EC) No 767/2008, Regulation (EC) No  810/2009, Regulation (EU) 2017/2226, Regulation (EU) 2016/399, Regulation XX/2018 [Interoperability Regulation], and Decision 2004/512/EC and repealing  Council Decision 2008/633/JHA - COM(2018) 302 final. To be updated once the  Regulation is adopted (April/May 2021) by the co-legislators. 
3. Eurodac 
(a) Amended proposal for a REGULATION OF THE EUROPEAN PARLIAMENT  AND OF THE COUNCIL on the establishment of 'Eurodac' for the comparison of  biometric data for the effective application of Regulation (EU) XXX/XXX 
 
[Regulation on Asylum and Migration Management] and of Regulation (EU)  XXX/XXX [Resettlement Regulation], for identifying an illegally staying third country national or stateless person and on requests for the comparison with Eurodac  data by Member States' law enforcement authorities and Europol for law  
enforcement purposes and amending Regulations (EU) 2018/1240 and (EU)  2019/818 – COM(2020) 614 final. 
4. Entry/Exit System 
(a) Regulation (EU) 2017/2226 of the European Parliament and of the Council of 30  November 2017 establishing an Entry/Exit System (EES) to register entry and exit  data and refusal of entry data of third-country nationals crossing the external borders  of the Member States and determining the conditions for access to the EES for law  enforcement purposes, and amending the Convention implementing the Schengen  Agreement and Regulations (EC) No 767/2008 and (EU) No 1077/2011 (OJ L 327,  9.12.2017, p. 20). 
5. European Travel Information and Authorisation System 
(a) Regulation (EU) 2018/1240 of the European Parliament and of the Council of 12  September 2018 establishing a European Travel Information and Authorisation  System (ETIAS) and amending Regulations (EU) No 1077/2011, (EU) No 515/2014,  (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 19.9.2018, p. 1). 
(b) Regulation (EU) 2018/1241 of the European Parliament and of the Council of 12  September 2018 amending Regulation (EU) 2016/794 for the purpose of establishing  a European Travel Information and Authorisation System (ETIAS) (OJ L 236,  19.9.2018, p. 72). 
6. European Criminal Records Information System on third-country nationals and stateless  persons 
(a) Regulation (EU) 2019/816 of the European Parliament and of the Council of 17 April  2019 establishing a centralised system for the identification of Member States  holding conviction information on third-country nationals and stateless persons  (ECRIS-TCN) to supplement the European Criminal Records Information System  and amending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).
 
7. Interoperability 
(a) Regulation (EU) 2019/817 of the European Parliament and of the Council of 20 May  2019 on establishing a framework for interoperability between EU information  systems in the field of borders and visa (OJ L 135, 22.5.2019, p. 27). 
(b) Regulation (EU) 2019/818 of the European Parliament and of the Council of 20 May  2019 on establishing a framework for interoperability between EU information  systems in the field of police and judicial cooperation, asylum and migration (OJ L  135, 22.5.2019, p. 85).
 
ANNEX IXa 
Technical documentation referred to in Article 52c(1a): technical documentation for  providers of general purpose AI models: 
Section 1: Information to be provided by all providers of general-purpose AI  models  
The technical documentation referred to in Article X (b) shall contain at least the following  information as appropriate to the size and risk profile of the model:  
1. A general description of the general purpose AI model including: 
(a) the tasks that the model is intended to perform and the type and nature of AI systems  in which it can be integrated;  
(b) acceptable use policies applicable;  
(c) the date of release and methods of distribution;  
(d) the architecture and number of parameters;  
(e) modality (e.g. text, image) and format of inputs and outputs;  
(f) the license. 
2. A detailed description of the elements of the model referred to in paragraph 1, and  relevant information of the process for the development, including the  
following elements:  
(a) the technical means (e.g. instructions of use, infrastructure, tools) required for the  general-purpose AI model to be integrated in AI systems;  
(b) the design specifications of the model and training process, including training  methodologies and techniques, the key design choices including the rationale and  assumptions made; what the model is designed to optimise for and the relevance of  the different parameters, as applicable; 
 
(c) information on the data used for training, testing and validation, where applicable,  including type and provenance of data and curation methodologies (e.g. cleaning,  filtering etc), the number of data points, their scope and main characteristics; how the  data was obtained and selected as well as all other measures to detect the  unsuitability of data sources and methods to detect identifiable biases, where  applicable;  
(d) the computational resources used to train the model (e.g. number of floating point  operations – FLOPs), training time, and other relevant details related to the training; 
(e) known or estimated energy consumption of the model; in case not known, this could  be based on information about computational resources used ; 
Section 2: Additional information to be provided by providers of general purpose  AI model with systemic risk 
3. Detailed description of the evaluation strategies, including evaluation results, on the  basis of available public evaluation protocols and tools or otherwise of other  evaluation methodologies. Evaluation strategies shall include evaluation criteria,  metrics and the methodology on the identification of limitations. 
4. Where applicable, detailed description of the measures put in place for the purpose of  conducting internal and/or external adversarial testing (e.g. red teaming), model  adaptations, including alignment and fine-tuning. 
5. Where applicable, detailed description of the system architecture explaining how  software components build or feed into each other and integrate into the overall  processing.
 
ANNEX IXb 
Transparency information referred to in Article 52c(1b): technical documentation for  providers of general purpose AI models to downstream providers that integrate the model  into their AI system  
The information referred to in Article 52c shall contain at least the following:  1. A general description of the general purpose AI model including:  
(a) the tasks that the model is intended to perform and the type and nature of AI systems  in which it can be integrated;  
(b) acceptable use policies applicable;  
(c) the date of release and methods of distribution;  
(d) how the model interacts or can be used to interact with hardware or software that is  not part of the model itself, where applicable;  
(e) the versions of relevant software related to the use of the general purpose AI model,  where applicable;  
(f) architecture and number of parameters,  
(g) modality (e.g., text, image) and format of inputs and outputs;  
(h) the license for the model. 
2. A description of the elements of the model and of the process for its development,  including: 
(a) the technical means (e.g. instructions of use, infrastructure, tools) required for the  general-purpose AI model to be integrated in AI systems;  
(b) modality (e.g., text, image, etc.) and format of the inputs and outputs and their  maximum size (e.g., context window length, etc.); 
 
(c) information on the data used for training, testing and validation, where applicable,  including, type and provenance of data and curation methodologies.
 
ANNEX IXc 
Criteria for the designation of general purpose AI models with systemic risk referred to in  article 52a 
For the purpose of determining that a general purpose AI model has capabilities or impact  equivalent to those of points (a) and (b) in Article 52a, the Commission shall take into account the following criteria:  
(a) number of parameters of the model;  
(b) quality or size of the data set, for example measured through tokens;  
(c) the amount of compute used for training the model, measured in FLOPs or indicated  by a combination of other variables such as estimated cost of training, estimated time  required for the training, or estimated energy consumption for the training;  
(d) input and output modalities of the model, such as text to text (large language  models), text to image, multi-modality, and the state-of-the-art thresholds for  determining high-impact capabilities for each modality, and the specific type of  inputs and outputs (e.g. biological sequences);  
(e) benchmarks and evaluations of capabilities of the model, including considering the  number of tasks without additional training, adaptability to learn new, distinct tasks,  its degree of autonomy and scalability, the tools it has access to;  
(f) it has a high impact on the internal market due to its reach, which shall be presumed  when it has been made available to at least 10 000 registered business users  established in the Union;  
(g) number of registered end-users.