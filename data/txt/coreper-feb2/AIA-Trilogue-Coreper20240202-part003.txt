Fundamental rights impact assessment for high-risk AI systems 
1. Prior to deploying a high-risk AI system as defined in Article 6(2), with the exception of  AI systems intended to be used in the area listed in point 2 of Annex III, deployers that are  bodies governed by public law or private operators providing public services and operators  deploying high-risk systems referred to in Annex III, point 5, (b) and (ca) shall perform an  assessment of the impact on fundamental rights that the use of the system may produce. For  that purpose, deployers shall perform an assessment consisting of: 
(a) a description of the deployer’s processes in which the high-risk AI system will be  used in line with its intended purpose; 
(b) a description of the period of time and frequency in which each high-risk AI system  is intended to be used; 
(c) the categories of natural persons and groups likely to be affected by its use in the  specific context; 
(d) the specific risks of harm likely to impact the categories of persons or group of  persons identified pursuant point (c), taking into account the information given by  the provider pursuant to Article 13; 
(e) a description of the implementation of human oversight measures, according to the  instructions of use; 
(f) the measures to be taken in case of the materialization of these risks, including their  arrangements for internal governance and complaint mechanisms. 
2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system.  The deployer may, in similar cases, rely on previously conducted fundamental rights impact  assessments or existing impact assessments carried out by provider. If, during the use of the  high-risk AI system, the deployer considers that any of the factors listed in paragraph 1  change are or no longer up to date, the deployer will take the necessary steps to update the  information. 
3. Once the impact assessment has been performed, the deployer shall notify the market  surveillance authority of the results of the assessment, submitting the filled template referred  to in paragraph 5 as a part of the notification. In the case referred to in Article 47(1),  deployers may be exempted from these obligations.
 
4. If any of the obligations laid down in this article are already met through the data protection  impact assessment conducted pursuant to Article 35 of Regulation (EU) 2016/679 or Article  27 of Directive (EU) 2016/680, the fundamental rights impact assessment referred to in  paragraph 1 shall be conducted in conjunction with that data protection impact assessment. 
5. The AI Office shall develop a template for a questionnaire, including through an automated  tool, to facilitate deployers to implement the obligations of this Article in a simplified  manner. 
Chapter 4 
NOTIFIYING AUTHORITIES AND NOTIFIED BODIES 
Article 30 
Notifying authorities 
1. Each Member State shall designate or establish at least one notifying authority responsible  for setting up and carrying out the necessary procedures for the assessment, designation  and notification of conformity assessment bodies and for their monitoring. These  procedures shall be developed in cooperation between the notifying authorities of all  Member States. 
2. Member States may decide that the assessment and monitoring referred to in paragraph 1  shall be carried out by a national accreditation body within the meaning of and in  accordance with Regulation (EC) No 765/2008. 
3. Notifying authorities shall be established, organised and operated in such a way that no  conflict of interest arises with conformity assessment bodies and the objectivity and  impartiality of their activities are safeguarded. 
4. Notifying authorities shall be organised in such a way that decisions relating to the  notification of conformity assessment bodies are taken by competent persons different  from those who carried out the assessment of those bodies.
 
5. Notifying authorities shall not offer or provide any activities that conformity assessment  bodies perform or any consultancy services on a commercial or competitive basis. 
6. Notifying authorities shall safeguard the confidentiality of the information they obtain in  accordance with Article 70. 
7. Notifying authorities shall have an adequate number of competent personnel at their  disposal for the proper performance of their tasks. Competent personnel shall have the  necessary expertise, where applicable, for their function, in fields such as information  technologies, artificial intelligence and law, including the supervision of fundamental  rights. 
Article 31 
Application of a conformity assessment body for notification  
1. Conformity assessment bodies shall submit an application for notification to the notifying  authority of the Member State in which they are established. 
2. The application for notification shall be accompanied by a description of the conformity  assessment activities, the conformity assessment module or modules and the types of AI  systems for which the conformity assessment body claims to be competent, as well as by  an accreditation certificate, where one exists, issued by a national accreditation body  attesting that the conformity assessment body fulfils the requirements laid down in Article  33. Any valid document related to existing designations of the applicant notified body  under any other Union harmonisation legislation shall be added. 
3. Where the conformity assessment body concerned cannot provide an accreditation  certificate, it shall provide the notifying authority with all the documentary evidence  necessary for the verification, recognition and regular monitoring of its compliance with  the requirements laid down in Article 33. For notified bodies which are designated under  any other Union harmonisation legislation, all documents and certificates linked to those  designations may be used to support their designation procedure under this Regulation, as  appropriate. The notified body shall update the documentation referred to in paragraph 2  and paragraph 3 whenever relevant changes occur, in order to enable the authority 
 
responsible for notified bodies to monitor and verify continuous compliance with all the  requirements laid down in Article 33. 
Article 32 
Notification procedure 
1. Notifying authorities may only notify conformity assessment bodies which have satisfied  the requirements laid down in Article 33. 
2. Notifying authorities shall notify the Commission and the other Member States using the  electronic notification tool developed and managed by the Commission of each conformity  assessment body referred to in paragraph 1. 
3. The notification referred to in paragraph 2 shall include full details of the conformity  assessment activities, the conformity assessment module or modules and the types of AI  systems concerned and the relevant attestation of competence. Where a notification is not  based on an accreditation certificate as referred to in Article 31 (2), the notifying authority  shall provide the Commission and the other Member States with documentary evidence  which attests to the conformity assessment body's competence and the arrangements in  place to ensure that that body will be monitored regularly and will continue to satisfy the  requirements laid down in Article 33. 
4. The conformity assessment body concerned may perform the activities of a notified body  only where no objections are raised by the Commission or the other Member States within  two weeks of a notification by a notifying authority where it includes an accreditation  certificate referred to in Article 31(2), or within two months of a notification by the  notifying authority where it includes documentary evidence referred to in Article 31(3). 
4a. Where objections are raised, the Commission shall without delay enter into consultation  with the relevant Member States and the conformity assessment body. In view thereof, the  Commission shall decide whether the authorisation is justified or not. The Commission  shall address its decision to the Member State concerned and the relevant conformity  assessment body.
 
Article 33 
Requirements relating to notified bodies  
1. A notified body shall be established under national law of a Member State and have legal  personality. 
2. Notified bodies shall satisfy the organisational, quality management, resources and process  requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity  requirements. 
3. The organisational structure, allocation of responsibilities, reporting lines and operation of  notified bodies shall be such as to ensure that there is confidence in the performance by  and in the results of the conformity assessment activities that the notified bodies conduct. 
4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to  which it performs conformity assessment activities. Notified bodies shall also be  independent of any other operator having an economic interest in the high-risk AI system  that is assessed, as well as of any competitors of the provider. This shall not preclude the  use of assessed AI systems that are necessary for the operations of the conformity  assessment body or the use of such systems for personal purposes. 
4a. A conformity assessment body, its top-level management and the personnel responsible for  carrying out the conformity assessment tasks shall not be directly involved in the design,  development, marketing or use of high-risk AI systems, or represent the parties engaged in  those activities. They shall not engage in any activity that may conflict with their  independence of judgement or integrity in relation to conformity assessment activities for  which they are notified. This shall in particular apply to consultancy services. 
5. Notified bodies shall be organised and operated so as to safeguard the independence,  objectivity and impartiality of their activities. Notified bodies shall document and  implement a structure and procedures to safeguard impartiality and to promote and apply  the principles of impartiality throughout their organisation, personnel and assessment  activities. 
6. Notified bodies shall have documented procedures in place ensuring that their personnel,  committees, subsidiaries, subcontractors and any associated body or personnel of external 
 
bodies respect the confidentiality of the information in accordance with Article 70 which  comes into their possession during the performance of conformity assessment activities,  except when disclosure is required by law. The staff of notified bodies shall be bound to  observe professional secrecy with regard to all information obtained in carrying out their  tasks under this Regulation, except in relation to the notifying authorities of the Member  State in which their activities are carried out. 
7. Notified bodies shall have procedures for the performance of activities which take due  account of the size of an undertaking, the sector in which it operates, its structure, the  degree of complexity of the AI system in question. 
8. Notified bodies shall take out appropriate liability insurance for their conformity  assessment activities, unless liability is assumed by the Member State in which they are  established in accordance with national law or that Member State is itself directly  responsible for the conformity assessment. 
9. Notified bodies shall be capable of carrying out all the tasks falling to them under this  Regulation with the highest degree of professional integrity and the requisite competence  in the specific field, whether those tasks are carried out by notified bodies themselves or on  their behalf and under their responsibility. 
10. Notified bodies shall have sufficient internal competences to be able to effectively evaluate  the tasks conducted by external parties on their behalf. The notified body shall have  permanent availability of sufficient administrative, technical, legal and scientific personnel  who possess experience and knowledge relating to the relevant types of artificial  intelligence systems, data and data computing and to the requirements set out in Chapter 2  of this Title. 
11. Notified bodies shall participate in coordination activities as referred to in Article 38. They  shall also take part directly or be represented in European standardisation organisations, or  ensure that they are aware and up to date in respect of relevant standards.
 
Article 33a 
Presumption of conformity with requirements relating to notified bodies  
Where a conformity assessment body demonstrates its conformity with the criteria laid  down in the relevant harmonised standards or parts thereof the references of which have  been published in the Official Journal of the European Union it shall be presumed to  comply with the requirements set out in Article 33 in so far as the applicable harmonised  standards cover those requirements. 
Article 34 
Subsidiaries of and subcontracting by notified bodies 
1. Where a notified body subcontracts specific tasks connected with the conformity  assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the  subsidiary meets the requirements laid down in Article 33 and shall inform the notifying  authority accordingly. 
2. Notified bodies shall take full responsibility for the tasks performed by subcontractors or  subsidiaries wherever these are established. 
3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of  the provider. Notified bodies shall make a list of their subsidiaries publicly available. 
4. The relevant documents concerning the assessment of the qualifications of the  subcontractor or the subsidiary and the work carried out by them under this Regulation  shall be kept at the disposal of the notifying authority for a period of 5 years from the  termination date of the subcontracting activity. 
Article 34a 
Operational obligations of notified bodies 
1. Notified bodies shall verify the conformity of high-risk AI system in accordance with the  conformity assessment procedures referred to in Article 43.
 
2. Notified bodies shall perform their activities while avoiding unnecessary burdens for  providers, and taking due account of the size of an undertaking, the sector in which it  operates, its structure and the degree of complexity of the high risk AI system in question.  In so doing, the notified body shall nevertheless respect the degree of rigour and the level  of protection required for the compliance of the high risk AI system with the requirements  of this Regulation. Particular attention shall be paid to minimising administrative burdens  and compliance costs for micro and small enterprises as defined in Commission  Recommendation 2003/361/EC. 
3. Notified bodies shall make available and submit upon request all relevant documentation,  including the providers’ documentation, to the notifying authority referred to in Article 30  to allow that authority to conduct its assessment, designation, notification, monitoring  activities and to facilitate the assessment outlined in this Chapter. 
Article 35 
Identification numbers and lists of notified bodies designated under this Regulation 
1. The Commission shall assign an identification number to notified bodies. It shall assign a  single number, even where a body is notified under several Union acts. 
2. The Commission shall make publicly available the list of the bodies notified under this  Regulation, including the identification numbers that have been assigned to them and the  activities for which they have been notified. The Commission shall ensure that the list is  kept up to date. 
Article 36 
Changes to notifications 
-1. The notifying authority shall notify the Commission and the other Member States of any  relevant changes to the notification of a notified body via the electronic notification tool  referred to in Article 32(2).
 
-1a. The procedures described in Article 31 and 32 shall apply to extensions of the scope of the  notification. For changes to the notification other than extensions of its scope, the  procedures laid down in the following paragraphs shall apply. 
Where a notified body decides to cease its conformity assessment activities it shall inform  the notifying authority and the providers concerned as soon as possible and in the case of a  planned cessation one year before ceasing its activities. The certificates may remain valid  for a temporary period of nine months after cessation of the notified body's activities on  condition that another notified body has confirmed in writing that it will assume  responsibilities for the AI systems covered by those certificates. The new notified body  shall complete a full assessment of the AI systems affected by the end of that period before  issuing new certificates for those systems. Where the notified body has ceased its activity,  the notifying authority shall withdraw the designation. 
1. Where a notifying authority has sufficient reasons to consider that a notified body no  longer meets the requirements laid down in Article 33, or that it is failing to fulfil its  obligations, the notifying authority shall without delay investigate the matter with the  utmost diligence. In that context, it shall inform the notified body concerned about the  objections raised and give it the possibility to make its views known. If the notifying  authority comes to the conclusion that the notified body no longer meets the requirements  laid down in Article 33 or that it is failing to fulfil its obligations, it shall restrict, suspend  or withdraw notification as appropriate, depending on the seriousness of the failure to meet  those requirements or fulfil those obligations. It shall immediately inform the Commission  and the other Member States accordingly. 
2a. Where its designation has been suspended, restricted, or fully or partially withdrawn, the  notified body shall inform the manufacturers concerned at the latest within 10 days. 
2b. In the event of restriction, suspension or withdrawal of a notification, the notifying  authority shall take appropriate steps to ensure that the files of the notified body concerned  are kept and make them available to notifying authorities in other Member States and to  market surveillance authorities at their request. 
2c. In the event of restriction, suspension or withdrawal of a designation, the notifying  authority shall: 
(a) assess the impact on the certificates issued by the notified body; 
 
(b) submit a report on its findings to the Commission and the other Member States  within three months of having notified the changes to the notification; 
(c) require the notified body to suspend or withdraw, within a reasonable period of time  determined by the authority, any certificates which were unduly issued in order to  ensure the conformity of AI systems on the market;  
(d) inform the Commission and the Member States about certificates of which it has  required their suspension or withdrawal; 
(e) provide the national competent authorities of the Member State in which the provider  has its registered place of business with all relevant information about the certificates  for which it has required suspension or withdrawal. That competent authority shall  take the appropriate measures, where necessary, to avoid a potential risk to health,  safety or fundamental rights. 
2d. With the exception of certificates unduly issued, and where a notification has been  suspended or restricted, the certificates shall remain valid in the following circumstances: 
(a) the notifying authority has confirmed, within one month of the suspension or  restriction, that there is no risk to health, safety or fundamental rights in relation to  certificates affected by the suspension or restriction, and the notifying authority has  outlined a timeline and actions anticipated to remedy the suspension or restriction; or  
(b) the notifying authority has confirmed that no certificates relevant to the suspension  will be issued, amended or re-issued during the course of the suspension or  restriction, and states whether the notified body has the capability of continuing to  monitor and remain responsible for existing certificates issued for the period of the  suspension or restriction. In the event that the authority responsible for notified  bodies determines that the notified body does not have the capability to support  existing certificates issued, the provider shall provide to the national competent  authorities of the Member State in which the provider of the system covered by the  certificate has its registered place of business, within three months of the suspension  or restriction, a written confirmation that another qualified notified body is  
temporarily assuming the functions of the notified body to monitor and remain  responsible for the certificates during the period of suspension or restriction. 
 
2e. With the exception of certificates unduly issued, and where a designation has been  withdrawn, the certificates shall remain valid for a period of nine months in the following  circumstances: 
(a) where the national competent authority of the Member State in which the provider of  the AI system covered by the certificate has its registered place of business has  confirmed that there is no risk to health, safety and fundamental rights associated  with the systems in question; and  
(b) another notified body has confirmed in writing that it will assume immediate  responsibilities for those systems and will have completed assessment of them within  twelve months of the withdrawal of the designation. 
In the circumstances referred to in the first subparagraph, the national competent authority  of the Member State in which the provider of the system covered by the certificate has its  place of business may extend the provisional validity of the certificates for further periods  of three months, which altogether shall not exceed twelve months. 
2f. The national competent authority or the notified body assuming the functions of the  notified body affected by the change of notification shall immediately inform the  Commission, the other Member States and the other notified bodies thereof. 
Article 37 
Challenge to the competence of notified bodies 
1. The Commission shall, where necessary, investigate all cases where there are reasons to  doubt the competence of a notified body or the continued fulfilment by a notified body of  the requirements laid down in Article 33 and their applicable responsibilities. 
2. The Notifying authority shall provide the Commission, on request, with all relevant  information relating to the notification or the maintenance of the competence of the  notified body concerned. 
3. The Commission shall ensure that all sensitive information obtained in the course of its  investigations pursuant to this Article is treated confidentially in accordance with Article  70.
 
4. Where the Commission ascertains that a notified body does not meet or no longer meets  the requirements for its notification, it shall inform the notifying Member State accordingly  and request it to take the necessary corrective measures, including suspension or  withdrawal of the notification if necessary. Where the Member State fails to take the  necessary corrective measures, the Commission may, by means of implementing acts,  suspend, restrict or withdraw the designation. That implementing act shall be adopted in  accordance with the examination procedure referred to in Article 74(2). 
Article 38 
Coordination of notified bodies 
1. The Commission shall ensure that, with regard to high-risk AI systems, appropriate  coordination and cooperation between notified bodies active in the conformity assessment  procedures pursuant to this Regulation are put in place and properly operated in the form  of a sectoral group of notified bodies. 
2. The notifying authority shall ensure that the bodies notified by them participate in the work  of that group, directly or by means of designated representatives. 
2a. The Commission shall provide for the exchange of knowledge and best practices between  the Member States' notifying authorities.  
Article 39 
Conformity assessment bodies of third countries 
Conformity assessment bodies established under the law of a third country with which the  Union has concluded an agreement may be authorised to carry out the activities of notified  Bodies under this Regulation, provided that they meet the requirements in Article 33 or  they ensure an equivalent level of compliance.
 
Chapter 5 
STANDARDS, CONFORMITY ASSESSMENT, CERTIFICATES,  REGISTRATION 
Article 40 
Harmonised standards and standardisation deliverables 
1. High-risk AI systems or general purpose AI models which are in conformity with  harmonised standards or parts thereof the references of which have been published in the  Official Journal of the European Union in accordance with Regulation (EU) 1025/2012  shall be presumed to be in conformity with the requirements set out in Chapter 2 of this  Title or, as applicable, with the requirements set out in [Chapter on GPAI], to the extent  those standards cover those requirements.  
2. The Commission shall issue standardisation requests covering all requirements of Title II  Chapter III and as applicable [GPAI Chapter] of this Regulation, in accordance with  Article 10 of Regulation EU (No)1025/2012 without undue delay. The standardisation  request shall also ask for deliverables on reporting and documentation processes to  improve AI systems resource performance, such as reduction of energy and other resources  consumption of the high-risk AI system during its lifecycle, and on energy efficient  development of general-purpose AI models. When preparing standardisation request, the  Commission shall consult the Board and relevant stakeholders, including the Advisory  Forum.  
When issuing a standardisation request to European standardisation organisations, the  Commission shall specify that standards have to be consistent, including with the existing  and future standards developed in the various sectors for products covered by the existing  Union safety legislation listed in Annex II, clear and aimed at ensuring that AI systems or  models placed on the market or put into service in the Union meet the relevant  requirements laid down in this Regulation.
 
The Commission shall request the European standardisation organisations to provide  evidence of their best efforts to fulfil the above objectives in accordance with Article 24 of  Regulation EU 1025/2012. 
1c The actors involved in the standardisation process shall seek to promote investment and  innovation in AI, including through increasing legal certainty, as well as competitiveness  and growth of the Union market, and contribute to strengthening global cooperation on  standardisation and taking into account existing international standards in the field of AI  that are consistent with Union values, fundamental rights and interests, and enhance multi stakeholder governance ensuring a balanced representation of interests and effective  participation of all relevant stakeholders in accordance with Articles 5, 6, and 7 of  Regulation (EU) No 1025/2012 
Article 41 
Common specifications 
1. The Commission is empowered to adopt, after consulting the Advisory Forum referred to  in Article 58a, implementing acts in accordance with the examination procedure referred to  in Article 74(2) establishing common specifications for the requirements set out in Chapter  2 of this Title or, as applicable, with requirements set out in Article [GPAI Chapter], for AI  systems within the scope of this Regulation, where the following conditions have been  fulfilled: 
(a) the Commission has requested, pursuant to Article 10(1) of Regulation 1025/2012,  one or more European standardisation organisations to draft a harmonised standard  for the requirements set out in Chapter 2 of this Title; and  
(i) the request has not been accepted by any of the European standardisation  organisations; or  
(ii) the harmonised standards addressing that request are not delivered within the  deadline set in accordance with article 10(1) of Regulation 1025/2012; or  
(iii) the relevant harmonised standards insufficiently address fundamental rights  concerns; or 
 
(iv) the harmonised standards do not comply with the request; and 
(b) no reference to harmonised standards covering the requirements referred to in  Chapter II of this Title has been published in the Official Journal of the European  Union, in accordance with Regulation (EU) No 1025/2012, and no such reference is  expected to be published within a reasonable period. 
1a. Before preparing a draft implementing act, the Commission shall inform the committee  referred to in Article 22 of Regulation EU (No) 1025/2012 that it considers that the  conditions in paragraph 1 are fulfilled. 
3. High-risk AI systems which are in conformity with the common specifications referred to  in paragraph 1, or parts thereof, shall be presumed to be in conformity with the  requirements set out in Chapter 2 of this Title, to the extent those common specifications  cover those requirements. 
3a. Where a harmonised standard is adopted by a European standardisation organisation and  proposed to the Commission for the publication of its reference in the Official Journal of  the European Union, the Commission shall assess the harmonised standard in accordance  with Regulation (EU) No 1025/2012. When reference of a harmonised standard is  published in the Official Journal of the European Union, the Commission shall repeal acts  referred to in paragraph 1 and 1b, or parts thereof which cover the same requirements set  out in Chapter 2 of this Title. 
4. Where providers of high-risk AI systems do not comply with the common specifications  referred to in paragraph 1, they shall duly justify that they have adopted technical solutions  that meet the requirements referred to in Chapter II to a level at least equivalent thereto. 
4b. When a Member State considers that a common specification does not entirely satisfy the  requirements set out in Chapter 2 of this Title, it shall inform the Commission thereof with  a detailed explanation and the Commission shall assess that information and, if  appropriate, amend the implementing act establishing the common specification in  question.
 
Article 42 
Presumption of conformity with certain requirements 
1. High-risk AI systems that have been trained and tested on data reflecting the specific  geographical, behavioural, contextual or functional setting within which they are intended  to be used shall be presumed to be in compliance with the respective requirements set out  in Article 10(4). 
2. High-risk AI systems that have been certified or for which a statement of conformity has  been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 of the  European Parliament and of the Council1and the references of which have been published  in the Official Journal of the European Union shall be presumed to be in compliance with  the cybersecurity requirements set out in Article 15 of this Regulation in so far as the  cybersecurity certificate or statement of conformity or parts thereof cover those  requirements. 
Article 43 
Conformity assessment 
1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the  compliance of a high-risk AI system with the requirements set out in Chapter 2 of this  Title, the provider has applied harmonised standards referred to in Article 40, or, where  applicable, common specifications referred to in Article 41, the provider shall opt for one  of the following procedures: 
(a) the conformity assessment procedure based on internal control referred to in Annex  VI; or 
(b) the conformity assessment procedure based on assessment of the quality  management system and assessment of the technical documentation, with the  involvement of a notified body, referred to in Annex VII. 
In demonstrating the compliance of a high-risk AI system with the requirements set out in  Chapter 2 of this Title, the provider shall follow the conformity assessment procedure set  out in Annex VII in the following cases: 
 
(a) where harmonised standards referred to in Article 40, do not exist and common specifications referred to in Article 41 are not available;  
(aa) the provider has not applied or has applied only in part the harmonised standard;  
(b) where the common specifications referred to in point (a) exist but the provider has  not applied them;  
(c) where one or more of the harmonised standards referred to in point (a) has been  published with a restriction and only on the part of the standard that was restricted. 
For the purpose of the conformity assessment procedure referred to in Annex VII, the  provider may choose any of the notified bodies. However, when the system is intended to  be put into service by law enforcement, immigration or asylum authorities as well as EU  institutions, bodies or agencies, the market surveillance authority referred to in Article  63(5) or (6), as applicable, shall act as a notified body. 
2. For high-risk AI systems referred to in points 2 to 8 of Annex III providers shall follow the  conformity assessment procedure based on internal control as referred to in Annex VI,  which does not provide for the involvement of a notified body. 
3. For high-risk AI systems, to which legal acts listed in Annex II, section A, apply, the  provider shall follow the relevant conformity assessment as required under those legal acts.  The requirements set out in Chapter 2 of this Title shall apply to those high-risk AI systems  and shall be part of that assessment. Points 4.3., 4.4., 4.5. and the fifth paragraph of point  4.6 of Annex VII shall also apply. 
For the purpose of that assessment, notified bodies which have been notified under those  legal acts shall be entitled to control the conformity of the high-risk AI systems with the  requirements set out in Chapter 2 of this Title, provided that the compliance of those  notified bodies with requirements laid down in Article 33(4), (9) and (10) has been  assessed in the context of the notification procedure under those legal acts. 
Where the legal acts listed in Annex II, section A, enable the manufacturer of the product  to opt out from a third-party conformity assessment, provided that that manufacturer has  applied all harmonised standards covering all the relevant requirements, that manufacturer  may make use of that option only if he has also applied harmonised standards or, where 
 
applicable, common specifications referred to in Article 41, covering the requirements set  out in Chapter 2 of this Title. 
4. High-risk AI systems that have already been subject to a conformity assessment procedure  shall undergo a new conformity assessment procedure whenever they are substantially  modified, regardless of whether the modified system is intended to be further distributed or  continues to be used by the current deployer. 
For high-risk AI systems that continue to learn after being placed on the market or put into  service, changes to the high-risk AI system and its performance that have been pre determined by the provider at the moment of the initial conformity assessment and are part  of the information contained in the technical documentation referred to in point 2(f) of  Annex IV, shall not constitute a substantial modification. 
5. The Commission is empowered to adopt delegated acts in accordance with Article 73 for  the purpose of updating Annexes VI and Annex VII in light of technical progress.  
6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in  order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the  conformity assessment procedure referred to in Annex VII or parts thereof. The  Commission shall adopt such delegated acts taking into account the effectiveness of the  conformity assessment procedure based on internal control referred to in Annex VI in  preventing or minimizing the risks to health and safety and protection of fundamental  rights posed by such systems as well as the availability of adequate capacities and  resources among notified bodies.  
Article 44 
Certificates 
1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a  language which can be easily understood by the relevant authorities in the Member State in  which the notified body is established.  
2. Certificates shall be valid for the period they indicate, which shall not exceed five years for  AI systems covered by Annex II and four years for AI systems covered by Annex III. On  application by the provider, the validity of a certificate may be extended for further 
 
periods, each not exceeding five years for AI systems covered by Annex II and four years  for AI systems covered by Annex III, based on a re-assessment in accordance with the  applicable conformity assessment procedures. Any supplement to a certificate shall remain  valid as long as the certificate which it supplements is valid. 
3. Where a notified body finds that an AI system no longer meets the requirements set out in  Chapter 2 of this Title, it shall, taking account of the principle of proportionality, suspend  or withdraw the certificate issued or impose any restrictions on it, unless compliance with  those requirements is ensured by appropriate corrective action taken by the provider of the  
system within an appropriate deadline set by the notified body. The notified body shall  give reasons for its decision. 
An appeal procedure against decisions of the notified bodies, including on issued  conformity certificates, shall be available. 
Article 46 
Information obligations of notified bodies 
1. Notified bodies shall inform the notifying authority of the following:  
(a) any Union technical documentation assessment certificates, any supplements to those  certificates, quality management system approvals issued in accordance with the  requirements of Annex VII; 
(b) any refusal, restriction, suspension or withdrawal of a Union technical  documentation assessment certificate or a quality management system approval  issued in accordance with the requirements of Annex VII;  
(c) any circumstances affecting the scope of or conditions for notification; 
(d) any request for information which they have received from market surveillance  authorities regarding conformity assessment activities; 
(e) on request, conformity assessment activities performed within the scope of their  notification and any other activity performed, including cross-border activities and  subcontracting.
 
2. Each notified body shall inform the other notified bodies of: 
(a) quality management system approvals which it has refused, suspended or withdrawn,  and, upon request, of quality system approvals which it has issued; 
(b) EU technical documentation assessment certificates or any supplements thereto  which it has refused, withdrawn, suspended or otherwise restricted, and, upon  request, of the certificates and/or supplements thereto which it has issued. 
3. Each notified body shall provide the other notified bodies carrying out similar conformity  assessment activities covering the same types of AI systems with relevant information on  issues relating to negative and, on request, positive conformity assessment results. 
3a. The obligations referred to in paragraphs 1 to 3 shall be complied with in accordance with  Article 70. 
Article 47 
Derogation from conformity assessment procedure 
1. By way of derogation from Article 43 and upon a duly justified request, any market  surveillance authority may authorise the placing on the market or putting into service of  specific high-risk AI systems within the territory of the Member State concerned, for  exceptional reasons of public security or the protection of life and health of persons,  environmental protection and the protection of key industrial and infrastructural assets.  That authorisation shall be for a limited period of time while the necessary conformity  assessment procedures are being carried out, taking into account the exceptional reasons  justifying the derogation. The completion of those procedures shall be undertaken without  undue delay. 
1a. In a duly justified situation of urgency for exceptional reasons of public security or in case  of specific, substantial and imminent threat to the life or physical safety of natural persons,  law enforcement authorities or civil protection authorities may put a specific high-risk AI  system into service without the authorisation referred to in paragraph 1 provided that such  authorisation is requested during or after the use without undue delay, and if such  authorisation is rejected, its use shall be stopped with immediate effect and all the results  and outputs of this use shall be immediately discarded. 
 
2. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance  authority concludes that the high-risk AI system complies with the requirements of Chapter  2 of this Title. The market surveillance authority shall inform the Commission and the  other Member States of any authorisation issued pursuant to paragraph 1. This obligation  shall not cover sensitive operational data in relation to the activities of law enforcement  authorities.  
3. Where, within 15 calendar days of receipt of the information referred to in paragraph 2, no  objection has been raised by either a Member State or the Commission in respect of an  authorisation issued by a market surveillance authority of a Member State in accordance  with paragraph 1, that authorisation shall be deemed justified. 
4. Where, within 15 calendar days of receipt of the notification referred to in paragraph 2,  objections are raised by a Member State against an authorisation issued by a market  surveillance authority of another Member State, or where the Commission considers the  authorisation to be contrary to Union law or the conclusion of the Member States regarding  the compliance of the system as referred to in paragraph 2 to be unfounded, the  Commission shall without delay enter into consultation with the relevant Member State;  the operator(s) concerned shall be consulted and have the possibility to present their views.  In view thereof, the Commission shall decide whether the authorisation is justified or not.  The Commission shall address its decision to the Member State concerned and the relevant  operator or operators. 
5. If the authorisation is considered unjustified, this shall be withdrawn by the market  surveillance authority of the Member State concerned.  
6. For high-risk AI systems related to products covered by Union harmonisation legislation  referred to in Annex II Section A, only the conformity assessment derogation procedures  established in that legislation shall apply. 
Article 48 
EU declaration of conformity 
1. The provider shall draw up a written machine readable, physical or electronically signed  EU declaration of conformity for each high-risk AI system and keep it at the disposal of 
 
the national competent authorities for 10 years after the AI high-risk system has been  placed on the market or put into service. The EU declaration of conformity shall identify  the high-risk AI system for which it has been drawn up. A copy of the EU declaration of  conformity shall be submitted to the relevant national competent authorities upon request. 
2. The EU declaration of conformity shall state that the high-risk AI system in question meets  the requirements set out in Chapter 2 of this Title. The EU declaration of conformity shall  contain the information set out in Annex V and shall be translated into a language that can  be easily understood by the national competent authorities of the Member State(s) in which  the high-risk AI system is placed on the market or made available. 
3. Where high-risk AI systems are subject to other Union harmonisation legislation which  also requires an EU declaration of conformity, a single EU declaration of conformity shall  be drawn up in respect of all Union legislations applicable to the high-risk AI system. The  declaration shall contain all the information required for identification of the Union  harmonisation legislation to which the declaration relates.  
4. By drawing up the EU declaration of conformity, the provider shall assume responsibility  for compliance with the requirements set out in Chapter 2 of this Title. The provider shall  keep the EU declaration of conformity up-to-date as appropriate. 
5. The Commission shall be empowered to adopt delegated acts in accordance with Article 73  for the purpose of updating the content of the EU declaration of conformity set out in  Annex V in order to introduce elements that become necessary in light of technical  progress. 
Article 49 
CE marking of conformity 
1. The CE marking of conformity shall be subject to the general principles set out in Article  30 of Regulation (EC) No 765/2008. 
1a. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it  can be easily accessed via the interface from which the AI system is accessed or via an  easily accessible machine-readable code or other electronic means.
 
2. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems.  Where that is not possible or not warranted on account of the nature of the high-risk AI  system, it shall be affixed to the packaging or to the accompanying documentation, as  appropriate. 
3. Where applicable, the CE marking shall be followed by the identification number of the  notified body responsible for the conformity assessment procedures set out in Article 43.  The identification number of the notified body shall be affixed by the body itself or, under  its instructions, by the provider or by its authorised representative. The identification  number shall also be indicated in any promotional material which mentions that the high risk AI system fulfils the requirements for CE marking.  
3a. Where high-risk AI systems are subject to other Union law which also provides for the  affixing of the CE marking, the CE marking shall indicate that the high-risk AI system also  fulfil the requirements of that other law. 
Article 51 
Registration 
1. Before placing on the market or putting into service a high-risk AI system listed in Annex  III, with the exception of high risk AI systems referred to in Annex III point 2, the provider  or, where applicable, the authorised representative shall register themselves and their  system in the EU database referred to in Article 60.  
1a. Before placing on the market or putting into service an AI system for which the provider  has concluded that it is not high-risk in application of the procedure under Article 6(2a),  the provider or, where applicable, the authorised representative shall register themselves  and that system in the EU database referred to in Article 60. 
1b. Before putting into service or using a high-risk AI system listed in Annex III, with the  exception of high-risk AI systems listed in Annex III, point 2, deployers who are public  authorities, agencies or bodies or persons acting on their behalf shall register themselves,  select the system and register its use in the EU database referred to in Article 60.  
1c. For high-risk AI systems referred to Annex III, points 1, 6 and 7 in the areas of law  enforcement, migration, asylum and border control management, the registration referred 
 
to in paragraphs 1 to 1b shall be done in a secure non-public section of the EU database  referred to in Article 60 and include only the following information, as applicable: 
- points 1 to 9 of Annex VIII, section A with the exception of points 5a, 7 and 8;  - points 1 to 3 of Annex VIII, section B;  
- points 1 to 9 of Annex VIII, section X with the exception of points 6 and 7;  - points 1 to 5 of Annex VIIIa with the exception of point 4. 
Only the Commission and national authorities referred to in Art. 63(5) shall have access to  these restricted sections of the EU database.  
1d. High risk AI systems referred to in Annex III, point 2 shall be registered at national level. 
TITLE IV 
TRANSPARENCY OBLIGATIONS FOR PROVIDERS AND  DEPLOYERS OF CERTAIN AI SYSTEMS  
Article 52 
Transparency obligations for providers and users of certain AI systems and GPAI models 
1. Providers shall ensure that AI systems intended to directly interact with natural persons are  designed and developed in such a way that the concerned natural persons are informed that  they are interacting with an AI system, unless this is obvious from the point of view of a  natural person who is reasonably well-informed, observant and circumspect, taking into  account the circumstances and the context of use. This obligation shall not apply to AI  systems authorised by law to detect, prevent, investigate and prosecute criminal offences,  subject to appropriate safeguards for the rights and freedoms of third parties unless those  systems are available for the public to report a criminal offence. 
1a. Providers of AI systems, including GPAI systems, generating synthetic audio, image,  video or text content, shall ensure the outputs of the AI system are marked in a machine readable format and detectable as artificially generated or manipulated. Providers shall  ensure their technical solutions are effective, interoperable, robust and reliable as far as this 
 
is technically feasible, taking into account specificities and limitations of different types of  content, costs of implementation and the generally acknowledged state-of-the-art, as may  be reflected in relevant technical standards. This obligation shall not apply to the extent the  AI systems perform an assistive function for standard editing or do not substantially alter  the input data provided by the deployer or the semantics thereof, or where authorised by  law to detect, prevent, investigate and prosecute criminal offences.  
2. Deployers of an emotion recognition system or a biometric categorisation system shall  inform of the operation of the system the natural persons exposed thereto and process the  personal data in accordance with Regulation (EU) 2016/679, Regulation (EU) 2016/1725  and Directive (EU) 2016/280, as applicable. This obligation shall not apply to AI systems  used for biometric categorization and emotion recognition, which are permitted by law to  detect, prevent and investigate criminal offences, subject to appropriate safeguards for the  rights and freedoms of third parties, and in compliance with Union law. 
3. Deployers of an AI system that generates or manipulates image, audio or video content  constituting a deep fake, shall disclose that the content has been artificially generated or  manipulated. This obligation shall not apply where the use is authorised by law to detect,  prevent, investigate and prosecute criminal offence. Where the content forms part of an  evidently artistic, creative, satirical, fictional analogous work or programme, the  transparency obligations set out in this paragraph are limited to disclosure of the existence  of such generated or manipulated content in an appropriate manner that does not hamper  the display or enjoyment of the work.  
Deployers of an AI system that generates or manipulates text which is published with the  purpose of informing the public on matters of public interest shall disclose that the text has  been artificially generated or manipulated. This obligation shall not apply where the use is  authorised by law to detect, prevent, investigate and prosecute criminal offences or where  the AI-generated content has undergone a process of human review or editorial control and  where a natural or legal person holds editorial responsibility for the publication of the  content. 
3a. The information referred to in paragraphs 1 to 3 shall be provided to the concerned natural  persons in a clear and distinguishable manner at the latest at the time of the first interaction  or exposure. The information shall respect the applicable accessibility requirements.
 
4. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title III of  this Regulation and shall be without prejudice to other transparency obligations for users of  AI systems laid down in Union or national law. 
4a. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union  level to facilitate the effective implementation of the obligations regarding the detection  and labelling of artificially generated or manipulated content. The Commission is  empowered to adopt implementing acts to approve these codes of practice in accordance  with the procedure laid down in Article 52e paragraphs 6-8. If it deems the code is not  adequate, the Commission is empowered to adopt an implementing act specifying the  common rules for the implementation of those obligations in accordance with the  examination procedure laid down in Article 73 paragraph 2. 
TITLE VIIIA  
GENERAL PURPOSE AI MODELS 
Chapter 1  
CLASSIFICATION RULES 
Article 52a 
Classification of general purpose AI models as general purpose AI models with systemic risk 
1. A general purpose AI model shall be classified as general-purpose AI model with systemic  risk if it meets any of the following criteria: 
(a) it has high impact capabilities evaluated on the basis of appropriate technical tools  and methodologies, including indicators and benchmarks; 
(b) based on a decision of the Commission, ex officio or following a qualified alert by  the scientific panel that a general purpose AI model has capabilities or impact  equivalent to those of point (a).
 
2. A general purpose AI model shall be presumed to have high impact capabilities pursuant to  point a) of paragraph 1 when the cumulative amount of compute used for its training  measured in floating point operations (FLOPs) is greater than 10^25. 
3. The Commission shall adopt delegated acts in accordance with Article 73(2) to amend the  thresholds listed in the paragraphs above, as well as to supplement benchmarks and  indicators in light of evolving technological developments, such as algorithmic  improvements or increased hardware efficiency, when necessary, for these thresholds to  reflect the state of the art.  
Article 52b 
Procedure 
1. Where a general purpose AI model meets the requirements referred to in points (a) of  Article 52a(1), the relevant provider shall notify the Commission without delay and in any  event within 2 weeks after those requirements are met or it becomes known that these  requirements will be met. That notification shall include the information necessary to  demonstrate that the relevant requirements have been met. If the Commission becomes  aware of a general purpose AI model presenting systemic risks of which it has not been  notified, it may decide to designate it as a model with systemic risk. 
2. The provider of a general purpose AI model that meets the requirements referred to in  points (a) of Article 52a(1) may present, with its notification, sufficiently substantiated  arguments to demonstrate that, exceptionally, although it meets the said requirements, the  general-purpose AI model does not present, due to its specific characteristics, systemic  risks and therefore should not be classified as general-purpose AI model with systemic  risk. 
3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2  are not sufficiently substantiated and the relevant provider was not able to demonstrate that  the general purpose AI model does not present, due to its specific characteristics, systemic  risks, it shall reject those arguments and the general purpose AI model shall be considered  as general purpose AI model with systemic risk.
 
4. The Commission may designate a general purpose AI model as presenting systemic risks,  ex officio or following a qualified alert of the scientific panel pursuant to point (a) of  Article 68h [Alerts of systemic risks by the scientific panel] (1) on the basis of criteria set  out in Annex IXc. The Commission shall be empowered to specify and update the criteria  in Annex IXc by means of delegated acts in accordance with Article 74(2). 
4a. Upon a reasoned request of a provider whose model has been designated as a general  purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take  the request into account and may decide to reassess whether the general purpose AI model  can still be considered to present systemic risks on the basis of the criteria set out in Annex  IXc. Such request shall contain objective, concrete and new reasons that have arisen since  the designation decision. Providers may request reassessment at the earliest six months  after the designation decision. Where the Commission, following its reassessment, decides  to maintain the designation as general-purpose AI model with systemic risk, providers may  request reassessment at the earliest six months after this decision. 
5. The Commission shall ensure that a list of general purpose AI models with systemic risk is  published and shall keep that list up to date, without prejudice to the need to respect and  protect intellectual property rights and confidential business information or trade secrets in  accordance with Union and national law. 
Chapter 2 
OBLIGATIONS FOR PROVIDERS OF GENERAL PURPOSE AI  MODELS 
Article 52c 
Obligations for providers of general purpose AI models 
1. Providers of general purpose AI models shall: 
(a) draw up and keep up-to-date the technical documentation of the model, including its  training and testing process and the results of its evaluation, which shall contain, at a  minimum, the elements set out in Annex IXa for the purpose of providing it, upon  request, to the AI Office and the national competent authorities; 
 
(b) draw up, keep up-to-date and make available information and documentation to  providers of AI systems who intend to integrate the general purpose AI model in  their AI system. Without prejudice to the need to respect and protect intellectual  property rights and confidential business information or trade secrets in accordance  with Union and national law, the information and documentation shall: 
(i) enable providers of AI systems to have a good understanding of the capabilities  and limitations of the general purpose AI model and to comply with their  
obligations pursuant to this Regulation; and 
(ii) contain, at a minimum, the elements set out in Annex IXb. 
(c) put in place a policy to respect Union copyright law in particular to identify and  respect, including through state of the art technologies, the reservations of rights  expressed pursuant to Article 4(3) of Directive (EU) 2019/790; 
(d) draw up and make publicly available a sufficiently detailed summary about the  content used for training of the general-purpose AI model, according to a template  provided by the AI Office. 
-2. The obligations set out in paragraph 1, with the exception of letters (c) and (d), shall not  apply to providers of AI models that are made accessible to the public under a free and  open licence that allows for the access, usage, modification, and distribution of the model,  and whose parameters, including the weights, the information on the model architecture,  and the information on model usage, are made publicly available. This exception shall not  apply to general purpose AI models with systemic risks.  
2. Providers of general purpose AI models shall cooperate as necessary with the Commission  and the national competent authorities in the exercise of their competences and powers pursuant to this Regulation. 
3. Providers of general purpose AI models may rely on codes of practice within the meaning  of Article 52e demonstrate compliance with the obligations in paragraph 1, until a  harmonised standard is published. Compliance with a European harmonised standard  grants providers the presumption of conformity. Providers of general purpose AI models  with systemic risks who do not adhere to an approved code of practice shall demonstrate  alternative adequate means of compliance for approval of the Commission.
 
4. For the purpose of facilitating compliance with Annex IXa, notably point 2, (d) and (e), the  Commission shall be empowered to adopt delegated acts in accordance with Article 73 to  detail measurement and calculation methodologies with a view to allow comparable and  verifiable documentation. 
4a. The Commission is empowered to adopt delegated acts in accordance with Article 73(2) to  amend Annexes IXa and IXb in the light of the evolving technological developments. 
4b. Any information and documentation obtained pursuant to the provisions of this Article,  including trade secrets, shall be treated in compliance with the confidentiality obligations  set out in Article 70. 
Article 52ca 
Authorised representative 
1. Prior to placing a general purpose AI model on the Union market providers established  outside the Union shall, by written mandate, appoint an authorised representative which is  established in the Union and shall enable it to perform its tasks under this Regulation. 
2. The authorised representative shall perform the tasks specified in the mandate received  from the provider. It shall provide a copy of the mandate to the AI Office upon request, in  one of the official languages of the institutions of the Union. For the purpose of this Regulation, the mandate shall empower the authorised representative to carry out the  following tasks: 
(a) verify that the technical documentation specified in Annex IXa has been drawn up  and all obligations referred to in Articles 52c and, where applicable, Article 52d  have been fulfilled by the provider; 
(b) keep a copy of the technical documentation at the disposal of the AI Office and  national competent authorities, for a period ending 10 years after the model has  been placed on the market and the contact details of the provider by which the  authorised representative has been appointed;
 
(c) provide the AI Office, upon a reasoned request, with all the information and  documentation, including that kept according to point (a), necessary to  
demonstrate the compliance with the obligations in this Title; 
(d) cooperate with the AI Office and national competent authorities, upon a reasoned  request, on any action the latter takes in relation to the general purpose AI model  with systemic risks, including when the model is integrated into AI systems  placed on the market or put into service in the Union. 
3. The mandate shall empower the authorised representative to be addressed, in addition to or  instead of the provider, by the AI Office or the national competent authorities, on all issues  related to ensuring compliance with this Regulation.  
4. The authorised representative shall terminate the mandate if it considers or has reason to  consider that the provider acts contrary to its obligations under this Regulation. In such a  case, it shall also immediately inform the AI Office about the termination of the mandate  and the reasons thereof. 
5. The obligation set out in this article shall not apply to providers of general purpose AI  models that are made accessible to the public under a free and open source licence that  allows for the access, usage, modification, and distribution of the model, and whose  parameters, including the weights, the information on the model architecture, and the  information on model usage, are made publicly available, unless the general purpose AI  models present systemic risks.
 
Chapter 3 
OBLIGATIONS FOR PROVIDERS OF GENERAL PURPOSE AI  MODELS WITH SYSTEMIC RISK 
Article 52d 
Obligations for providers of general purpose AI models with systemic risk 
1. In addition to the obligations listed in Article 52c, providers of general purpose AI models with systemic risk shall: 
(a) perform model evaluation in accordance with standardised protocols and tools  reflecting the state of the art, including conducting and documenting adversarial  testing of the model with a view to identify and mitigate systemic risk; 
(b) assess and mitigate possible systemic risks at Union level, including their sources,  that may stem from the development, placing on the market, or use of general  purpose AI models with systemic risk; 
(c) keep track of, document and report without undue delay to the AI Office and, as  appropriate, to national competent authorities, relevant information about serious  incidents and possible corrective measures to address them; 
(d) ensure an adequate level of cybersecurity protection for the general purpose AI  model with systemic risk and the physical infrastructure of the model. 
2. Providers of general purpose AI models with systemic risk may rely on codes of practice  within the meaning of Article E to demonstrate compliance with the obligations in  paragraph 1, until a harmonised standard is published. Compliance with a European  harmonised standard grants providers the presumption of conformity. Providers of general purpose AI models with systemic risks who do not adhere to an approved code of practice  shall demonstrate alternative adequate means of compliance for approval of the  Commission.
 
3. Any information and documentation obtained pursuant to the provisions of this Article,  including trade secrets, shall be treated in compliance with the confidentiality obligations  set out in Article 70. 
Article 52e 
Codes of practice 
1. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union  level as an element to contribute to the proper application of this Regulation, taking into  account international approaches. 
2. The AI Office and the AI Board shall aim to ensure that the codes of practice cover, but  not necessarily be limited to, the obligations provided for in Articles 52c and 52d,  including the following issues: 
(a) means to ensure that the information referred to in Article 52c (a) and (b) is kept up  to date in the light of market and technological developments, and the adequate level  of detail for the summary about the content used for training; 
(b) the identification of the type and nature of the systemic risks at Union level,  including their sources when appropriate; 
(c) the measures, procedures and modalities for the assessment and management of the  systemic risks at Union level, including the documentation thereof. The assessment  and management of the systemic risks at Union level shall be proportionate to the  risks, take into consideration their severity and probability and take into account the  specific challenges of tackling those risks in the light of the possible ways in which  such risks may emerge and materialize along the AI value chain. 
3. The AI Office may invite the providers of general purpose AI models, as well as relevant  national competent authorities, to participate in the drawing up of codes of practice. Civil  society organisations, industry, academia and other relevant stakeholders, such as  downstream providers and independent experts, may support the process.  
4. The AI Office and the Board shall aim to ensure that the codes of practice clearly set out  their specific objectives and contain commitments or measures, including key performance 
 
indicators as appropriate, to ensure the achievement of those objectives and take due  account of the needs and interests of all interested parties, including affected persons, at  Union level. 
5. The AI Office may invite all providers of general purpose AI models to participate in the  codes of practice. For providers of general purpose AI models not presenting systemic  risks this participation should be limited to obligations foreseen in paragraph 2 point (a) of  this Article, unless they declare explicitly their interest to join the full code. 
6. The AI Office shall aim to ensure that participants to the codes of practice report regularly  to the AI Office on the implementation of the commitments and the measures taken and  their outcomes, including as measured against the key performance indicators as  appropriate. Key performance indicators and reporting commitments shall take into  account differences in size and capacity between different participants. 
7. The AI Office and the AI Board shall regularly monitor and evaluate the achievement of  the objectives of the codes of practice by the participants and their contribution to the  proper application of this Regulation. The AI Office and the Board shall assess whether the  codes of practice cover the obligations provided for in Articles 52c and 52d, including the  issues listed in paragraph 2 of this Article, and shall regularly monitor and evaluate the  achievement of their objectives. They shall publish their assessment of the adequacy of the  codes of practice. The Commission may, by way of implementing acts, decide to approve  the code of practice and give it a general validity within the Union. Those implementing  acts shall be adopted in accordance with the examination procedure set out in Article  74(2). 
8. As appropriate, the AI Office shall also encourage and facilitate review and adaptation of  the codes of practice, in particular in light of emerging standards. The AI Office shall assist  in the assessment of available standards. 
9. If, by the time the Regulation becomes applicable, a Code of Practice cannot be finalised,  or if the AI Office deems it is not adequate following under paragraph 7, the Commission  may provide, by means of implementing acts, common rules for the implementation of the  obligations provided for in Articles 52c and 52d, including the issues set out in paragraph  2.
 
TITLE V 
MEASURES IN SUPPORT OF INNOVATION 
Article 53 
AI regulatory sandboxes  
1. Member States shall ensure that their competent authorities establish at least one AI  regulatory sandbox at national level, which shall be operational 24 months after entry into  force. This sandbox may also be established jointly with one or several other Member  States’ competent authorities. The Commission may provide technical support, advice and  tools for the establishment and operation of AI regulatory sandboxes. 
The obligation established in previous paragraph can also be fulfilled by participation in an  existing sandbox insofar as this participation provides equivalent level of national coverage  for the participating Member States. 
1a. Additional AI regulatory sandboxes at regional or local levels or jointly with other  Member States' competent authorities may also be established. 
1b. The European Data Protection Supervisor may also establish an AI regulatory sandbox for  the EU institutions, bodies and agencies and exercise the roles and the tasks of national  competent authorities in accordance with this chapter. 
1c. Member States shall ensure that competent authorities referred to in paragraphs 1 and 1a  allocate sufficient resources to comply with this Article effectively and in a timely manner. Where appropriate, national competent authorities shall cooperate with other relevant  authorities and may allow for the involvement of other actors within the AI ecosystem. This Article shall not affect other regulatory sandboxes established under national or Union  law. Member States shall ensure an appropriate level of cooperation between the  authorities supervising those other sandboxes and the national competent authorities.  
1d. AI regulatory sandboxes established under Article 53(1) of this Regulation shall, in  accordance with Articles 53 and 53a, provide for a controlled environment that fosters  innovation and facilitates the development, training, testing and validation of innovative AI  systems for a limited time before their placement on the market or putting into service 
 
pursuant to a specific sandbox plan agreed between the prospective providers and the  competent authority. Such regulatory sandboxes may include testing in real world  conditions supervised in the sandbox.  
1e. Competent authorities shall provide, as appropriate, guidance, supervision and support  within the sandbox with a view to identifying risks, in particular to fundamental rights,  health and safety, testing, mitigation measures, and their effectiveness in relation to the  obligations and requirements of this Regulation and, where relevant, other Union and  Member States legislation supervised within the sandbox.  
1f. Competent authorities shall provide providers and prospective providers with guidance on  regulatory expectations and how to fulfil the requirements and obligations set out in this  Regulation. 
Upon request of the provider or prospective provider of the AI system, the competent  authority shall provide a written proof of the activities successfully carried out in the  sandbox. The competent authority shall also provide an exit report detailing the activities  carried out in the sandbox and the related results and learning outcomes. Providers may use  such documentation to demonstrate the compliance with this Regulation through the  conformity assessment process or relevant market surveillance activities. In this regard, the  exit reports and the written proof provided by the national competent authority shall be  taken positively into account by market surveillance authorities and notified bodies, with a  view to accelerate conformity assessment procedures to a reasonable extent. 
1fa. Subject to the confidentiality provisions in Article 70 and with the agreement of the  sandbox provider/prospective provider, the European Commission and the Board shall be  authorised to access the exit reports and shall take them into account, as appropriate, when  exercising their tasks under this Regulation. If both provider and prospective provider and  the national competent authority explicitly agree to this, the exit report can be made  publicly available through the single information platform referred to in this article. 
1g. The establishment of AI regulatory sandboxes shall aim to contribute to the following  objectives: 
(a) improve legal certainty to achieve regulatory compliance with this Regulation or,  where relevant, other applicable Union and Member States legislation; 
(b) support the sharing of best practices through cooperation with the authorities 
 
involved in the AI regulatory sandbox; 
(c) foster innovation and competitiveness and facilitate the development of an AI  ecosystem; 
(d) contribute to evidence-based regulatory learning;  
(e) facilitate and accelerate access to the Union market for AI systems, in particular  when provided by small and medium-sized enterprises (SMEs), including start ups.  
2. National competent authorities shall ensure that, to the extent the innovative AI systems  involve the processing of personal data or otherwise fall under the supervisory remit of  other national authorities or competent authorities providing or supporting access to data,  the national data protection authorities, and those other national authorities are associated to the operation of the AI regulatory sandbox and involved in the supervision of those  aspects to the extent of their respective tasks and powers, as applicable. 
3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the  competent authorities supervising the sandboxes, including at regional or local level. Any  significant risks to health and safety and fundamental rights identified during the  development and testing of such AI systems shall result in an adequate mitigation. National  competent authorities shall have the power to temporarily or permanently suspend the  testing process, or participation in the sandbox if no effective mitigation is possible and  inform the AI Office of such decision. National competent authorities shall exercise their  supervisory powers within the limits of the relevant legislation, using their discretionary  powers when implementing legal provisions to a specific AI sandbox project, with the  objective of supporting innovation in AI in the Union.  
4. Providers and prospective providers in the AI regulatory sandbox shall remain liable under  applicable Union and Member States liability legislation for any damage inflicted on third  parties as a result of the experimentation taking place in the sandbox. However, provided  that the prospective provider(s) respect the specific plan and the terms and conditions for  their participation and follow in good faith the guidance given by the national competent  authority, no administrative fines shall be imposed by the authorities for infringements of  this Regulation. To the extent that other competent authorities responsible for other Union  and Member States’ legislation have been actively involved in the supervision of the AI 
 
system in the sandbox and have provided guidance for compliance, no administrative fines  shall be imposed regarding that legislation. 
4b. The AI regulatory sandboxes shall be designed and implemented in such a way that, where  relevant, they facilitate cross-border cooperation between national competent authorities.  
5. National competent authorities shall coordinate their activities and cooperate within the  framework of the Board. 
5a. National competent authorities shall inform the AI Office and the Board of the  establishment of a sandbox and may ask for support and guidance. A list of planned and  existing AI sandboxes shall be made publicly available by the AI Office and kept up to  date in order to encourage more interaction in the regulatory sandboxes and cross-border  cooperation.  
5b. National competent authorities shall submit to the AI Office and to the Board, annual  reports, starting one year after the establishment of the AI regulatory sandbox and then  every year until its termination and a final report. Those reports shall provide information  on the progress and results of the implementation of those sandboxes, including best  practices, incidents, lessons learnt and recommendations on their setup and, where  relevant, on the application and possible revision of this Regulation, including its delegated  and implementing acts, and other Union law supervised within the sandbox. Those annual  reports or abstracts thereof shall be made available to the public, online. The Commission  shall, where appropriate, take the annual reports into account when exercising their tasks  under this Regulation. 
6. The Commission shall develop a single and dedicated interface containing all relevant  information related to sandboxes to allow stakeholders to interact with regulatory  sandboxes and to raise enquiries with competent authorities, and to seek non-binding  guidance on the conformity of innovative products, services, business models embedding  AI technologies, in accordance with Article 55(1)(c). The Commission shall proactively  coordinate with national competent authorities, where relevant. 
Article 53a  
Modalities and functioning of AI regulatory sandboxes
 
1. In order to avoid fragmentation across the Union, the Commission shall adopt an  implementing act detailing the modalities for the establishment, development,  implementation, operation and supervision of the AI regulatory sandboxes. The  implementing act shall include common principles on the following issues: 
(a) eligibility and selection for participation in the AI regulatory sandbox;  (b) procedure for the application, participation, monitoring, exiting from and  termination of the AI regulatory sandbox, including the sandbox plan and the exit  report;  
(c) the terms and conditions applicable to the participants. 
The implementing acts shall ensure that: 
(a) regulatory sandboxes are open to any applying prospective provider of an AI  system who fulfils eligibility and selection criteria. The criteria for accessing to  the regulatory sandbox are transparent and fair and establishing authorities inform  applicants of their decision within 3 months of the application; 
(b) regulatory sandboxes allow broad and equal access and keep up with demand for  participation; prospective providers may also submit applications in partnerships  with users and other relevant third parties; 
(c) the modalities and conditions concerning regulatory sandboxes shall to the best  extent possible support flexibility for national competent authorities to establish  and operate their AI regulatory sandboxes; 
(d) access to the AI regulatory sandboxes is free of charge for SMEs and start-ups  without prejudice to exceptional costs that national competent authorities may  recover in a fair and proportionate manner; 
(e) they facilitate prospective providers, by means of the learning outcomes of the  sandboxes, to conduct the conformity assessment obligations of this Regulation or  the voluntary application of the codes of conduct referred to in Article 69; 
(f) regulatory sandboxes facilitate the involvement of other relevant actors within the  AI ecosystem, such as notified bodies and standardisation organisations (SMEs,  start- ups, enterprises, innovators, testing and experimentation facilities, research  and experimentation labs and digital innovation hubs, centres of excellence,  individual researchers), in order to allow and facilitate cooperation with the  public and private sector; 
(g) procedures, processes and administrative requirements for application, selection,  participation and exiting the sandbox are simple, easily intelligible, clearly 
 
communicated in order to facilitate the participation of SMEs and start-ups with  limited legal and administrative capacities and are streamlined across the Union,  in order to avoid fragmentation and that participation in a regulatory sandbox  established by a Member State, or by the EDPS is mutually and uniformly  
recognised and carries the same legal effects across the Union; 
(h) participation in the AI regulatory sandbox is limited to a period that is appropriate  to the complexity and scale of the project. This period may be extended by the  national competent authority; 
(i) the sandboxes shall facilitate the development of tools and infrastructure for  testing, benchmarking, assessing and explaining dimensions of AI systems  relevant for regulatory learning, such as accuracy, robustness and  
cybersecurity as well as measures to mitigate risks to fundamental rights and  the society at large.  
3. Prospective providers in the sandboxes, in particular SMEs and start-ups, shall be directed,  where relevant, to pre-deployment services such as guidance on the implementation of this  Regulation, to other value-adding services such as help with standardisation documents  and certification, Testing & Experimentation Facilities, Digital Hubs and Centres of  Excellence.  
4. When national competent authorities consider authorising testing in real world conditions  supervised within the framework of an AI regulatory sandbox established under this  Article, they shall specifically agree with the participants on the terms and conditions of  such testing and in particular on the appropriate safeguards with the view to protect  fundamental rights, health and safety. Where appropriate, they shall cooperate with other  national competent authorities with a view to ensure consistent practices across the Union. 
Article 54 
Further processing of personal data for developing certain AI systems in the public interest in  the AI regulatory sandbox 
1. In the AI regulatory sandbox personal data lawfully collected for other purposes may be  processed solely for the purposes of developing, training and testing certain AI systems in  the sandbox when all of the following conditions are met:
 
(a) AI systems shall be developed for safeguarding substantial public interest by a public  authority or another natural or legal person governed by public law or by private law  and in one or more of the following areas: 
(ii) public safety and public health, including disease detection, diagnosis  prevention, control and treatment and improvement of health care systems; 
(iii) a high level of protection and improvement of the quality of the environment,  protection of biodiversity, pollution as well as green transition, climate change  mitigation and adaptation; 
(iiia) energy sustainability; 
(iiib) safety and resilience of transport systems and mobility, critical infrastructure  and networks; 
(iiic) efficiency and quality of public administration and public services; 
(b) the data processed are necessary for complying with one or more of the requirements  referred to in Title III, Chapter 2 where those requirements cannot be effectively  fulfilled by processing anonymised, synthetic or other non-personal data; 
(c) there are effective monitoring mechanisms to identify if any high risks to the rights  and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU)  2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise during the  sandbox experimentation as well as response mechanism to promptly mitigate those  risks and, where necessary, stop the processing;  
(d) any personal data to be processed in the context of the sandbox are in a functionally  separate, isolated and protected data processing environment under the control of the  prospective provider and only authorised persons have access to that those data; 
(e) providers can only further share the originally collected data in compliance with EU  data protection law. Any personal data crated in the sandbox cannot be shared  outside the sandbox;
 
(f) any processing of personal data in the context of the sandbox do not lead to measures  or decisions affecting the data subjects nor affect the application of their rights laid  down in Union law on the protection of personal data; 
(g) any personal data processed in the context of the sandbox are protected by means of  appropriate technical and organisational measures and deleted once the participation  in the sandbox has terminated or the personal data has reached the end of its retention  period; 
(h) the logs of the processing of personal data in the context of the sandbox are kept for  the duration of the participation in the sandbox, unless provided otherwise by Union  or national law; 
(i) complete and detailed description of the process and rationale behind the training,  testing and validation of the AI system is kept together with the testing results as part  of the technical documentation in Annex IV; 
(j) a short summary of the AI project developed in the sandbox, its objectives and  expected results published on the website of the competent authorities. This  obligation shall not cover sensitive operational data in relation to the activities of law  enforcement, border control, immigration or asylum authorities. 
1a. For the purpose of prevention, investigation, detection or prosecution of criminal offences  or the execution of criminal penalties, including the safeguarding against and the  prevention of threats to public security, under the control and responsibility of law  enforcement authorities, the processing of personal data in AI regulatory sandboxes shall  be based on a specific Member State or Union law and subject to the same cumulative  conditions as referred to in paragraph 1. 
2. Paragraph 1 is without prejudice to Union or Member States legislation excluding  processing for other purposes than those explicitly mentioned in that legislation, as well as  to Union or Member States laws laying down the basis for the processing of personal data  which is necessary for the purpose of developing, testing and training of innovative AI  systems or any other legal basis, in compliance with Union law on the protection of  personal data.
 
Article 54a 
Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes 
1. Testing of AI systems in real world conditions outside AI regulatory sandboxes may be  conducted by providers or prospective providers of high-risk AI systems listed in Annex  III, in accordance with the provisions of this Article and the real-world testing plan  referred to in this Article, without prejudice to the prohibitions under Article 5. 
The detailed elements of the real world testing plan shall be specified in implementing acts  adopted by the Commission in accordance with the examination procedure referred to in  Article 74(2). 
This provision shall be without prejudice to Union or national law for the testing in real  world conditions of high-risk AI systems related to products covered by legislation listed  in Annex II. 
2. Providers or prospective providers may conduct testing of high-risk AI systems referred to  in Annex III in real world conditions at any time before the placing on the market or  putting into service of the AI system on their own or in partnership with one or more  prospective deployers.  
3. The testing of high-risk AI systems in real world conditions under this Article shall be  without prejudice to ethical review that may be required by national or Union law. 
4. Providers or prospective providers may conduct the testing in real world conditions only  where all of the following conditions are met: 
(a) the provider or prospective provider has drawn up a real world testing plan and  submitted it to the market surveillance authority in the Member State(s) where the  testing in real world conditions is to be conducted; 
(b) the market surveillance authority in the Member State(s) where the testing in real  world conditions is to be conducted has approved the testing in real world conditions  and the real world testing plan. Where the market surveillance authority in that  Member State has not provided with an answer in 30 days, the testing in real world  conditions and the real world testing plan shall be understood as approved. In cases 
 
where national law does not foresee a tacit approval, the testing in real world  conditions shall be subject to an authorisation; 
(c) the provider or prospective provider with the exception of high-risk AI systems  referred to in Annex III, points 1, 6 and 7 in the areas of law enforcement, migration,  asylum and border control management, and high risk AI systems referred to in  Annex III point 2, has registered the testing in real world conditions in the non-public  part of the EU database referred to in Article 60(3) with a Union-wide unique single  identification number and the information specified in Annex VIIIa;  
(d) the provider or prospective provider conducting the testing in real world conditions is  established in the Union or it has appointed a legal representative who is established  in the Union; 
(e) data collected and processed for the purpose of the testing in real world conditions  shall only be transferred to third countries outside the Union provided appropriate  and applicable safeguards under Union law are implemented; 
(f) the testing in real world conditions does not last longer than necessary to achieve its  objectives and in any case not longer than 6 months, which may be extended for an  additional amount of 6 months, subject to prior notification by the provider to the  market surveillance authority, accompanied by an explanation on the need for such  time extension; 
(g) persons belonging to vulnerable groups due to their age, physical or mental disability  are appropriately protected; 
(h) where a provider or prospective provider organises the testing in real world  conditions in cooperation with one or more prospective deployers, the latter have  been informed of all aspects of the testing that are relevant to their decision to  participate, and given the relevant instructions on how to use the AI system referred  to in Article 13; the provider or prospective provider and the deployer(s) shall  conclude an agreement specifying their roles and responsibilities with a view to  ensuring compliance with the provisions for testing in real world conditions under  this Regulation and other applicable Union and Member States legislation;
 
(i) the subjects of the testing in real world conditions have given informed consent in  accordance with Article 54b, or in the case of law enforcement, where the seeking of  informed consent would prevent the AI system from being tested, the testing itself  and the outcome of the testing in the real world conditions shall not have any  negative effect on the subject and his or her personal data shall be deleted after the  test is performed; 
(j) the testing in real world conditions is effectively overseen by the provider or  prospective provider and deployer(s) with persons who are suitably qualified in the  relevant field and have the necessary capacity, training and authority to perform their  tasks; 
(k) the predictions, recommendations or decisions of the AI system can be effectively  reversed and disregarded. 
5 Any subject of the testing in real world conditions, or his or her legally designated  representative, as appropriate, may, without any resulting detriment and without having to  provide any justification, withdraw from the testing at any time by revoking his or her  informed consent and request the immediate and permanent deletion of their personal data.  The withdrawal of the informed consent shall not affect the activities already carried out. 
5a. In accordance with Article 63a, Member States shall confer their market surveillance  authorities the powers of requiring providers and prospective providers information, of  carrying out unannounced remote or on-site inspections and on performing checks on the  development of the testing in real world conditions and the related products. Market  surveillance authorities shall use these powers to ensure a safe development of these tests. 
6. Any serious incident identified in the course of the testing in real world conditions shall be  reported to the national market surveillance authority in accordance with Article 62 of this  Regulation. The provider or prospective provider shall adopt immediate mitigation  measures or, failing that, suspend the testing in real world conditions until such mitigation  takes place or otherwise terminate it. The provider or prospective provider shall establish a  procedure for the prompt recall of the AI system upon such termination of the testing in  real world conditions.
 
7. Providers or prospective providers shall notify the national market surveillance authority in  the Member State(s) where the testing in real world conditions is to be conducted of the  suspension or termination of the testing in real world conditions and the final outcomes. 
8. The provider and prospective provider shall be liable under applicable Union and Member  States liability legislation for any damage caused in the course of their participation in the  testing in real world conditions. 
Article 54b  
Informed consent to participate in testing in real world conditions outside AI regulatory  sandboxes 
1. For the purpose of testing in real world conditions under Article 54a, informed consent  shall be freely given by the subject of testing prior to his or her participation in such testing  and after having been duly informed with concise, clear, relevant, and understandable  information regarding: 
(i) the nature and objectives of the testing in real world conditions and the possible  inconvenience that may be linked to his or her participation;  
(ii) the conditions under which the testing in real world conditions is to be conducted,  including the expected duration of the subject's participation;  
(iii) the subject's rights and guarantees regarding participation, in particular his or her right  to refuse to participate in and the right to withdraw from testing in real world  conditions at any time without any resulting detriment and without having to provide  any justification;  
(iv) the modalities for requesting the reversal or the disregard of the predictions,  recommendations or decisions of the AI system;  
(v) the Union-wide unique single identification number of the testing in real world  conditions in accordance with Article 54a(4c) and the contact details of the provider or  its legal representative from whom further information can be obtained. 
2 The informed consent shall be dated and documented and a copy shall be given to the  subject or his or her legal representative.
 
Article 55 
Measures for providers and deployers, in particular SMEs, including start-ups 1. Member States shall undertake the following actions: 
(a) provide SMEs, including start-ups, having a registered office or a branch in the  Union, with priority access to the AI regulatory sandboxes, to the extent that they  fulfil the eligibility conditions and selection criteria. The priority access shall not  preclude other SMEs including start-ups other than those referred to in the first  subparagraph to access to the AI regulatory sandbox, provided that they fulfil the  eligibility conditions and selection criteria;  
(b) organise specific awareness raising and training activities on the application of this  Regulation tailored to the needs of SMEs including start-ups, users and, as  appropriate, local public authorities; 
(c) utilise existing dedicated channels and where appropriate, establish new ones for  communication with SMEs including start-ups, users, other innovators and, as  appropriate, local public authorities to provide advice and respond to queries about  the implementation of this Regulation, including as regards participation in AI  regulatory sandboxes;  
(ca) facilitate the participation of SMEs and other relevant stakeholders in the  standardisation development process. 
2. The specific interests and needs of the SME providers, including start-ups, shall be taken  into account when setting the fees for conformity assessment under Article 43, reducing  those fees proportionately to their size, market size and other relevant indicators. 
2a. The AI Office shall undertake the following actions: 
(a) upon request of the AI Board, provide standardised templates for the areas covered  by this Regulation; 
(b) develop and maintain a single information platform providing easy to use  information in relation to this Regulation for all operators across the Union; 
 
(c) organise appropriate communication campaigns to raise awareness about the  obligations arising from this Regulation; 
(d) evaluate and promote the convergence of best practices in public procurement  procedures in relation to AI systems. 
Article 55a 
Derogations for specific operators 
2b. Microenterprises as defined in Article 2(3) of the Annex to the Commission  Recommendation 2003/361/EC concerning the definition of micro, small and medium sized enterprises, provided those enterprises do not have partner enterprises or linked  enterprises as defined in Article 3 of the same Annex may fulfil certain elements of the  quality management system required by Article 17 of this Regulation in a simplified  manner. For this purpose, the Commission shall develop guidelines on the elements of the  quality management system which may be fulfilled in a simplified manner considering the  needs of micro enterprises without affecting the level of protection and the need for  compliance with the requirements for high-risk AI systems.  
2c. Paragraph 1 shall not be interpreted as exempting those operators from fulfilling any other  requirements and obligations laid down in this Regulation, including those established in  Articles 9, 10, 11, 12, 13, 14, 15, 61 and 62. 
TITLE VI 
GOVERNANCE 
Article 55b 
Governance at Union level  
1. The Commission shall develop Union expertise and capabilities in the field of artificial  intelligence. For this purpose, the Commission has established the European AI Office by  Decision […]. 
 
2. Member States shall facilitate the tasks entrusted to the AI Office, as reflected in this Regulation.  
Chapter 1 
EUROPEAN ARTIFICIAL INTELLIGENCE BOARD 
Article 56 
Establishment and structure of the European Artificial Intelligence Board  1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established.  
2. The Board shall be composed of one representative per Member State. The European Data  Protection Supervisor shall participate as observer. The AI Office shall also attend the  Board’s meetings without taking part in the votes. Other national and Union authorities,  bodies or experts may be invited to the meetings by the Board on a case by case basis,  where the issues discussed are of relevance for them.  
2a. Each representative shall be designated by their Member State for a period of 3 years,  renewable once.  
2b. Member States shall ensure that their representatives in the Board:  
(a) have the relevant competences and powers in their Member State so as to contribute  actively to the achievement of the Board’s tasks referred to in Article 58;  
(b) are designated as a single contact point vis-à-vis the Board and, where appropriate,  taking into account Member States’ needs, as a single contact point for stakeholders;  
(c) are empowered to facilitate consistency and coordination between national competent  authorities in their Member State as regards the implementation of this Regulation,  including through the collection of relevant data and information for the purpose of  fulfilling their tasks on the Board. 
 
3. The designated representatives of the Member States shall adopt the Board’s rules of  procedure by a two-thirds majority. The rules of procedure shall, in particular, lay down  procedures for the selection process, duration of mandate and specifications of the tasks of  the Chair, the voting modalities, and the organisation of the Board’s activities and its sub groups. 
3a. The Board shall establish two standing sub-groups to provide a platform for cooperation  and exchange among market surveillance authorities and notifying authorities on issues  related to market surveillance and notified bodies respectively.  
The standing sub-group for market surveillance should act as the Administrative  Cooperation Group (ADCO) for this Regulation in the meaning of Article 30 of Regulation  (EU) 2019/1020. 
The Board may establish other standing or temporary sub-groups as appropriate for the  purpose of examining specific issues. Where appropriate, representatives of the advisory  forum as referred to in Article 58a may be invited to such sub-groups or to specific  meetings of those subgroups in the capacity of observers. 
3b. The Board shall be organised and operated so as to safeguard the objectivity and  impartiality of its activities. 
4. The Board shall be chaired by one of the representatives of the Member States. The  European AI Office shall provide the Secretariat for the Board, convene the meetings upon  request of the Chair and prepare the agenda in accordance with the tasks of the Board  pursuant to this Regulation and its rules of procedure. 
Article 58 
Tasks of the Board 
The Board shall advise and assist the Commission and the Member States in order to  facilitate the consistent and effective application of this Regulation. For this purpose the  Board may in particular: 
 
(a) contribute to the coordination among national competent authorities responsible for  the application of this Regulation and, in cooperation and subject to agreement of the  concerned market surveillance authorities, support joint activities of market  surveillance authorities referred to in Article 63(7a); 
(b) collect and share technical and regulatory expertise and best practices among  Member States;  
(c) provide advice in the implementation of this Regulation, in particular as regards the  enforcement of rules on general purpose AI models; 
(d) contribute to the harmonisation of administrative practices in the Member States,  including in relation to the derogation from the conformity assessment procedures  referred to in Article 47, the functioning of regulatory sandboxes and testing in real  world conditions referred to in Articles 53, 54 and 54a;  
(e) upon the request of the Commission or on its own initiative, issue recommendations  and written opinions on any relevant matters related to the implementation of this  Regulation and to its consistent and effective application, including:  
(i) on the development and application of codes of conduct and code of practice  pursuant to this Regulation as well as the Commission’s guidelines; 
(ii) the evaluation and review of this Regulation pursuant to Article 84, including  as regards the serious incident reports referred to in Article 62 and the  functioning of the database referred to in Article 60, the preparation of the  delegated or implementing acts, and possible alignments of this Regulation  with the legal acts listed in Annex II; 
(iii) on technical specifications or existing standards regarding the requirements set  out in Title III, Chapter 2;  
(iv) on the use of harmonised standards or common specifications referred to in  Articles 40 and 41;  
(v) trends, such as European global competitiveness in artificial intelligence, the  uptake of artificial intelligence in the Union and the development of digital  skills; 
 
(via) trends on the evolving typology of AI value chains, in particular on the  resulting implications in terms of accountability;  
(vi) on the potential need for amendment of Annex III in accordance with Article 7  and on the potential need for possible revision of Article 5 pursuant to Article  84, taking into account relevant available evidence and the latest developments  in technology;  
(f) support the Commission in promoting AI literacy, public awareness and  understanding of the benefits, risks, safeguards and rights and obligations in relation  to the use of AI systems; 
(g) facilitate the development of common criteria and a shared understanding among  market operators and competent authorities of the relevant concepts provided for in  this Regulation, including by contributing to the development of benchmarks; 
(h) cooperate, as appropriate, with other Union institutions, bodies, offices and agencies,  as well as relevant Union expert groups and networks in particular in the fields of  product safety, cybersecurity, competition, digital and media services, financial  services, consumer protection, data and fundamental rights protection;  
(i) contribute to the effective cooperation with the competent authorities of third  countries and with international organisations; 
(j) assist national competent authorities and the Commission, in developing the  organisational and technical expertise required for the implementation of this  Regulation, including by contributing to the assessment of training needs for staff of  Member States involved in implementing this Regulation; 
(j1) assist the AI Office in supporting national competent authorities in the establishment  and development of regulatory sandboxes and facilitate cooperation and information sharing among regulatory sandboxes; 
(k) contribute and provide relevant advice in the development of guidance documents;  (l) advise the Commission in relation to international matters on artificial intelligence;
 
(m) provide opinions to the Commission on the qualified alerts regarding general purpose  AI models; 
(n) receive opinions by the Member states on the qualified alerts regarding general  purpose AI models and on national experiences and practices on the monitoring and  enforcement of the AI systems, in particular systems integrating the general purpose  AI models. 
Article 58a 
 Advisory forum  
1. An advisory forum shall be established to advise and provide technical expertise to the  Board and the Commission to contribute to their tasks under this Regulation.  
2. The membership of the advisory forum shall represent a balanced selection of  stakeholders, including industry, start-ups, SMEs, civil society and academia. The  membership of the advisory forum shall be balanced with regard to commercial and non commercial interests and, within the category of commercial interests, with regards to  SMEs and other undertakings.  
3. The Commission shall appoint the members of the advisory forum, in accordance with the  criteria set out in the previous paragraph, among stakeholders with recognised expertise in  the field of AI.  
4. The term of office of the members of the advisory forum shall be two years, which may be  extended by up to no more than four years.  
5. The Fundamental Rights Agency, European Union Agency for Cybersecurity, the  European Committee for Standardization (CEN), the European Committee for  Electrotechnical Standardization (CENELEC), and the European Telecommunications  Standards Institute (ETSI) shall be permanent members of the advisory forum.  
6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from  among its members, in accordance with criteria set out in paragraph 2. The term of office  of the co-chairs shall be two years, renewable once. 
 
7. The advisory forum shall hold meetings at least two times a year. The advisory forum may  invite experts and other stakeholders to its meetings.  
8. In fulfilling its role as set out in paragraph 1, the advisory forum may prepare opinions,  recommendations and written contributions upon request of the Board or the Commission.  
9. The advisory forum may establish standing or temporary subgroups as appropriate for the  purpose of examining specific questions related to the objectives of this Regulation.  
10. The advisory forum shall prepare an annual report of its activities. That report shall be  made publicly available.  
Chapter 1a 
SCIENTIFIC PANEL OF INDEPENDENT EXPERTS 
Article 58b 
Scientific panel of independent experts 
1. The Commission shall, by means of an implementing act, make provisions on the  establishment of a scientific panel of independent experts (the ‘scientific panel’) intended  to support the enforcement activities under this Regulation. Those implementing acts shall  be adopted in accordance with the examination procedure referred to in Article 74(2).  
2. The scientific panel shall consist of experts selected by the Commission on the basis of up to-date scientific or technical expertise in the field of artificial intelligence necessary for  the tasks set out in paragraph 3, and shall be able to demonstrate meeting all of the  following conditions:  
(a) particular expertise and competence and scientific or technical expertise in the field  of artificial intelligence;  
(b) independence from any provider of AI systems or general purpose AI models or  systems; 
 
(c) ability to carry out activities diligently, accurately and objectively.  
The Commission, in consultation with the AI Board, shall determine the number of experts  in the panel in accordance with the required needs and shall ensure fair gender and  geographical representation.  
3. The scientific panel shall advise and support the European AI Office, in particular with  regard to the following tasks:  
(a) support the implementation and enforcement of this Regulation as regards general  purpose AI models and systems, in particular by 
(i) alerting the AI Office of possible systemic risks at Union level of general  purpose AI models, in accordance with Article 68h [Alerts of systemic risks  by the scientific panel]; 
(ii) contributing to the development of tools and methodologies for evaluating  capabilities of general purpose AI models and systems, including through  
benchmarks;  
(iii) providing advice on the classification of general purpose AI models with  systemic risk;  
(iiii) providing advice on the classification of different general purpose AI models  and systems;  
(iv) contributing to the development of tools and templates; 
(b) support the work of market surveillance authorities, at their request; 
(c) support cross-border market surveillance activities as referred to in Article  63(7a), without prejudice of the powers of market surveillance authorities;  
(d) support the AI Office when carrying out its duties in the context of the safeguard  clause pursuant to Article 66. 
4. The experts shall perform their tasks with impartiality, objectivity and ensure the  confidentiality of information and data obtained in carrying out their tasks and activities.  They shall neither seek nor take instructions from anyone when exercising their tasks 
 
under paragraph 3. Each expert shall draw up a declaration of interests, which shall be  made publicly available. The AI Office shall establish systems and procedures to actively  manage and prevent potential conflicts of interest.  
5. The implementing act referred to in paragraph 1 shall include provisions on the conditions,  procedure and modalities for the scientific panel and its members to issue alerts and  request the assistance of the AI Office to the performance of its tasks. 
Article 58c 
Access to the pool of experts by the Member States  
1. Member States may call upon experts of the scientific panel to support their enforcement  activities under this Regulation. 
2. The Member States may be required to pay fees for the advice and support by the experts.  The structure and the level of fees as well as the scale and structure of recoverable costs  shall be set out in the implementing act referred to in Article 58b(1), taking into account  the objectives of the adequate implementation of this Regulation, cost-effectiveness and  the necessity to ensure an effective access to experts by all Member States. 
3. The Commission shall facilitate timely access to the experts by the Member States, as  needed, and ensure that the combination of support activities carried out by EU AI testing  support pursuant to Article 68a and experts pursuant to this Article is efficiently organised  and provides the best possible added value.
 
Chapter 2 
NATIONAL COMPETENT AUTHORITIES 
Article 59 
Designation of national competent authorities and single point of contact 
2. Each Member State shall establish or designate at least one notifying authority and at least  one market surveillance authority for the purpose of this Regulation as national competent  authorities. These national competent authorities shall exercise their powers independently,  impartially and without bias so as to safeguard the principles of objectivity of their  activities and tasks and to ensure the application and implementation of this Regulation.  The members of these authorities shall refrain from any action incompatible with their  duties. Provided that those principles are respected, such activities and tasks may be  performed by one or several designated authorities, in accordance with the organisational  needs of the Member State.  
3. Member States shall communicate to the Commission the identity of the notifying  authorities and the market surveillance authorities and the tasks of those authorities and as  well as any subsequent changes thereto. Member States shall make publicly available  information on how competent authorities and single point of contact can be contacted,  through electronic communication means by… [12 months after the date of entry into force  of this Regulation]. Member States shall designate a market surveillance authority to act as  single point of contact for this Regulation and notify the Commission of the identity of the  single point of contact. The Commission shall make a list of the single points of contact  publicly available. 
4. Member States shall ensure that the national competent authority is provided with adequate  technical, financial and human resources, and infrastructure to fulfil their tasks effectively  under this Regulation. In particular, the national competent authority shall have a sufficient  number of personnel permanently available whose competences and expertise shall include  an in-depth understanding of artificial intelligence technologies, data and data computing,  personal data protection, cybersecurity, fundamental rights, health and safety risks and  knowledge of existing standards and legal requirements. Member States shall assess and, if 
 
deemed necessary, update competence and resource requirements referred to in this  paragraph on an annual basis. 
4a. National competent authorities shall satisfy an adequate level of cybersecurity measures. 
4c. When performing their tasks, the national competent authorities shall act in compliance  with the confidentiality obligations set out in Article 70. 
5. By one year after entry into force of this Regulation and once every two years thereafter  Member States shall report to the Commission on the status of the financial and human  resources of the national competent authorities with an assessment of their adequacy. The  Commission shall transmit that information to the Board for discussion and possible  recommendations. 
6. The Commission shall facilitate the exchange of experience between national competent  authorities. 
7. National competent authorities may provide guidance and advice on the implementation of  this Regulation, in particular to SMEs including start-ups, taking into account the Board's  and the Commission’s guidance and advice, as appropriate. Whenever national competent  authorities intend to provide guidance and advice with regard to an AI system in areas  covered by other Union legislation, the competent national authorities under that Union  legislation shall be consulted, as appropriate. 
8. When Union institutions, agencies and bodies fall within the scope of this Regulation, the  European Data Protection Supervisor shall act as the competent authority for their  supervision.
 
TITLE VII 
EU DATABASE FOR HIGH-RISK AI SYSTEMS LISTED IN  ANNEX III 
Article 60 
EU database for high-risk AI systems listed in Annex III 
1. The Commission shall, in collaboration with the Member States, set up and maintain a EU  database containing information referred to in paragraphs 2 and 2a concerning high-risk AI  systems referred to in Article 6(2) which are registered in accordance with Articles 51 and  54a. When setting the functional specifications of such database, the Commission shall  consult the relevant experts, and when updating the functional specifications of such  database, the Commission shall consult the AI Board. 
2. The data listed in Annex VIII, Section A, shall be entered into the EU database by the  provider or where applicable the authorised representative. 
2a. The data listed in Annex VIII, Section B, shall be entered into the EU database by the  deployer who is or who acts on behalf of public authorities, agencies or bodies, according  to articles 51(1a) and (1b). 
3. With the exception of the section referred to in Article 51(1c) and Article 54a(5), the  information contained in the EU database registered in accordance with Article 51 shall be  accessible and publicly available in a user friendly manner. The information should be  easily navigable and machine-readable. The information registered in accordance with  Article 54a shall be accessible only to market surveillance authorities and the Commission,  unless the prospective provider or provider has given consent for making this information  also accessible the public.  
4. The EU database shall contain personal data only insofar as necessary for collecting and  processing information in accordance with this Regulation. That information shall include  the names and contact details of natural persons who are responsible for registering the  system and have the legal authority to represent the provider or the deployer, as applicable.
 
5. The Commission shall be the controller of the EU database. It shall make available to  providers, prospective providers and deployers adequate technical and administrative  support. The database shall comply with the applicable accessibility requirements. 
TITLE VIII 
POST-MARKET MONITORING, INFORMATION SHARING,  MARKET SURVEILLANCE 
Chapter 1 
POST-MARKET MONITORING 
Article 61 
Post-market monitoring by providers and post-market monitoring plan for high-risk AI  systems 
1. Providers shall establish and document a post-market monitoring system in a manner that  is proportionate to the nature of the artificial intelligence technologies and the risks of the  high-risk AI system. 
2. The post-market monitoring system shall actively and systematically collect, document and  analyse relevant data which may be provided by deployers or which may be collected  through other sources on the performance of high-risk AI systems throughout their  lifetime, and allow the provider to evaluate the continuous compliance of AI systems with  the requirements set out in Title III, Chapter 2. Where relevant, post-market monitoring  shall include an analysis of the interaction with other AI systems. This obligation shall not  cover sensitive operational data of deployers which are law enforcement authorities. 
3. The post-market monitoring system shall be based on a post-market monitoring plan. The  post-market monitoring plan shall be part of the technical documentation referred to in  Annex IV. The Commission shall adopt an implementing act laying down detailed  provisions establishing a template for the post-market monitoring plan and the list of 
 
elements to be included in the plan by six months before the entry into application of this  Regulation. 
4. For high-risk AI systems covered by the legal acts referred to in Annex II, Section A,  where a post-market monitoring system and plan is already established under that  legislation, in order to ensure consistency, avoid duplications and minimise additional  burdens, providers shall have a choice to integrate, as appropriate, the necessary elements  described in paragraphs 1, 2 and 3 using the template referred in paragraph 3 into already  existing system and plan under the Union harmonisation legislation listed in Annex II,  Section A, provided it achieves an equivalent level of protection. 
The first subparagraph shall also apply high-risk AI systems referred to in point 5 of  Annex III placed on the market or put into service by financial institutions that are subject  to requirements regarding their internal governance, arrangements or processes under  Union financial services legislation.  
Chapter 2 
SHARING OF INFORMATION ON SERIOUS INCIDENTS 
Article 62 
Reporting of serious incidents  
1. Providers of high-risk AI systems placed on the Union market shall report any serious  incident to the market surveillance authorities of the Member States where that incident  occurred. 
1a. As a general rule, the period for the reporting referred to in paragraph 1 shall take account  of the severity of the serious incident. 
1b. The notification referred to in paragraph 1 shall be made immediately after the provider  has established a causal link between the AI system and the serious incident or the  reasonable likelihood of such a link, and, in any event, not later than 15 days after the  provider or, where applicable, the deployer, becomes aware of the serious incident. 
 
1c. Notwithstanding paragraph 1b, in the event of a widespread infringement or a serious  incident as defined in Article 3(44) point (b) the report referred to in paragraph 1 shall be  provided immediately, and not later than 2 days after the provider or, where applicable, the  deployer becomes aware of that incident. 
1d. Notwithstanding paragraph 1b, in the event of death of a person the report shall be  provided immediately after the provider or the deployer has established or as soon as it  suspects a causal relationship between the high-risk AI system and the serious incident but  not later than 10 days after the date on which the provider or, where applicable, the  deployer becomes aware of the serious incident. 
1e. Where necessary to ensure timely reporting, the provider or, where applicable, the  deployer, may submit an initial report that is incomplete followed up by a complete report. 
1a. Following the reporting of a serious incident pursuant to the first subparagraph, the  provider shall, without delay, perform the necessary investigations in relation to the serious  incident and the AI system concerned. This shall include a risk assessment of the incident  and corrective action. The provider shall co-operate with the competent authorities and  where relevant with the notified body concerned during the investigations referred to in the  first subparagraph and shall not perform any investigation which involves altering the AI  system concerned in a way which may affect any subsequent evaluation of the causes of  the incident, prior to informing the competent authorities of such action. 
2. Upon receiving a notification related to a serious incident referred to in Article 3(44)(c),  the relevant market surveillance authority shall inform the national public authorities or  bodies referred to in Article 64(3). The Commission shall develop dedicated guidance to  facilitate compliance with the obligations set out in paragraph 1. That guidance shall be  issued 12 months after the entry into force of this Regulation, at the latest, and shall be  assessed regularly. 
2a. The market surveillance authority shall take appropriate measures, as provided in Article  19 of the Regulation 2019/1020, within 7 days from the date it received the notification  referred to in paragraph 1 and follow the notification procedures as provided in the  Regulation 2019/1020. 
3. For high-risk AI systems referred to in Annex III that are placed on the market or put into  service by providers that are subject to Union legislative instruments laying down reporting 
 
obligations equivalent to those set out in this Regulation, the notification of serious  incidents shall be limited to those referred to in Article 3(44)(c).  
3a. For high-risk AI systems which are safety components of devices, or are themselves  devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746 the  notification of serious incidents shall be limited to those referred to in Article 3(44)(c) and  be made to the national competent authority chosen for this purpose by the Member States  where that incident occurred. 
3a. National competent authorities shall immediately notify the Commission of any serious  incident, whether or not it has taken action on it, in accordance with Article 20 of  Regulation 2019/1020. 
Chapter 3 
ENFORCEMENT  
Article 63 
Market surveillance and control of AI systems in the Union market 
1. Regulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation.  However, for the purpose of the effective enforcement of this Regulation: 
(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall be  understood as including all operators identified in Article 2(1) of this Regulation; 
(b) any reference to a product under Regulation (EU) 2019/1020 shall be understood as  including all AI systems falling within the scope of this Regulation. 
2. As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020,  the market surveillance authorities shall report annually, to the Commission and relevant  national competition authorities any information identified in the course of market  surveillance activities that may be of potential interest for the application of Union law on 
 
competition rules. They shall also annually report to the Commission about the use of  prohibited practices that occurred during that year and about the measures taken.  
3. For high-risk AI systems, related to products to which legal acts listed in Annex II, section  A apply, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for market surveillance activities designated under those legal acts.  By derogation from the previous paragraph in justified circumstances, Member States may  designate another relevant authority to act as a market surveillance authority provided that  coordination is ensured with the relevant sectoral market surveillance authorities  responsible for the enforcement of the legal acts listed in Annex II. 
3a. The procedures referred to in Articles 65, 66, 67 and 68 of this Regulation shall not apply  to AI systems related to products, to which legal acts listed in Annex II, section A apply,  when such legal acts already provide for procedures ensuring an equivalent level of  protection and having the same objective. In such a case, these sectoral procedures shall  apply instead. 
3b. Without prejudice to the powers of market surveillance authorities under Article 14 of  Regulation 2019/1020, for the purpose of ensuring the effective enforcement of this  Regulation, market surveillance authorities may exercise the powers referred to in Article  14(4)(d) and (j) of Regulation 2019/1020 remotely as appropriate.  
4. For high-risk AI systems placed on the market, put into service or used by financial  institutions regulated by Union legislation on financial services, the market surveillance  authority for the purposes of this Regulation shall be the relevant national authority  responsible for the financial supervision of those institutions under that legislation in so far  as the placement on the market, putting into service or the use of the AI system is in direct  connection with the provision of those financial services. 
4a. By way of a derogation from the previous subparagraph, in justified circumstances and  provided that coordination is ensured, another relevant authority may be identified by the  Member State as market surveillance authority for the purposes of this Regulation. 
National market surveillance authorities supervising regulated credit institutions regulated  under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism  (SSM) established by Council Regulation No 1204/2013, should report, without delay, to the  European Central Bank any information identified in the course of their market surveillance 
 
activities that may be of potential interest for the European Central Bank’s prudential  supervisory tasks as specified in that Regulation. 
5. For high-risk AI systems listed in point 1 in so far as the systems are used for law  enforcement purposes and for purposes listed in points 6, 7 and 8 of Annex III, Member  States shall designate as market surveillance authorities for the purposes of this Regulation  either the competent data protection supervisory authorities under Regulation 2016/679, or  Directive (EU) 2016/680 or any other authority designated pursuant to the same conditions  laid down in Articles 1 to 44 of Directive or Directive (EU) 2016/680. Market surveillance  activities shall in no way affect the independence of judicial authorities or otherwise  interfere with their activities when acting in their judicial capacity.  
6. Where Union institutions, agencies and bodies fall within the scope of this Regulation, the  European Data Protection Supervisor shall act as their market surveillance authority except  in relation to the Court of Justice acting in its judicial capacity. 
7. Member States shall facilitate the coordination between market surveillance authorities  designated under this Regulation and other relevant national authorities or bodies which  supervise the application of Union harmonisation legislation listed in Annex II or other  Union legislation that might be relevant for the high-risk AI systems referred to in Annex  III. 
7a. Market surveillance authorities and the Commission shall be able to propose joint  activities, including joint investigations, to be conducted by either market surveillance  authorities or market surveillance authorities jointly with the Commission, that have the  aim of promoting compliance, identifying non-compliance, raising awareness and  providing guidance in relation to this Regulation with respect to specific categories of  high-risk AI systems that are found to present a serious risk across several Member States  in accordance with Article 9 of the 2019/1020. The AI Office shall provide coordination  support for joint investigations.  
7a. Without prejudice to powers provided under Regulation (EU) 2019/1020, and where  relevant and limited to what is necessary to fulfil their tasks, the market surveillance  authorities shall be granted full access by the provider to the documentation as well as the  training, validation and testing datasets used for the development of the high-risk AI  system, including, where appropriate and subject to security safeguards, through 
 
application programming interfaces (‘API’) or other relevant technical means and tools  enabling remote access. 
7b. Market surveillance authorities shall be granted access to the source code of the high-risk  AI system upon a reasoned request and only when the following cumulative conditions are  fulfilled:  
(a) access to source code is necessary to assess the conformity of a high-risk AI system  with the requirements set out in Title III, Chapter 2; and 
(b) testing/auditing procedures and verifications based on the data and documentation  provided by the provider have been exhausted or proved insufficient. 
7c. Any information and documentation obtained by market surveillance authorities shall be  treated in compliance with the confidentiality obligations set out in Article 70. 
Article 63a 
Mutual Assistance, market surveillance and control of general purpose AI systems 
1. Where an AI system is based on a general purpose AI model and the model and the system  are developed by the same provider, the AI office shall have powers to monitor and  supervise compliance of this AI system with the obligations of this Regulation. To carry  monitoring and supervision tasks the AI Office shall have all the powers of a market  surveillance authority within the meaning of the Regulation 2019/1020. 
2. Where the relevant market surveillance authorities have sufficient reasons to consider that  general purpose AI systems that can be used directly by deployers for at least one purpose  that is classified as high-risk pursuant to this Regulation, is non-compliant with the  requirements laid down in this Regulation, it shall cooperate with the AI Office to carry out  evaluation of compliance and inform the Board and other market surveillance authorities  accordingly. 
3. When a national market surveillance authority is unable to conclude its investigation on the  high-risk AI system because of its inability to access certain information related to the  general purpose AI model despite having made all appropriate efforts to obtain that 
 
information, it may submit a reasoned request to the AI Office where access to this  information can be enforced. In this case the AI Office shall supply to the applicant  authority without delay, and in any event within 30 days, any information that the AI  Office considers to be relevant in order to establish whether a high-risk AI system is non compliant. National market authorities shall safeguard the confidentiality of the  information they obtain in accordance with the Article 70. The procedure provided in  Chapter VI of the Regulation (EU) 1020/2019 shall apply by analogy. 
Article 63b 
Supervision of testing in real world conditions by market surveillance authorities 
1. Market surveillance authorities shall have the competence and powers to ensure that testing  in real world conditions is in accordance with this Regulation. 
2. Where testing in real world conditions is conducted for AI systems that are supervised  within an AI regulatory sandbox under Article 54, the market surveillance authorities shall  verify the compliance with the provisions of Article 54a as part of their supervisory role  for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in  real world conditions to be conducted by the provider or prospective provider in derogation  to the conditions set out in Article 54a(4) (f) and (g). 
3. Where a market surveillance authority has been informed by the prospective provider, the  provider or any third party of a serious incident or has other grounds for considering that  the conditions set out in Articles 54a and 54b are not met, it may take any of the following  decisions on its territory, as appropriate: 
(a) suspend or terminate the testing in real world conditions; 
(b) require the provider or prospective provider and user(s) to modify any aspect of the  testing in real world conditions. 
4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of  this Article or has issued an objection within the meaning of Article 54a(4)(b), the decision  or the objection shall indicate the grounds thereof and the modalities and conditions for the  provider or prospective provider to challenge the decision or objection.
 
5. Where applicable, where a market surveillance authority has taken a decision referred to in  paragraph 3 of this Article, it shall communicate the grounds therefor to the market  surveillance authorities of the other Member States in which the AI system has been tested  in accordance with the testing plan. 
Article 64 
Powers of authorities protecting fundamental rights 
3. National public authorities or bodies which supervise or enforce the respect of obligations  under Union law protecting fundamental rights, including the right to non-discrimination,  in relation to the use of high-risk AI systems referred to in Annex III shall have the power  to request and access any documentation created or maintained under this Regulation in  
accessible language and format when access to that documentation is necessary for  effectively fulfilling their mandate within the limits of their jurisdiction. The relevant  public authority or body shall inform the market surveillance authority of the Member  State concerned of any such request. 
4. By three months after the entering into force of this Regulation, each Member State shall  identify the public authorities or bodies referred to in paragraph 3 and make a list publicly  available. Member States shall notify the list to the Commission and all other Member  States and keep the list up to date.  
5. Where the documentation referred to in paragraph 3 is insufficient to ascertain whether a  breach of obligations under Union law intended to protect fundamental rights has occurred,  the public authority or body referred to in paragraph 3 may make a reasoned request to the  market surveillance authority, to organise testing of the high-risk AI system through  technical means. The market surveillance authority shall organise the testing with the close  involvement of the requesting public authority or body within reasonable time following  the request. 
6. Any information and documentation obtained by the national public authorities or bodies  referred to in paragraph 3 pursuant to the provisions of this Article shall be treated in  compliance with the confidentiality obligations set out in Article 70.
 
Article 65 
Procedure for dealing with AI systems presenting a risk at national level 
1. AI systems presenting a risk shall be understood as a product presenting a risk defined in  Article 3, point 19 of Regulation (EU) 2019/1020 insofar as risks to the health or safety or  to fundamental rights of persons are concerned. 
2. Where the market surveillance authority of a Member State has sufficient reasons to  consider that an AI system presents a risk as referred to in paragraph 1, it shall carry out an  evaluation of the AI system concerned in respect of its compliance with all the  requirements and obligations laid down in this Regulation. Particular attention shall be  given to AI systems presenting a risk to vulnerable groups (referred to in Article 5). When  risks to fundamental rights are identified, the market surveillance authority shall also  inform and fully cooperate with the relevant national public authorities or bodies referred  to in Article 64(3). The relevant operators shall cooperate as necessary with the market  surveillance authority and the other national public authorities or bodies referred to in  Article 64(3). 
Where, in the course of that evaluation, the market surveillance authority and where  applicable in cooperation with the national public authority referred to in Article 64(3)  finds that the AI system does not comply with the requirements and obligations laid down  in this Regulation, it shall without undue delay require the relevant operator to take all  appropriate corrective actions to bring the AI system into compliance, to withdraw the AI  system from the market, or to recall it within a period it may prescribe and in any event no  later than fifteen working days or as provided for in the relevant Union harmonisation law  as applicable 
The market surveillance authority shall inform the relevant notified body accordingly.  Article 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the  second subparagraph.  
3. Where the market surveillance authority considers that non-compliance is not restricted to  its national territory, it shall inform the Commission, and the other Member States without  undue delay of the results of the evaluation and of the actions which it has required the  operator to take.
 
4. The operator shall ensure that all appropriate corrective action is taken in respect of all the  AI systems concerned that it has made available on the market throughout the Union. 
5. Where the operator of an AI system does not take adequate corrective action within the  period referred to in paragraph 2, the market surveillance authority shall take all  appropriate provisional measures to prohibit or restrict the AI system's being made  available on its national market or put into service, to withdraw the product or the  standalone AI system from that market or to recall it. That authority shall without undue  delay notify the Commission and the other Member States of those measures. 
6. The notification referred to in paragraph 5 shall include all available details, in particular  the information necessary for the identification of the non-compliant AI system, the origin  of the AI system and the supply chain, the nature of the non-compliance alleged and the  risk involved, the nature and duration of the national measures taken and the arguments put  forward by the relevant operator. In particular, the market surveillance authorities shall  indicate whether the non-compliance is due to one or more of the following: 
(-a) non-compliance with the prohibition of the artificial intelligence practices referred to  in Article 5; 
(a) a failure of a high-risk AI system to meet requirements set out in Title III, Chapter 2; 
(b) shortcomings in the harmonised standards or common specifications referred to in  Articles 40 and 41 conferring a presumption of conformity; 
(ba) non-compliance with provisions set out in Article 52. 
7. The market surveillance authorities of the Member States other than the market  surveillance authority of the Member State initiating the procedure shall without undue  delay inform the Commission and the other Member States of any measures adopted and of  any additional information at their disposal relating to the non-compliance of the AI  system concerned, and, in the event of disagreement with the notified national measure, of  their objections. 
8. Where, within three months of receipt of the notification referred to in paragraph 5, no  objection has been raised by either a market surveillance authority of a Member State or  the Commission in respect of a provisional measure taken by a market surveillance  authority of another Member State, that measure shall be deemed justified. This is without