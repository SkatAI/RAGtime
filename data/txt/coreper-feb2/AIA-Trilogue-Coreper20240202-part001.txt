Council of the 
European Union 
Brussels, 26 January 2024 
(OR. en) 
5662/24 
Interinstitutional File: 
2021/0106(COD) 
NOTE 
From: Presidency 
LIMITE 
TELECOM 22 
JAI 98 
COPEN 18 
CYBER 14 
DATAPROTECT 32 EJUSTICE 3 
COSI 6 
IXIM 15 
ENFOPOL 21 
RELEX 77 
MI 65 
COMPET 68 
CODEC 133 
To: Permanent Representatives Committee 
No. Cion doc.: 8115/21 
Subject: Proposal for a Regulation of the European Parliament and of the Council  laying down harmonised rules on artificial intelligence (Artificial Intelligence  Act) and amending certain Union legislative acts 
- Analysis of the final compromise text with a view to agreement 
I. INTRODUCTION 
1. The Commission adopted the proposal for a Regulation laying down harmonised rules on  artificial intelligence (Artificial Intelligence Act, hereinafter: the AI Act) on 21 April 2021. 
2. The Council unanimously adopted its General Approach on the proposal on 6 December  2022, while the European Parliament (hereinafter: the EP) confirmed its position in a plenary  vote on 14 June 2023. 
3. On 14 June 2023, 18 July 2023, 2-3 October 2023 and 24 October 2023 the first four political  trilogues were held, during which some of the less controversial parts of the proposal were  agreed and compromise was also found on the provisions concerning measures in support of 
 
innovation, as well as and on the mechanism for classification of AI systems as high-risk.  Moreover, during those initial trilogues the co-legislators explored potential landing zones  with regard to the remaining issues, in particular the regulation of general purpose AI models and systems, governance, as well the prohibitions and law enforcement package. 
4. On 29 November 2023 and 1 December 2023, the Committee of Permanent Representatives  granted the Presidency a revised mandate to continue the negotiations on all the outstanding  elements of the proposal. The fifth and final trilogue was held between 6 and 8 December  2023. During this extended round of negotiations the Council and the EP came to an  agreement on all political issues and successfully closed the interinstitutional negotiations.  Following this political agreement, the co-legislators proceeded to carry out further technical  work in January 2024 in order to align the text of the recitals with the text of the articles as  agreed during the final trilogue.  
5. In the Annex to this document delegations will find the text of the proposal for a Regulation  laying down harmonised rules on artificial intelligence (the AI Act), updated according to the  provisional political agreement reached at the fifth trilogue between 6 and 8 December 2023. 
II. MAIN ELEMENTS OF THE COMPROMISE 
1. Subject matter and scope 
As regards the subject matter, the compromise text of Article 1(1) includes a high-level  statement that one of the purposes of the AI Act is to ensure a high level of protection of  health, safety and fundamental rights enshrined in the Charter, which includes democracy,  rule of law and environmental protection. However, all subsequent references in the text to  the risks addressed by the Regulation only include risks to health, safety and fundamental  rights, in line with the Council’s mandate. 
Concerning the scope, the compromise text makes it clear that national security is excluded.  The wording concerning this exclusion in Article 2(3) has been fine tuned to align it more  closely with the respective language used in recently agreed legal acts, such as the Cyber  Resilience Act and the Data Act, while the text of the corresponding Recital 12a has  remained the same as in the Council’s mandate. 
 
2. Definition of an AI system 
The definition of an AI system in Article 3(1) has been modified to align it more closely  with the work of international organisations working on artificial intelligence, notably the  OECD. Moreover, the corresponding Recital 6 details further the key characteristics of the  definition and clarifies that the definition is not intended to cover simpler traditional  software systems or programming approaches, which are based on the rules defined solely  by natural persons to automatically execute operations. Additionally, the Commission has  been tasked to develop guidelines on the application of the definition of an AI system.  
3. Prohibited AI practices 
Regarding the list of prohibited AI practices in Article 5, the compromise text includes the  changes as previously accepted by the Committee of Permanent Representatives in the  Council’s revised mandate. This means that the list includes the prohibition of real-time  biometric identification by law enforcement authorities in publicly accessible spaces, but  with some notable and clearly defined exceptions, as listed in Article 5(1)(d), which are  now subject to a range of safeguards, including monitoring and oversight measures and  limited reporting obligations at EU level. Additional accepted prohibitions include  untargeted scraping of facial images for the purpose of a creating or expanding facial  recognition databases, emotion recognition but only at the workplace and in educational  institutions (and with exceptions for safety and medical reasons), a very limited prohibition  of biometric categorization based on certain specific beliefs or characteristics, as well as a  limited and targeted ban on individual predictive policing. The wording on the predictive  policing ban has been slightly modified and it now covers systems which assess or predict  the risk of a natural person to commit a criminal offence, based solely on the profiling of a  natural person or on assessing their personality traits and characteristics. However, this  prohibition will not apply to AI systems used to support the human assessment of the  involvement of a person in a criminal activity.  
Finally, it is important to point out that as part of the compromise other biometric  categorisation systems and emotion recognition systems, as well as post-remote biometric  identification systems, which have not been included in the list of prohibitions in Article 5, 
 
have been added to Annex III (List of high-risk AI systems), with some further limitations  of their scope. 
4. Post-remote biometric identification 
In addition to being included in the list of high-risk AI systems in Annex III, the use of  post-remote biometric identification by law enforcement authorities has been made subject  to some additional safeguards in Article 29. These safeguards include some limited  transparency measures and the requirement for law enforcement authorities to request an  authorisation for the use of post-remote biometric identification, which could be given by a  judicial authority or an administrative authority whose decision is binding and subject to a  judicial review. This can be done prior or ex post, within 48 hours. Furthermore, the  compromise text clarifies that post-remote biometric identification should not be used for  law enforcement purposes in an untargeted way without any link to a criminal offence, a  criminal proceeding, a genuine and present or genuine and foreseeable threat of a criminal  offence or the search for a specific missing person. 
5. Exceptions for law enforcement authorities 
The compromise agreement preserves all the exceptions for law enforcement authorities that  were added in the Council’s revised mandate, including derogation from conformity  assessment and the possibility to start real world testing of high-risk AI systems without  prior authorisation. Nevertheless, in order to secure these concessions, the Council had to  agree that market surveillance authorities for high-risk AI systems in the field of law  enforcement would be either the data protection authorities or any other authorities  designated pursuant to conditions laid down in Articles 1 to 44 of the Law Enforcement  Directive. 
6. Fundamental rights impact assessment 
In line with the Council’s revised mandate, in Article 29a the compromise text includes a  light version of the obligation for some deployers to conduct a fundamental rights impact  assessment. The obligation is relatively easy to comply with, and it concerns only deployers  that are bodies governed by public law, private actors providing public services, and 
 
deployers that are banking and insurance service providers using AI systems listed as high risk in Annex III, point 5, (b) and (ca). The fundamental rights impact assessment will  need to be carried out only for aspects not covered by other legal obligations, such as Data  Protection Impact Assessment under the GDPR and will be procedurally aligned with  existing processes in order to eliminate any overlap and additional burden. Moreover,  compliance with this obligation will be facilitated by the AI Office, which has been tasked  with the development of a template for a questionnaire which deployers will be able to use  to meet the relevant requirements.  
7. Testing high-risk AI systems in real world conditions outside AI regulatory sandboxes 
As part of the compromise, the text of the AI Act includes provisions on testing high-risk AI  systems in real world conditions outside AI regulatory sandboxes, in Articles 54a and 54b.  The Council’s revised mandate has been preserved in this respect, which means that testing  high-risk AI systems in real world conditions outside AI regulatory sandboxes will be  possible, but it will be subject to a range of safeguards, which include, inter alia, the  requirement for approval from the market surveillance authority, the right for affected  persons to request to delete their data after testing in real world conditions, the right for  market surveillance authorities to request information related to testing in real world  conditions from providers, including the power to conduct inspections, limited duration of  such testing, as well as some additional safeguards designed specifically for testing in real  world conditions in the areas of law enforcement, migration, asylum and border control  management. 
8. General purpose AI models 
The compromise agreement includes new provisions concerning general purpose AI models  (GPAI models) in Articles 52a – 52e. These new rules introduce horizontal obligations for  all GPAI models, which will include keeping up-to-date and making available, upon request,  technical documentation to the AI Office and national competent authorities. They also  include providing certain information and documentation to downstream providers for the  purpose of compliance with the AI Act. There are some additional requirements for models  with systemic risks, which would include performing model evaluation, making risk  assessments and taking risk mitigation measures, ensuring an adequate level of 
 
cybersecurity protection, and reporting serious incidents to the AI Office and national  competent authorities. Compliance with these requirements can be achieved through codes  of practice, which will be developed by the industry, with the participation of the Member  States (through the AI Board), and facilitated by the AI Office. The drawing up of the codes  of practice should be an open process to which all interested stakeholders will be invited,  both companies as well as civil society and academia. The AI Office will also evaluate these  codes of practice and can formally approve them, or, in case they are inadequate to cover the  obligations, provide common rules for the implementation of the obligations through an  implementing act. The compromise agreement also provides that standards will be a  possibility for compliance in the future. 
Classification of GPAI models as presenting systemic risks will initially depend on the  capability, either based on a quantitative threshold of the cumulative amount of compute  used for its training measured in floating point operations (FLOPs), or on an individual  designation decision of the Commission that takes into account criteria listed in Annex IXc (e.g. the number of parameters, quality and size of the dataset, input and output modalities  or the reach measures in business users).The initial FLOPs threshold for this has been set as  10^25, so higher than what the EP initially wanted but lower than in the Council’s mandate,  which was 10^26. However, in this context it is important to note that the Commission will  be obliged to adapt the threshold in the light of evolving technological developments, such  as algorithmic improvements or increased hardware efficiency, in order to keep up with the  state of the art, reflecting the capabilities recorded in the most advanced general purpose AI  models. 
Regarding copyright, the compromise agreement states that providers of GPAI models will  need to put in place a policy to respect Union copyright law, as well as make publicly  available a sufficiently detailed summary about the content used for training of the general purpose AI model, based on a template provided by the AI Office. Recital 60k explains that  this summary should not be technically detailed but comprehensive at a general level, while  taking into due account the need to protect trade secrets and confidential business  information.  
Finally, with regard to GPAI systems, the compromise agreement defines them as AI  systems based on GPAI models that can serve a variety of purposes. Article 28 includes 
 
rules to align them to the risk-based approach of the AI Act. Providers of such systems will  be obliged to provide all the information and elements to downstream providers of high-risk  AI systems so that they can comply with the respective requirements, including for the  purpose of conformity assessment. Article 63a further clarifies the conditions in which  national market surveillance authorities should collaborate with the AI Office to ensure the  compliance of GPAI systems that can be used directly by deployers for at least one purpose  classified as high-risk, when they consider the GPAI systems are not compliant.  
9. Governance and enforcement 
Linked to the agreements on GPAI models, as summarized in point 8 above, the  compromise text also includes the new rules on governance in Title VI. While for AI  systems the market surveillance system on the national level will apply, these new rules for  GPAI models provide for a new more centralised system of oversight and enforcement. For  this purpose, the AI Office will be established as a new governance structure with a number  of specific tasks in respect of GPAI models, and with a strong link with the scientific  community to support its work.  
The newly proposed governance structure also includes an enhanced role for the AI Board.  Its list of tasks has been extended to give the Member States a stronger coordination role,  including with regard to AI regulatory sandboxes, consultations with stakeholders and  awareness raising activities. The AI Board will also provide opinions to the Commission on  the qualified alerts concerning general purpose AI models. The provisions on the  composition and functioning of the AI Board have been maintained as in the Council’s  General Approach.  
The final text also introduces two new advisory bodies. A scientific panel of independent  experts will provide technical advice and input to the AI Office and market surveillance  authorities. The scientific panel also has a key role in the enforcement of rules for GPAI  models, as it will be able to launch qualified alerts of possible risks to the AI Office.  Building on the General Approach, Member States will have the possibility to call upon  experts of the scientific panels to support their market surveillance activities. An advisory  forum will provide stakeholder input to the Commission (including the AI Office) and to the 
 
AI Board. It will comprise a balanced selection of stakeholders, including industry, start ups, SMEs, civil society and academia. 
Finally, on the issue of appointing more than one competent authority covered by Article  59(2), the compromise text maintains the Council’s position, which gives the Member States  the flexibility to appoint at least one notifying authority and at least one market surveillance  authority as national competent authorities. In addition, Member States shall designate one  market surveillance authority to act as a single point of contact.  
10. Derogation from conformity assessment 
In line with the Council’s mandate, the compromise text preserves the derogation from  conformity assessment in Article 47(1), and also the related exemption (urgency procedure)  for law enforcement authorities in Article 47(1a), without the need for judicial  authorisation. However, to secure this concession, it has been necessary to re-introduce in  the text the wording regarding the control of the authorisation process by the Commission,  as provided in points 3, 4 and 5 of Article 47, which were previously deleted in the  Council’s General Approach. 
11. AI systems already placed on the market or put into service 
Concerning AI systems already placed on the market or put into service, in the case of  public authorities which are providers or deployers of high-risk AI systems the compromise  agreement in Article 83(2) is to offer them 4 years from the entry into application to make  their systems compliant. This change does not depart significantly from the Council’s  position, as it is highly unlikely that within this 4-year period such systems would not go  through a substantial modification, which was supposed to trigger the requirement to  comply in the Council’s General Approach. Moreover, in Article 83(3) it has been agreed  that every GPAI model that is placed on the market before the entry into application of the  provisions related to GPAI models (12 months after entry into force) will have 2 years after  the date if entry into application of these provisions (3 years in total) to be brought in  compliance (irrespective of whether there is a substantial modification or not). 
12. Implementing acts and delegated acts
 
With regard to delegated and implementing acts, the compromise text preserves the choices  as in the Council’s revised mandate, with the exception of two cases of implementing acts  where it was clarified that legally only delegated acts are possible, namely to update the  thresholds of FLOPs in Article 52a and to update the methodology for the classification of  high-impact models in Article 52b. 
13. Penalties 
The amounts of penalties for the infringements of the various aspects of the AI Act have  been fine tuned in Article 71. Notably, for non-compliance with the provisions concerning  prohibited AI practices outlined in Article 5, the penalty has been set at 35 million EUR or  7% of annual turnover, slightly above the limit set in the Council’s revised mandate, which  was 35 million EUR or 6.5% of annual turnover. On the other hand, the amounts of  penalties agreed for the remaining cases are slightly lower than in the Council’s position. 
Finally, Article 72a sets out the fines for providers of general purpose AI models, in case of  infringements of the obligations or non-compliance with the enforcement measures, e.g.  requests for information. The maximum amount of fines has been aligned to those for  providers of high-risk AI systems. It is important to highlight that there will be an additional  grace period for providers of general purpose AI models, because no fines can be imposed  during the first year after entry into application of the rules. 
14. Entry into application 
On the entry of application as specified in Article 85, the compromise agreement provides  for 24 months with regard to most parts of the Regulation, with slightly shorter deadlines for  some elements, namely 6 months for prohibitions and 12 months for provisions concerning  notifying authorities and notified bodies, governance, general purpose AI models,  confidentiality and penalties, and a slightly longer deadline of 36 months for high-risk AI  systems covered by Annex II. 
III. CONCLUSION
 
1. The Presidency invites the Committee of the Permanent Representatives to:  
a. endorse the annexed compromise text as agreed with the European Parliament during  the final trilogue, and 
b. mandate the Presidency to inform the European Parliament that, should the European  Parliament adopt its position at first reading, in accordance with Article 294  paragraph 3 of the Treaty, in the form set out in the compromise package contained  in the Annex to this document (subject to revision by the lawyer linguists of both  institutions), the Council would, in accordance with Article 294, paragraph 4 of the  Treaty, approve the European Parliament’s position and the act shall be adopted in  the wording which corresponds to the European Parliament’s position. 
_______________________
 
ANNEX 
2021/0106 (COD) 
Proposal for a 
REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL 
LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE  (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION  LEGISLATIVE ACTS 
THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION, 
Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16  and 114 thereof, 
Having regard to the proposal from the European Commission, 
After transmission of the draft legislative act to the national parliaments, 
Having regard to the opinion of the European Economic and Social Committee1, Having regard to the opinion of the European Central Bank2, 
Having regard to the joint opinion of the European Data Protection Board and the European Data  Protection Supervisor, 
Having regard to the opinion of the Committee of the Regions3, 
Acting in accordance with the ordinary legislative procedure, 
Whereas: 
(1) The purpose of this Regulation is to improve the functioning of the internal market by  laying down a uniform legal framework in particular for the development, placing on the  
  
1 OJ C […], […], p. […]. 
2 Reference to ECB opinion. 
3 OJ C […], […], p. […].
 
market, putting into service and the use of artificial intelligence systems in the Union in  conformity with Union values, to promote the uptake of human centric and trustworthy  artificial intelligence while ensuring a high level of protection of health, safety,  fundamental rights enshrined in the Charter, including democracy and rule of law and  environmental protection, against harmful effects of artificial intelligence systems in the  Union and to support innovation. This regulation ensures the free movement of AI-based  goods and services cross-border, thus preventing Member States from imposing  restrictions on the development, marketing and use of Artificial Intelligence systems (AI  systems), unless explicitly authorised by this Regulation.  
(1a) This Regulation should be applied in conformity with the values of the Union enshrined in  the Charter facilitating the protection of individuals, companies, democracy and rule of law  and the environment while boosting innovation and employment and making the Union a  leader in the uptake of trustworthy AI. 
(2) AI systems can be easily deployed in multiple sectors of the economy and society,  including cross border, and circulate throughout the Union. Certain Member States have  already explored the adoption of national rules to ensure that artificial intelligence is  trustworthy and safe and is developed and used in compliance with fundamental rights  obligations. Differing national rules may lead to fragmentation of the internal market and  decrease legal certainty for operators that develop, import or use AI systems. A consistent  and high level of protection throughout the Union should therefore be ensured in order to  achieve trustworthy AI, while divergences hampering the free circulation, innovation,  deployment and uptake of AI systems and related products and services within the internal  market should be prevented, by laying down uniform obligations for operators and  guaranteeing the uniform protection of overriding reasons of public interest and of rights of  persons throughout the internal market based on Article 114 of the Treaty on the  Functioning of the European Union (TFEU). To the extent that this Regulation contains  specific rules on the protection of individuals with regard to the processing of personal data  concerning restrictions of the use of AI systems for remote biometric identification for the  purpose of law enforcement, for the use of AI systems for risk assessments of natural  persons for the purpose of law enforcement and for the use of AI systems of biometric  categorization for the purpose of law enforcement, it is appropriate to base this Regulation,  in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those 
 
specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the  European Data Protection Board. 
(3) Artificial intelligence is a fast evolving family of technologies that contributes to a wide  array of economic, environmental and societal benefits across the entire spectrum of  industries and social activities. By improving prediction, optimising operations and  resource allocation, and personalising digital solutions available for individuals and  organisations, the use of artificial intelligence can provide key competitive advantages to  companies and support socially and environmentally beneficial outcomes, for example in  healthcare, farming, food safety, education and training, media, sports, culture,  infrastructure management, energy, transport and logistics, public services, security,  justice, resource and energy efficiency, environmental monitoring, the conservation and  restoration of biodiversity and ecosystems and climate change mitigation and adaptation. 
(4) At the same time, depending on the circumstances regarding its specific application, use,  and level of technological development, artificial intelligence may generate risks and cause  harm to public interests and fundamental rights that are protected by Union law. Such harm  might be material or immaterial, including physical, psychological, societal or economic  harm. 
(4a) Given the major impact that artificial intelligence can have on society and the need to build  trust, it is vital for artificial intelligence and its regulatory framework to be developed according to Union values enshrined in Article 2 TEU, the fundamental rights and  freedoms enshrined in the Treaties, the Charter. As a pre-requisite, artificial intelligence  should be a human-centric technology. It should serve as a tool for people, with the  ultimate aim of increasing human well-being. 
(4aa) In order to ensure a consistent and high level of protection of public interests as regards  health, safety and fundamental rights, common rules for all high-risk AI systems should be  established. Those rules should be consistent with the Charter of fundamental rights of the  European Union (the Charter) and should be non-discriminatory and in line with the  Union’s international trade commitments. They should also take into account the European  Declaration on Digital Rights and Principles for the Digital Decade (2023/C 23/01) and the  Ethics Guidelines for Trustworthy Artificial Intelligence (AI) of the High-Level Expert  Group on Artificial Intelligence. 
 
(5) A Union legal framework laying down harmonised rules on artificial intelligence is  therefore needed to foster the development, use and uptake of artificial intelligence in the  internal market that at the same time meets a high level of protection of public interests,  such as health and safety and the protection of fundamental rights, including democracy,  rule of law and environmental protection as recognised and protected by Union law. To  achieve that objective, rules regulating the placing on the market, putting into service and  use of certain AI systems should be laid down, thus ensuring the smooth functioning of the  internal market and allowing those systems to benefit from the principle of free movement  of goods and services. These rules should be clear and robust in protecting fundamental  rights, supportive of new innovative solutions, enabling to a European ecosystem of public  and private actors creating AI systems in line with Union values and unlocking the  potential of the digital transformation across all regions of the Union. By laying down  those rules as well as measures in support of innovation with a particular focus on SMEs  including startups, this Regulation supports the objective of promoting the European  human-centric approach to AI and being a global leader in the development of secure,  trustworthy and ethical artificial intelligence as stated by the European Council4, and it  ensures the protection of ethical principles, as specifically requested by the the European  Parliament5. 
(5a) The harmonised rules on the placing on the market, putting into service and use of AI  systems laid down in this Regulation should apply across sectors and, in line with its New  Legislative Framework approach, should be without prejudice to existing Union law,  notably on data protection, consumer protection, fundamental rights, employment, and  protection of workers, and product safety, to which this Regulation is complementary. As a  consequence all rights and remedies provided for by such Union law to consumers, and  other persons who may be negatively impacted by AI systems, including as regards the  compensation of possible damages pursuant to Council Directive 85/374/EEC of 25 July  1985 on the approximation of the laws, regulations and administrative provisions of the  Member States concerning liability for defective products, remain unaffected and fully  applicable. Furthermore, in the context of employment and protection of workers, this  Regulation should therefore not affect Union law on social policy and national labour law,  in compliance with Union law, concerning employment and working conditions, including    
4 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO  13/20, 2020, p. 6. 
5 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework  of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL).
 
health and safety at work and the relationship between employers and workers. This  Regulation should also not affect the exercise of fundamental rights as recognised in the  Member States and at Union level, including the right or freedom to strike or to take other  action covered by the specific industrial relations systems in Member States as well as, the  right to negotiate, to conclude and enforce collective agreements or to take collective  action in accordance with national law. [This Regulation should not affect the provisions  aiming to improve working conditions in platform work set out in Directive ... [COD  2021/414/EC]] On top of that, this Regulation aims to strengthen the effectiveness of such  existing rights and remedies by establishing specific requirements and obligations,  including in respect of transparency, technical documentation and record-keeping of AI  systems. Furthermore, the obligations placed on various operators involved in the AI value  chain under this Regulation should apply without prejudice to national laws, in compliance  with Union law, having the effect of limiting the use of certain AI systems where such  laws fall outside the scope of this Regulation or pursue other legitimate public interest  objectives than those pursued by this Regulation. For example, national labour law and the  laws on the protection of minors (i.e. persons below the age of 18) taking into account the  United Nations General Comment No 25 (2021) on children’s rights, insofar as they are  not specific to AI systems and pursue other legitimate public interest objectives, should not  be affected by this Regulation. 
(5aa) The fundamental right to the protection of personal data is safeguarded in particular by  Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive 2016/680. Directive  2002/58/EC additionally protects private life and the confidentiality of communications,  including by way of providing conditions for any personal and non-personal data storing in  and access from terminal equipment. Those Union legal acts provide the basis for  sustainable and responsible data processing, including where datasets include a mix of  personal and non-personal data. This Regulation does not seek to affect the application of  existing Union law governing the processing of personal data, including the tasks and  powers of the independent supervisory authorities competent to monitor compliance with  those instruments. It also does not affect the obligations of providers and deployers of AI  systems in their role as data controllers or processors stemming from national or Union law  on the protection of personal data in so far as the design, the development or the use of AI  systems involves the processing of personal data. It is also appropriate to clarify that data  subjects continue to enjoy all the rights and guarantees awarded to them by such Union  law, including the rights related to solely automated individual decision-making, including 
 
profiling. Harmonised rules for the placing on the market, the putting into service and the  use of AI systems established under this Regulation should facilitate the effective  implementation and enable the exercise of the data subjects’ rights and other remedies  guaranteed under Union law on the protection of personal data and of other fundamental  rights. 
(5ab) This Regulation should be without prejudice to the provisions regarding the liability of  intermediary service providers set out in Directive 2000/31/EC of the European Parliament  and of the Council [as amended by the Digital Services Act]. 
(6) The notion of AI system in this Regulation should be clearly defined and closely aligned  with the work of international organisations working on artificial intelligence to ensure  legal certainty, facilitate international convergence and wide acceptance, while providing  the flexibility to accommodate the rapid technological developments in this field.  Moreover, it should be based on key characteristics of artificial intelligence systems, that  distinguish it from simpler traditional software systems or programming approaches and  should not cover systems that are based on the rules defined solely by natural persons to  automatically execute operations. A key characteristic of AI systems is their capability to  infer. This inference refers to the process of obtaining the outputs, such as predictions,  content, recommendations, or decisions, which can influence physical and virtual  environments and to a capability of AI systems to derive models and/or algorithms from  inputs/data. The techniques that enable inference while building an AI system include  machine learning approaches that learn from data how to achieve certain objectives; and  logic- and knowledge-based approaches that infer from encoded knowledge or symbolic  representation of the task to be solved. The capacity of an AI system to infer goes beyond  basic data processing, enable learning, reasoning or modelling. The term “machine-based”  refers to the fact that AI systems run on machines. The reference to explicit or implicit  objectives underscores that AI systems can operate according to explicit defined objectives  or to implicit objectives. The objectives of the AI system may be different from the  intended purpose of the AI system in a specific context. For the purposes of this  Regulation, environments should be understood as the contexts in which the AI systems  operate, whereas outputs generated by the AI system, reflect different functions performed  by AI systems and include predictions, content, recommendations or decisions. AI  systems are designed to operate with varying levels of autonomy, meaning that they have  some degree of independence of actions from human involvement and of capabilities to 
 
operate without human intervention. The adaptiveness that an AI system could exhibit after  deployment, refers to self-learning capabilities, allowing the system to change while in use.  AI systems can be used on a stand-alone basis or as a component of a product, irrespective  
of whether the system is physically integrated into the product (embedded) or serve the  functionality of the product without being integrated therein (non-embedded). 
(6a) The notion of ‘deployer’ referred to in this Regulation should be interpreted as any natural  or legal person, including a public authority, agency or other body, using an AI system  under its authority, except where the AI system is used in the course of a personal non professional activity. Depending on the type of AI system, the use of the system may affect  persons other than the deployer.  
(7) The notion of biometric data used in this Regulation should be interpreted in light of the  notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 of the  European Parliament and of the Council 6, Article 3(18) of Regulation (EU) 2018/1725 of  the European Parliament and of the Council 7 and Article 3(13) of Directive (EU)  2016/680 of the European Parliament and of the Council 8.Biometric data can allow for the  authentication, identification or categorisation of natural persons and for the recognition of  emotions of natural persons. 
(7a) The notion of biometric identification as used in this Regulation should be defined as the  automated recognition of physical, physiological and behavioural human features such as  the face, eye movement, body shape, voice, prosody, gait, posture, heart rate, blood  pressure, odour, keystrokes characteristics, for the purpose of establishing an individual’s  identity by comparing biometric data of that individual to stored biometric data of  individuals in a reference database, irrespective of whether the individual has given its  consent or not. This excludes AI systems intended to be used for biometric verification,  which includes authentication, whose sole purpose is to confirm that a specific natural  
  
6 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection  of natural persons with regard to the processing of personal data and on the free movement of such data, and  repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). 
7 Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the  protection of natural persons with regard to the processing of personal data by the Union institutions, bodies,  offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and  Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39) 
8 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of  natural persons with regard to the processing of personal data by competent authorities for the purposes of the  prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties,  and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (Law  
Enforcement Directive) (OJ L 119, 4.5.2016, p. 89).
 
person is the person he or she claims to be and to confirm the identity of a natural person  for the sole purpose of having access to a service, unlocking a device or having security  access to premises. 
(7b) The notion of biometric categorisation as used in this Regulation should be defined as  assigning natural persons to specific categories on the basis of their biometric data. Such  specific categories can relate to aspects such as sex, age, hair colour, eye colour, tattoos,  behavioural or personality traits, language, religion, membership of a national minority,  sexual or political orientation. This does not include biometric categorization systems that  are a purely ancillary feature intrinsically linked to another commercial service meaning  that the feature cannot, for objective technical reasons, be used without the principal  service and the integration of that feature or functionality is not a means to circumvent the  applicability of the rules of this Regulation. For example, filters categorizing facial or body  features used on online marketplaces could constitute such an ancillary feature as they can  only be used in relation to the principal service which consists in selling a product by  allowing the consumer to preview the display of the product on him or herself and help the  consumer to make a purchase decision. Filters used on online social network services  which categorise facial or body features to allow users to add or modify pictures or videos  could also be considered as ancillary feature as such filter cannot be used without the  principal service of the social network services consisting in the sharing of content online. 
(8) The notion of remote biometric identification system as used in this Regulation should be  defined functionally, as an AI system intended for the identification of natural persons  without their active involvement, typically at a distance, through the comparison of a  person’s biometric data with the biometric data contained in a reference database,  irrespectively of the particular technology, processes or types of biometric data used. Such  remote biometric identification systems are typically used to perceive multiple persons or  their behaviour simultaneously in order to facilitate significantly the identification of  natural persons without their active involvement. This excludes AI systems intended to be  used for biometric verification, which includes authentication, whose sole purpose is to  confirm that a specific natural person is the person he or she claims to be and to confirm  the identity of a natural person for the sole purpose of having access to a service, unlocking  a device or having security access to premises. This exclusion is justified by the fact that  such systems are likely to have a minor impact on fundamental rights of natural persons  compared to the remote biometric identification systems which may be used for the 
 
processing of the biometric data of a large number of persons without their active  involvement. In the case of ‘real-time’ systems, the capturing of the biometric data, the  comparison and the identification occur all instantaneously, near-instantaneously or in any  event without a significant delay. In this regard, there should be no scope for  circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems in  question by providing for minor delays. ‘Real-time’ systems involve the use of ‘live’ or  ‘near-‘live’ material, such as video footage, generated by a camera or other device with  similar functionality. In the case of ‘post’ systems, in contrast, the biometric data have  already been captured and the comparison and identification occur only after a significant  delay. This involves material, such as pictures or video footage generated by closed circuit  television cameras or private devices, which has been generated before the use of the  system in respect of the natural persons concerned. 
(8a) The notion of emotion recognition system for the purpose of this regulation should be  defined as an AI system for the purpose of identifying or inferring emotions or intentions  of natural persons on the basis of their biometric data. This refers to emotions or intentions  such as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame,  contempt, satisfaction and amusement. It does not include physical states, such as pain or  fatigue. It does not refer for example to systems used in detecting the state of fatigue of  professional pilots or drivers for the purpose of preventing accidents. It does also not  include the mere detection of readily apparent expressions, gestures or movements, unless  they are used for identifying or inferring emotions. These expressions can be basic facial  expressions such as a frown or a smile, or gestures such as the movement of hands, arms or  head, or characteristics of a person’s voice, for example a raised voice or whispering.  
(9) For the purposes of this Regulation the notion of publicly accessible space should be  understood as referring to any physical place that is accessible to an undetermined number  of natural persons, and irrespective of whether the place in question is privately or publicly  owned and irrespective of the activity for which the place may be used, such as commerce  (for instance, shops, restaurants, cafés), services (for instance, banks, professional  activities, hospitality), sport (for instance, swimming pools, gyms, stadiums), transport (for  instance, bus, metro and railway stations, airports, means of transport ), entertainment (for  instance, cinemas, theatres, museums, concert and conference halls) leisure or otherwise  (for instance, public roads and squares, parks, forests, playgrounds). A place should be  classified as publicly accessible also if, regardless of potential capacity or security 
 
restrictions, access is subject to certain predetermined conditions, which can be fulfilled by  an undetermined number of persons, such as purchase of a ticket or title of transport, prior  registration or having a certain age. By contrast, a place should not be considered publicly  accessible if access is limited to specific and defined natural persons through either Union  
or national law directly related to public safety or security or through the clear  manifestation of will by the person having the relevant authority on the place. The factual  possibility of access alone (e.g. an unlocked door, an open gate in a fence) does not imply  that the place is publicly accessible in the presence of indications or circumstances  suggesting the contrary (e.g. signs prohibiting or restricting access). Company and factory  premises as well as offices and workplaces that are intended to be accessed only by  relevant employees and service providers are places that are not publicly accessible.  Publicly accessible spaces should not include prisons or border control. Some other areas  may be composed of both not publicly accessible and publicly accessible areas, such as the  hallway of a private residential building necessary to access a doctor's office or an airport.  Online spaces are not covered either, as they are not physical spaces. Whether a given  space is accessible to the public should however be determined on a case-by-case basis,  having regard to the specificities of the individual situation at hand. 
(9b) In order to obtain the greatest benefits from AI systems while protecting fundamental  rights, health and safety and to enable democratic control, AI literacy should equip  providers, deployers and affected persons with the necessary notions to make informed  decisions regarding AI systems. These notions may vary with regard to the relevant context  and can include understanding the correct application of technical elements during the AI  system’s development phase, the measures to be applied during its use, the suitable ways in  which to interpret the AI system’s output, and, in the case of affected persons, the  knowledge necessary to understand how decisions taken with the assistance of AI will  impact them. In the context of the application this Regulation, AI literacy should provide  all relevant actors in the AI value chain with the insights required to ensure the appropriate  compliance and its correct enforcement. Furthermore, the wide implementation of AI  literacy measures and the introduction of appropriate follow-up actions could contribute to  improving working conditions and ultimately sustain the consolidation, and innovation  path of trustworthy AI in the Union. The European Artificial Intelligence Board should  support the Commission, to promote AI literacy tools, public awareness and understanding  of the benefits, risks, safeguards, rights and obligations in relation to the use of AI systems.  In cooperation with the relevant stakeholders, the Commission and the Member States 
 
should facilitate the drawing up of voluntary codes of conduct to advance AI literacy  among persons dealing with the development, operation and use of AI. 
(10) In order to ensure a level playing field and an effective protection of rights and freedoms of  individuals across the Union, the rules established by this Regulation should apply to  providers of AI systems in a non-discriminatory manner, irrespective of whether they are  established within the Union or in a third country, and to deployers of AI systems  established within the Union.  
(11) In light of their digital nature, certain AI systems should fall within the scope of this  Regulation even when they are neither placed on the market, nor put into service, nor used  in the Union. This is the case for example of an operator established in the Union that  contracts certain services to an operator established outside the Union in relation to an  activity to be performed by an AI system that would qualify as high-risk. In those  circumstances, the AI system used by the operator outside the Union could process data  lawfully collected in and transferred from the Union, and provide to the contracting  operator in the Union the output of that AI system resulting from that processing, without  that AI system being placed on the market, put into service or used in the Union. To  prevent the circumvention of this Regulation and to ensure an effective protection of  natural persons located in the Union, this Regulation should also apply to providers and  deployers of AI systems that are established in a third country, to the extent the output  produced by those systems is intended to be used in the Union. Nonetheless, to take into  account existing arrangements and special needs for future cooperation with foreign  partners with whom information and evidence is exchanged, this Regulation should not  apply to public authorities of a third country and international organisations when acting in  the framework of cooperation or international agreements concluded at national or  European level for law enforcement and judicial cooperation with the Union or with its  Member States, under the condition that this third country or international organisations  provide adequate safeguards with respect to the protection of fundamental rights and  freedoms of individuals. Where relevant, this may also cover activities of entities entrusted  by the third countries to carry out specific tasks in support of such law enforcement and  judicial cooperation. Such framework for cooperation or agreements have been established  bilaterally between Member States and third countries or between the European Union,  Europol and other EU agencies and third countries and international organisations. The  authorities competent for supervision of the law enforcement and judicial authorities under 
 
the AI Act should assess whether these frameworks for cooperation or international  agreements include adequate safeguards with respect to the protection of fundamental  rights and freedoms of individuals. Recipient Member States authorities and Union  institutions, offices and bodies making use of such outputs in the Union remain  accountable to ensure their use complies with Union law. When those international  agreements are revised or new ones are concluded in the future, the contracting parties  should undertake the utmost effort to align those agreements with the requirements of this  Regulation. 
(12) This Regulation should also apply to Union institutions, offices, bodies and agencies when  acting as a provider or deployer of an AI system. 
(12a) If and insofar AI systems are placed on the market, put into service, or used with or  without modification of such systems for military, defence or national security purposes,  those should be excluded from the scope of this Regulation regardless of which type of  entity is carrying out those activities, such as whether it is a public or private entity. As  regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU  and by the specificities of the Member States’ and the common Union defence policy  covered by Chapter 2 of Title V of the Treaty on European Union (TEU) that are subject to  public international law, which is therefore the more appropriate legal framework for the  regulation of AI systems in the context of the use of lethal force and other AI systems in  the context of military and defence activities. As regards national security purposes, the  exclusion is justified both by the fact that national security remains the sole responsibility  of Member States in accordance with Article 4(2) TEU and by the specific nature and  operational needs of national security activities and specific national rules applicable to  those activities. Nonetheless, if an AI system developed, placed on the market, put into  service or used for military, defence or national security purposes is used outside those  temporarily or permanently for other purposes (for example, civilian or humanitarian  purposes, law enforcement or public security purposes), such a system would fall within  the scope of this Regulation. In that case, the entity using the system for other than  military, defence or national security purposes should ensure compliance of the system  with this Regulation, unless the system is already compliant with this Regulation. AI  systems placed on the market or put into service for an excluded (i.e. military, defence or  national security) and one or more non excluded purposes (e.g. civilian purposes, law  enforcement, etc.), fall within the scope of this Regulation and providers of those systems 
 
should ensure compliance with this Regulation. In those cases, the fact that an AI system  may fall within the scope of this Regulation should not affect the possibility of entities  carrying out national security, defence and military activities, regardless of the type of  entity carrying out those activities, to use AI systems for national security, military and  defence purposes, the use of which is excluded from the scope of this Regulation. An AI  system placed on the market for civilian or law enforcement purposes which is used with  or without modification for military, defence or national security purposes should not fall  within the scope of this Regulation, regardless of the type of entity carrying out those  activities. 
(12c) This Regulation should support innovation, respect freedom of science, and should not  undermine research and development activity. It is therefore necessary to exclude from its  scope AI systems and models specifically developed and put into service for the sole  purpose of scientific research and development. Moreover, it is necessary to ensure that the  Regulation does not otherwise affect scientific research and development activity on AI  systems or models prior to being placed on the market or put into service. As regards  product oriented research, testing and development activity regarding AI systems or  models, the provisions of this Regulation should also not apply prior to these systems and  models being put into service or placed on the market. This is without prejudice to the  obligation to comply with this Regulation when an AI system falling into the scope of this  Regulation is placed on the market or put into service as a result of such research and  development activity and to the application of provisions on regulatory sandboxes and  testing in real world conditions. Furthermore, without prejudice to the foregoing regarding  AI systems specifically developed and put into service for the sole purpose of scientific  research and development, any other AI system that may be used for the conduct of any  research and development activity should remain subject to the provisions of this  Regulation. Under all circumstances, any research and development activity should be  carried out in accordance with recognised ethical and professional standards for scientific  research and should be conducted according to applicable Union law. 
(14) In order to introduce a proportionate and effective set of binding rules for AI systems, a  clearly defined risk-based approach should be followed. That approach should tailor the  type and content of such rules to the intensity and scope of the risks that AI systems can  generate. It is therefore necessary to prohibit certain unacceptable artificial intelligence 
 
practices, to lay down requirements for high-risk AI systems and obligations for the  relevant operators, and to lay down transparency obligations for certain AI systems. 
(14a) While the risk-based approach is the basis for a proportionate and effective set of binding  rules, it is important to recall the 2019 Ethics Guidelines for Trustworthy AI developed by  the independent High-Level Expert Group on AI (HLEG) appointed by the Commission.  In those Guidelines the HLEG developed seven non-binding ethical principles for AI  which should help ensure that AI is trustworthy and ethically sound. The seven principles  include: human agency and oversight; technical robustness and safety; privacy and data  governance; transparency; diversity, non-discrimination and fairness; societal and  environmental well-being and accountability. Without prejudice to the legally binding  requirements of this Regulation and any other applicable Union law, these Guidelines  contribute to the design of a coherent, trustworthy and human-centric Artificial  Intelligence, in line with the Charter and with the values on which the Union is founded.  According to the Guidelines of HLEG, human agency and oversight means that AI systems  are developed and used as a tool that serves people, respects human dignity and personal  autonomy, and that is functioning in a way that can be appropriately controlled and  overseen by humans. Technical robustness and safety means that AI systems are developed  and used in a way that allows robustness in case of problems and resilience against  attempts to alter the use or performance of the AI system so as to allow unlawful use by  third parties, and minimise unintended harm. Privacy and data governance means that AI  systems are developed and used in compliance with existing privacy and data protection  rules, while processing data that meets high standards in terms of quality and integrity.  Transparency means that AI systems are developed and used in a way that allows  appropriate traceability and explainability, while making humans aware that they  communicate or interact with an AI system, as well as duly informing deployers of the  capabilities and limitations of that AI system and affected persons about their rights.  Diversity, non-discrimination and fairness means that AI systems are developed and used  in a way that includes diverse actors and promotes equal access, gender equality and  cultural diversity, while avoiding discriminatory impacts and unfair biases that are  prohibited by Union or national law. Social and environmental well-being means that AI  systems are developed and used in a sustainable and environmentally friendly manner as  well as in a way to benefit all human beings, while monitoring and assessing the long-term  impacts on the individual, society and democracy. The application of these principles  should be translated, when possible, in the design and use of AI models. They should in 
 
any case serve as a basis for the drafting of codes of conduct under this Regulation. All  stakeholders, including industry, academia, civil society and standardisation organisations,  are encouraged to take into account as appropriate the ethical principles for the  development of voluntary best practices and standards.  
(15) Aside from the many beneficial uses of artificial intelligence, that technology can also be  misused and provide novel and powerful tools for manipulative, exploitative and social  control practices. Such practices are particularly harmful and abusive and should be  prohibited because they contradict Union values of respect for human dignity, freedom,  equality, democracy and the rule of law and Union fundamental rights, including the right  to non-discrimination, data protection and privacy and the rights of the child. 
(16) AI-enabled manipulative techniques can be used to persuade persons to engage in  unwanted behaviours, or to deceive them by nudging them into decisions in a way that  subverts and impairs their autonomy, decision-making and free choices. The placing on the  market, putting into service or use of certain AI systems with the objective to or the effect  of materially distorting human behaviour, whereby significant harms, in particular having  sufficiently important adverse impacts on physical, psychological health or financial  interests are likely to occur, are particularly dangerous and should therefore be forbidden.  Such AI systems deploy subliminal components such as audio, image, video stimuli that  persons cannot perceive as those stimuli are beyond human perception or other  manipulative or deceptive techniques that subvert or impair person’s autonomy, decision making or free choices in ways that people are not consciously aware of, or even if aware  they are still deceived or not able to control or resist. This could be for example, facilitated  by machine-brain interfaces or virtual reality as they allow for a higher degree of control of  what stimuli are presented to persons, insofar as they may be materially distorting their  behaviour in a significantly harmful manner. In addition, AI systems may also otherwise  exploit vulnerabilities of a person or a specific group of persons due to their age, disability  within the meaning of Directive (EU) 2019/882, or a specific social or economic situation  that is likely to make those persons more vulnerable to exploitation such as persons living  in extreme poverty, ethnic or religious minorities. Such AI systems can be placed on the  market, put into service or used with the objective to or the effect of materially distorting  the behaviour of a person and in a manner that causes or is reasonably likely to cause  significant harm to that or another person or groups of persons, including harms that may  be accumulated over time and should therefore be prohibited. The intention to distort the 
 
behaviour may not be presumed if the distortion results from factors external to the AI  system which are outside of the control of the provider or the deployer, meaning factors  that may not be reasonably foreseen and mitigated by the provider or the deployer of the  AI system. In any case, it is not necessary for the provider or the deployer to have the  intention to cause significant harm, as long as such harm results from the manipulative or  exploitative AI-enabled practices. The prohibitions for such AI practices are  complementary to the provisions contained in Directive 2005/29/EC, notably unfair  commercial practices leading to economic or financial harms to consumers are prohibited  under all circumstances, irrespective of whether they are put in place through AI systems  or otherwise. The prohibitions of manipulative and exploitative practices in this  Regulation should not affect lawful practices in the context of medical treatment such as  psychological treatment of a mental disease or physical rehabilitation, when those practices  are carried out in accordance with the applicable legislation and medical standards, for  example explicit consent of the individuals or their legal representatives. In addition,  common and legitimate commercial practices, for example in the field of advertising, that  are in compliance with the applicable law should not in themselves be regarded as  constituting harmful manipulative AI practices. 
(16a) Biometric categorisation systems that are based on individuals’ biometric data, such as an  individual person’s face or fingerprint, to deduce or infer an individuals’ political opinions,  trade union membership, religious or philosophical beliefs, race, sex life or sexual  orientation should be prohibited. This prohibition does not cover the lawful labelling,  filtering or categorisation of biometric datasets acquired in line with Union or national law  according to biometric data, such as the sorting of images according to hair colour or eye  colour, which can for example be used in the area of law enforcement. 
(17) AI systems providing social scoring of natural persons by public or private actors may lead  to discriminatory outcomes and the exclusion of certain groups. They may violate the right  to dignity and non-discrimination and the values of equality and justice. Such AI systems  evaluate or classify natural persons or groups thereof based on multiple data points related  to their social behaviour in multiple contexts or known, inferred or predicted personal or  personality characteristics over certain periods of time. The social score obtained from  such AI systems may lead to the detrimental or unfavourable treatment of natural persons  or whole groups thereof in social contexts, which are unrelated to the context in which the  data was originally generated or collected or to a detrimental treatment that is 
 
disproportionate or unjustified to the gravity of their social behaviour. AI systems entailing  such unacceptable scoring practices leading to such detrimental or unfavourable outcomes  should be therefore prohibited. This prohibition should not affect lawful evaluation  practices of natural persons done for a specific purpose in compliance with national and  Union law. 
(18) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in  publicly accessible spaces for the purpose of law enforcement is particularly intrusive to  the rights and freedoms of the concerned persons, to the extent that it may affect the  private life of a large part of the population, evoke a feeling of constant surveillance and  indirectly dissuade the exercise of the freedom of assembly and other fundamental rights.  Technical inaccuracies of AI systems intended for the remote biometric identification of  natural persons can lead to biased results and entail discriminatory effects. This is  particularly relevant when it comes to age, ethnicity, race, sex or disabilities. In addition,  the immediacy of the impact and the limited opportunities for further checks or corrections  in relation to the use of such systems operating in ‘real-time’ carry heightened risks for the  rights and freedoms of the persons that are concerned by law enforcement activities.  
(19) The use of those systems for the purpose of law enforcement should therefore be  prohibited, except in exhaustively listed and narrowly defined situations, where the use is  strictly necessary to achieve a substantial public interest, the importance of which  outweighs the risks. Those situations involve the search for certain victims of crime  including missing people; certain threats to the life or physical safety of natural persons or  of a terrorist attack; and the localisation or identification of perpetrators or suspects of the  criminal offences referred to in Annex IIa if those criminal offences are punishable in the  Member State concerned by a custodial sentence or a detention order for a maximum  period of at least four years and as they are defined in the law of that Member State. Such  threshold for the custodial sentence or detention order in accordance with national law  contributes to ensure that the offence should be serious enough to potentially justify the use  of ‘real-time’ remote biometric identification systems. Moreover, the list of criminal  offences as referred in Annex IIa is based on the 32 criminal offences listed in the Council  Framework Decision 2002/584/JHA9, taking into account that some are in practice likely  to be more relevant than others, in that the recourse to ‘real-time’ remote biometric  
  
9 Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender  procedures between Member States (OJ L 190, 18.7.2002, p. 1).
 
identification will foreseeably be necessary and proportionate to highly varying degrees for  the practical pursuit of the localisation or identification of a perpetrator or suspect of the  different criminal offences listed and having regard to the likely differences in the  seriousness, probability and scale of the harm or possible negative consequences.  An imminent threat to life or physical safety of natural persons could also result from a  serious disruption of critical infrastructure, as defined in Article 2, point (a) of Directive  2008/114/EC, where the disruption or destruction of such critical infrastructure would  result in an imminent threat to life or physical safety of a person, including through serious  harm to the provision of basic supplies to the population or to the exercise of the core  function of the State.  
In addition, this Regulation should preserve the ability for law enforcement, border control,  immigration or asylum authorities to carry out identity checks in the presence of the person  that is concerned in accordance with the conditions set out in Union and national law for  such checks. In particular, law enforcement, border control, immigration or asylum  authorities should be able to use information systems, in accordance with Union or national  law, to identify a person who, during an identity check, either refuses to be identified or is  unable to state or prove his or her identity, without being required by this Regulation to  obtain prior authorisation. This could be, for example, a person involved in a crime, being  unwilling, or unable due to an accident or a medical condition, to disclose their identity to  law enforcement authorities.  
(20) In order to ensure that those systems are used in a responsible and proportionate manner, it  is also important to establish that, in each of those exhaustively listed and narrowly  defined situations, certain elements should be taken into account, in particular as regards  the nature of the situation giving rise to the request and the consequences of the use for the  rights and freedoms of all persons concerned and the safeguards and conditions provided  for with the use. In addition, the use of ‘real-time’ remote biometric identification systems  in publicly accessible spaces for the purpose of law enforcement should only be deployed  to confirm the specifically target individual’s identity and should be limited to what is  strictly necessary concerning the period of time as well as geographic and personal scope,  having regard in particular to the evidence or indications regarding the threats, the victims  or perpetrator. The use of the ‘real-time’ remote biometric identification system in publicly  accessible spaces should only be authorised if the law enforcement authority has completed  a fundamental rights impact assessment and, unless provided otherwise in this Regulation,  has registered the system in the database as set out in this Regulation. The reference 
 
database of persons should be appropriate for each use case in each of the situations  mentioned above.  
(21) Each use of a ‘real-time’ remote biometric identification system in publicly accessible  spaces for the purpose of law enforcement should be subject to an express and specific  authorisation by a judicial authority or by an independent administrative authority whose  decision is binding of a Member State. Such authorisation should in principle be obtained  prior to the use of the system with a view to identify a person or persons. Exceptions to this  rule should be allowed in duly justified situations of urgency, that is, situations where the  need to use the systems in question is such as to make it effectively and objectively  impossible to obtain an authorisation before commencing the use. In such situations of  urgency, the use should be restricted to the absolute minimum necessary and be subject to  appropriate safeguards and conditions, as determined in national law and specified in the  context of each individual urgent use case by the law enforcement authority itself. In  addition, the law enforcement authority should in such situations request such  authorisation whilst providing the reasons for not having been able to request it earlier,  without undue delay and, at the latest within 24 hours. If such authorisation is rejected, the  use of real-time biometric identification systems linked to that authorisation should be  stopped with immediate effect and all the data related to such use should be discarded and  deleted. Such data includes input data directly acquired by an AI system in the course of  the use of such system as well as the results and outputs of the use linked to that  authorisation. It should not include input legally acquired in accordance with another  national or Union law. In any case, no decision producing an adverse legal effect on a  person may be taken solely based on the output of the remote biometric identification  system. 
(21a) In order to carry out their tasks in accordance with the requirements set out in this  Regulation as well as in national rules, the relevant market surveillance authority and the  national data protection authority should be notified of each use of the ‘real-time biometric  identification system’. National market surveillance authorities and the national data  protection authorities that have been notified should submit to the Commission an annual  report on the use of ‘real-time biometric identification systems’.  
(22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this  Regulation that such use in the territory of a Member State in accordance with this 
 
Regulation should only be possible where and in as far as the Member State in question has  decided to expressly provide for the possibility to authorise such use in its detailed rules of  national law. Consequently, Member States remain free under this Regulation not to  provide for such a possibility at all or to only provide for such a possibility in respect of  some of the objectives capable of justifying authorised use identified in this Regulation.  These national rules should be notified to the Commission at the latest 30 days following  their adoption.  
(23) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in  publicly accessible spaces for the purpose of law enforcement necessarily involves the  processing of biometric data. The rules of this Regulation that prohibit, subject to certain  exceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in  respect of the rules on the processing of biometric data contained in Article 10 of Directive  (EU) 2016/680, thus regulating such use and the processing of biometric data involved in  an exhaustive manner. Therefore, such use and processing should only be possible in as far  as it is compatible with the framework set by this Regulation, without there being scope,  outside that framework, for the competent authorities, where they act for purpose of law  enforcement, to use such systems and process such data in connection thereto on the  grounds listed in Article 10 of Directive (EU) 2016/680. In this context, this Regulation is  not intended to provide the legal basis for the processing of personal data under Article 8  of Directive 2016/680. However, the use of ‘real-time’ remote biometric identification  systems in publicly accessible spaces for purposes other than law enforcement, including  by competent authorities, should not be covered by the specific framework regarding such  use for the purpose of law enforcement set by this Regulation. Such use for purposes other  than law enforcement should therefore not be subject to the requirement of an authorisation  under this Regulation and the applicable detailed rules of national law that may give effect  to it. 
(24) Any processing of biometric data and other personal data involved in the use of AI systems  for biometric identification, other than in connection to the use of ‘real-time’ remote  biometric identification systems in publicly accessible spaces for the purpose of law  enforcement as regulated by this Regulation, should continue to comply with all  requirements resulting from Article 10 of Directive (EU) 2016/680. For purposes other  than law enforcement, Article 9(1) of Regulation (EU) 2016/679 and Article 10(1) of  Regulation (EU) 2018/1725 prohibit the processing of biometric data subject to limited 
 
exceptions as provided in those articles. In application of Article 9(1) of Regulation (EU)  2016/679, the use of remote biometric identification for purposes other than law  enforcement has already been subject to prohibition decisions by national data protection  authorities.  
(25) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom  and Ireland in respect of the area of freedom, security and justice, as annexed to the TEU  and to the TFEU, Ireland is not bound by the rules laid down in Article 5(1), point (d), (2),  (3), (3a), (4) and (5), Article 5(1)(ba) to the extent it applies to the use of biometric  categorisation systems for activities in the field of police cooperation and judicial  cooperation in criminal matters, Article 5(1)(da) to the extent it applies to the use of AI  systems covered by that provision and Article 29(6a) of this Regulation adopted on the  basis of Article 16 of the TFEU which relate to the processing of personal data by the  Member States when carrying out activities falling within the scope of Chapter 4 or  Chapter 5 of Title V of Part Three of the TFEU, where Ireland is not bound by the rules  governing the forms of judicial cooperation in criminal matters or police cooperation  which require compliance with the provisions laid down on the basis of Article 16 of the  TFEU. 
(26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark,  annexed to the TEU and TFEU, Denmark is not bound by rules laid down in Article 5(1),  point (d), (2), (3), (3a), (4) and (5), Article 5(1)(ba) to the extent it applies to the use of  biometric categorisation systems for activities in the field of police cooperation and  judicial cooperation in criminal matters, Article 5(1)(da) to the extent it applies to the use  of AI systems covered by that provision and Article 29(6a) of this Regulation adopted on  the basis of Article 16 of the TFEU, or subject to their application, which relate to the  processing of personal data by the Member States when carrying out activities falling  within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU. 
(26a) In line with the presumption of innocence, natural persons in the EU should always be  judged on their actual behaviour. Natural persons should never be judged on AI-predicted  behaviour based solely on their profiling, personality traits or characteristics, such as  nationality, place of birth, place of residence, number of children, debt, their type of car,  without a reasonable suspicion of that person being involved in a criminal activity based on  objective verifiable facts and without human assessment thereof. Therefore, risk 
 
assessments of natural persons in order to assess the risk of them offending or for  predicting the occurrence of an actual or potential criminal offence solely based on the  profiling of a natural person or on assessing their personality traits and characteristics  should be prohibited. In any case, this prohibition does not refer to nor touch upon risk  analytics that are not based on the profiling of individuals or on the personality traits and  characteristics of individuals, such as AI systems using risk analytics to assess the risk of  financial fraud by undertakings based on suspicious transactions or risk analytic tools to  predict the likelihood of localisation of narcotics or illicit goods by customs authorities, for  example based on known trafficking routes. 
(26b) The placing on the market, putting into service for this specific purpose, or use of AI  systems that create or expand facial recognition databases through the untargeted scraping  of facial images from the internet or CCTV footage should be prohibited, as this practice  adds to the feeling of mass surveillance and can lead to gross violations of fundamental  rights, including the right to privacy. 
(26c) There are serious concerns about the scientific basis of AI systems aiming to identify or  infer emotions, particularly as expression of emotions vary considerably across cultures  and situations, and even within a single individual. Among the key shortcomings of such  systems are the limited reliability, the lack of specificity and the limited generalizability.  Therefore, AI systems identifying or inferring emotions or intentions of natural persons on  the basis of their biometric data may lead to discriminatory outcomes and can be intrusive  to the rights and freedoms of the concerned persons. Considering the imbalance of power  in the context of work or education, combined with the intrusive nature of these systems,  such systems could lead to detrimental or unfavourable treatment of certain natural persons  or whole groups thereof. Therefore, the placing on the market, putting into service, or use  of AI systems intended to be used to detect the emotional state of individuals in situations  related to the workplace and education should be prohibited. This prohibition should not  cover AI systems placed on the market strictly for medical or safety reasons, such as  systems intended for therapeutical use.  
(26d) Practices that are prohibited by Union legislation, including data protection law, non discrimination law, consumer protection law, and competition law, should not be affected  by this Regulation.
 
(27) High-risk AI systems should only be placed on the Union market, put into service or used  if they comply with certain mandatory requirements. Those requirements should ensure  that high-risk AI systems available in the Union or whose output is otherwise used in the  Union do not pose unacceptable risks to important Union public interests as recognised and  protected by Union law. Following the New Legislative Framework approach, as clarified  in Commission notice the ‘Blue Guide’ on the implementation of EU product rules 2022  (C/2022/3637) the general rule is that several pieces of the EU legislation, such as  Regulation (EU) 2017/745 on Medical Devices and Regulation (EU) 2017/746 on In Vitro  Diagnostic Devices or Directive 2006/42/EC on Machinery, may have to be taken into  consideration for one product, since the making available or putting into service can only  take place when the product complies with all applicable Union harmonisation legislation.  To ensure consistency and avoid unnecessary administrative burden or costs, providers of a  product that contains one or more high-risk artificial intelligence system, to which the  requirements of this Regulation as well as requirements of the Union harmonisation  legislation listed in Annex II, Section A apply, should have a flexibility on operational  decisions on how to ensure compliance of a product that contains one or more artificial  intelligence systems with all applicable requirements of the Union harmonised legislation  in a best way. AI systems identified as high-risk should be limited to those that have a  significant harmful impact on the health, safety and fundamental rights of persons in the  Union and such limitation minimises any potential restriction to international trade, if any.  
(28) AI systems could have an adverse impact to health and safety of persons, in particular  when such systems operate as safety components of products. Consistently with the  objectives of Union harmonisation legislation to facilitate the free movement of products in  the internal market and to ensure that only safe and otherwise compliant products find their  way into the market, it is important that the safety risks that may be generated by a product  as a whole due to its digital components, including AI systems, are duly prevented and  mitigated. For instance, increasingly autonomous robots, whether in the context of  manufacturing or personal assistance and care should be able to safely operate and  performs their functions in complex environments. Similarly, in the health sector where the  stakes for life and health are particularly high, increasingly sophisticated diagnostics  systems and systems supporting human decisions should be reliable and accurate.  
(28a) The extent of the adverse impact caused by the AI system on the fundamental rights  protected by the Charter is of particular relevance when classifying an AI system as high-
 
risk. Those rights include the right to human dignity, respect for private and family life,  protection of personal data, freedom of expression and information, freedom of assembly  and of association, and non-discrimination, right to education consumer protection,  workers’ rights, rights of persons with disabilities, gender equality, intellectual property  rights, right to an effective remedy and to a fair trial, right of defence and the presumption  of innocence, right to good administration. In addition to those rights, it is important to  highlight that children have specific rights as enshrined in Article 24 of the EU Charter and  in the United Nations Convention on the Rights of the Child (further elaborated in the  UNCRC General Comment No. 25 as regards the digital environment), both of which  require consideration of the children’s vulnerabilities and provision of such protection and  care as necessary for their well-being. The fundamental right to a high level of  environmental protection enshrined in the Charter and implemented in Union policies  should also be considered when assessing the severity of the harm that an AI system can  cause, including in relation to the health and safety of persons. 
(29) As regards high-risk AI systems that are safety components of products or systems, or  which are themselves products or systems falling within the scope of Regulation (EC) No  300/2008 of the European Parliament and of the Council10, Regulation (EU) No 167/2013  of the European Parliament and of the Council11, Regulation (EU) No 168/2013 of the  European Parliament and of the Council12, Directive 2014/90/EU of the European  Parliament and of the Council13, Directive (EU) 2016/797 of the European Parliament and  of the Council14, Regulation (EU) 2018/858 of the European Parliament and of the  Council15, Regulation (EU) 2018/1139 of the European Parliament and of the Council16,  
  
10 Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common  rules in the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p.  72). 
11 Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the  approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1). 12 Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the  
approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52). 13 Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment  and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146). 
14 Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the  interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44). 
15 Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and  market surveillance of motor vehicles and their trailers, and of systems, components and separate technical  units intended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and  repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1). 
16 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in  the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending  Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives  2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) 
 
and Regulation (EU) 2019/2144 of the European Parliament and of the Council17, it is  appropriate to amend those acts to ensure that the Commission takes into account, on the  basis of the technical and regulatory specificities of each sector, and without interfering  with existing governance, conformity assessment and enforcement mechanisms and  authorities established therein, the mandatory requirements for high-risk AI systems laid  down in this Regulation when adopting any relevant future delegated or implementing acts  on the basis of those acts. 
(30) As regards AI systems that are safety components of products, or which are themselves  products, falling within the scope of certain Union harmonisation legislation listed in  Annex II, it is appropriate to classify them as high-risk under this Regulation if the product  in question undergoes the conformity assessment procedure with a third-party conformity  assessment body pursuant to that relevant Union harmonisation legislation. In particular,  such products are machinery, toys, lifts, equipment and protective systems intended for use  in potentially explosive atmospheres, radio equipment, pressure equipment, recreational  craft equipment, cableway installations, appliances burning gaseous fuels, medical devices,  and in vitro diagnostic medical devices. 
(31) The classification of an AI system as high-risk pursuant to this Regulation should not  necessarily mean that the product whose safety component is the AI system, or the AI  system itself as a product, is considered ‘high-risk’ under the criteria established in the  relevant Union harmonisation legislation that applies to the product. This is notably the  case for Regulation (EU) 2017/745 of the European Parliament and of the Council18 and  
  
No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation  (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1). 
17 Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type approval requirements for motor vehicles and their trailers, and systems, components and separate technical  units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and  vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and  repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament  and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010,  (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU)  No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No  351/2012, (EU) No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1). 
18 Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices,  amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and  repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1).
 
Regulation (EU) 2017/746 of the European Parliament and of the Council19, where a third party conformity assessment is provided for medium-risk and high-risk products. 
(32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are  safety components of products, or which are themselves products, it is appropriate to  classify them as high-risk if, in the light of their intended purpose, they pose a high risk of  harm to the health and safety or the fundamental rights of persons, taking into account both  the severity of the possible harm and its probability of occurrence and they are used in a  number of specifically pre-defined areas specified in the Regulation. The identification of  those systems is based on the same methodology and criteria envisaged also for any future  amendments of the list of high-risk AI systems that the Commission should be empowered  to adopt, via delegated acts, to take into account the rapid pace of technological  development, as well as the potential changes in the use of AI systems. 
(32a) It is also important to clarify that there may be specific cases in which AI systems referred  to pre-defined areas specified in this Regulation do not lead to a significant risk of harm to  the legal interests protected under those areas, because they do not materially influence the  decision-making or do not harm those interests substantially. For the purpose of this  
Regulation an AI system not materially influencing the outcome of decision-making  should be understood as an AI system that does not impact the substance, and thereby the  outcome, of decision-making, whether human or automated. This could be the case if one  or more of the following conditions are fulfilled. The first criterion should be that the AI  system is intended to perform a narrow procedural task, such as an AI system that  transforms unstructured data into structured data, an AI system that classifies incoming  documents into categories or an AI system that is used to detect duplicates among a large  number of applications. These tasks are of such narrow and limited nature that they pose  only limited risks which are not increased through the use in a context listed in Annex III.  The second criterion should be that the task performed by the AI system is intended to  improve the result of a previously completed human activity that may be relevant for the purpose of the use case listed in Annex III. Considering these characteristics, the AI system  only provides an additional layer to a human activity with consequently lowered risk. For  example, this criterion would apply to AI systems that are intended to improve the  language used in previously drafted documents, for instance in relation to professional    
19 Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro  diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L  117, 5.5.2017, p. 176).
 
tone, academic style of language or by aligning text to a certain brand messaging. The third  criterion should be that the AI system is intended to detect decision-making patterns or  deviations from prior decision-making patterns. The risk would be lowered because the use  of the AI system follows a previously completed human assessment which it is not meant  to replace or influence, without proper human review. Such AI systems include for  instance those that, given a certain grading pattern of a teacher, can be used to check ex  post whether the teacher may have deviated from the grading pattern so as to flag potential  inconsistencies or anomalies. The fourth criterion should be that the AI system is intended  to perform a task that is only preparatory to an assessment relevant for the purpose of the  use case listed in Annex III, thus making the possible impact of the output of the system  very low in terms of representing a risk for the assessment to follow. For example, this  criterion covers smart solutions for file handling, which include various functions from  indexing, searching, text and speech processing or linking data to other data sources, or AI  systems used for translation of initial documents. In any case, AI systems referred to in  Annex III should be considered to pose significant risks of harm to the health, safety or  fundamental rights of natural persons if the AI system implies profiling within the meaning  of Article 4(4) of Regulation (EU) 2016/679 and Article 3(4) of Directive (EU) 2016/680  and Article 3(5) of Regulation 2018/1725. To ensure traceability and transparency, a  provider who considers that an AI system referred to in Annex III is not high-risk on the  basis of the aforementioned criteria should draw up documentation of the assessment  before that system is placed on the market or put into service and should provide this  documentation to national competent authorities upon request. Such provider should be  obliged to register the system in the EU database established under this Regulation. With a  view to provide further guidance for the practical implementation of the criteria under  which AI systems referred to in Annex III are exceptionally not high-risk, the Commission  should, after consulting the AI Board, provide guidelines specifying this practical  implementation completed by a comprehensive list of practical examples of high risk and  non-high risk use cases of AI systems. 
(33a) As biometric data constitutes a special category of sensitive personal data, it is appropriate  to classify as high-risk several critical use-cases of biometric systems, insofar as their use  is permitted under relevant Union and national law. Technical inaccuracies of AI systems  intended for the remote biometric identification of natural persons can lead to biased  results and entail discriminatory effects. This is particularly relevant when it comes to age,  ethnicity, race, sex or disabilities. Therefore, remote biometric identification systems 
 
should be classified as high-risk in view of the risks that they pose. This excludes AI  systems intended to be used for biometric verification, which includes authentication,  whose sole purpose is to confirm that a specific natural person is the person he or she  
claims to be and to confirm the identity of a natural person for the sole purpose of having  access to a service, unlocking a device or having secure access to premises. 
In addition, AI systems intended to be used for biometric categorisation according to  sensitive attributes or characteristics protected under Article 9(1) of Regulation (EU)  2016/679 based on biometric data, in so far as these are not prohibited under this  Regulation, and emotion recognition systems that are not prohibited under this Regulation,  should be classified as high-risk. Biometric systems which are intended to be used solely  for the purpose of enabling cybersecurity and personal data protection measures should not  be considered as high-risk systems. 
(34) As regards the management and operation of critical infrastructure, it is appropriate to  classify as high-risk the AI systems intended to be used as safety components in the  management and operation of critical digital infrastructure as listed in Annex I point 8 of  the Directive on the resilience of critical entities, road traffic and the supply of water, gas,  heating and electricity, since their failure or malfunctioning may put at risk the life and  health of persons at large scale and lead to appreciable disruptions in the ordinary conduct  of social and economic activities. Safety components of critical infrastructure, including  critical digital infrastructure, are systems used to directly protect the physical integrity of  critical infrastructure or health and safety of persons and property but which are not  necessary in order for the system to function. Failure or malfunctioning of such  components might directly lead to risks to the physical integrity of critical infrastructure  and thus to risks to health and safety of persons and property. Components intended to be  used solely for cybersecurity purposes should not qualify as safety components. Examples  of safety components of such critical infrastructure may include systems for monitoring  water pressure or fire alarm controlling systems in cloud computing centres. 
(35) Deployment of AI systems in education is important to promote high-quality digital  education and training and to allow all learners and teachers to acquire and share the  necessary digital skills and competences, including media literacy, and critical thinking, to  take an active part in the economy, society, and in democratic processes. However, AI  systems used in education or vocational training, notably for determining access or  admission, for assigning persons to educational and vocational training institutions or 
 
programmes at all levels, for evaluating learning outcomes of persons, for assessing the  appropriate level of education for an individual and materially influencing the level of  education and training that individuals will receive or be able to access or for monitoring  and detecting prohibited behaviour of students during tests should be classified as high-risk  AI systems, since they may determine the educational and professional course of a  person’s life and therefore affect their ability to secure their livelihood. When improperly  designed and used, such systems can be particularly intrusive and may violate the right to  education and training as well as the right not to be discriminated against and perpetuate  historical patterns of discrimination, for example against women, certain age groups,  persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation.  
(36) AI systems used in employment, workers management and access to self-employment,  notably for the recruitment and selection of persons, for making decisions affecting terms  of the work related relationship promotion and termination of work-related contractual  relationships for allocating tasks based on individual behaviour, personal traits or  characteristics and for monitoring or evaluation of persons in work-related contractual  relationships, should also be classified as high-risk, since those systems may appreciably  impact future career prospects, livelihoods of these persons and workers’ rights. Relevant  work-related contractual relationships should meaningfully involve employees and persons  providing services through platforms as referred to in the Commission Work Programme  2021. Throughout the recruitment process and in the evaluation, promotion, or retention of  persons in work-related contractual relationships, such systems may perpetuate historical  patterns of discrimination, for example against women, certain age groups, persons with  disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems  used to monitor the performance and behaviour of these persons may also undermine their  fundamental rights to data protection and privacy.  
(37) Another area in which the use of AI systems deserves special consideration is the access to  and enjoyment of certain essential private and public services and benefits necessary for  people to fully participate in society or to improve one’s standard of living. In particular,  natural persons applying for or receiving essential public assistance benefits and services  from public authorities namely healthcare services, social security benefits, social services  providing protection in cases such as maternity, illness, industrial accidents, dependency or  old age and loss of employment and social and housing assistance, are typically dependent  on those benefits and services and in a vulnerable position in relation to the responsible 
 
authorities. If AI systems are used for determining whether such benefits and services  should be granted, denied, reduced, revoked or reclaimed by authorities, including whether  beneficiaries are legitimately entitled to such benefits or services, those systems may have  a significant impact on persons’ livelihood and may infringe their fundamental rights, such  as the right to social protection, non-discrimination, human dignity or an effective remedy  and should therefore be classified as high-risk. Nonetheless, this Regulation should not  hamper the development and use of innovative approaches in the public administration,  which would stand to benefit from a wider use of compliant and safe AI systems, provided  that those systems do not entail a high risk to legal and natural persons. In addition, AI systems used to evaluate the credit score or creditworthiness of natural persons should be  classified as high-risk AI systems, since they determine those persons’ access to financial  resources or essential services such as housing, electricity, and telecommunication  services. AI systems used for this purpose may lead to discrimination of persons or groups  and perpetuate historical patterns of discrimination, for example based on racial or ethnic  origins, gender, disabilities, age, sexual orientation, or create new forms of discriminatory  impacts. However, AI systems provided for by Union law for the purpose of detecting  fraud in the offering of financial services and for prudential purposes to calculate credit  institutions’ and insurances undertakings’ capital requirements should not be considered as  high-risk under this Regulation. Moreover, AI systems intended to be used for risk  assessment and pricing in relation to natural persons for health and life insurance can also  have a significant impact on persons’ livelihood and if not duly designed, developed and  used, can infringe their fundamental rights and can lead to serious consequences for  people’s life and health, including financial exclusion and discrimination. Finally, AI  systems used to evaluate and classify emergency calls by natural persons or to dispatch or  establish priority in the dispatching of emergency first response services, including by  police, firefighters and medical aid, as well as of emergency healthcare patient triage  systems, should also be classified as high-risk since they make decisions in very critical  situations for the life and health of persons and their property. 
(38) Given their role and responsibility, actions by law enforcement authorities involving  certain uses of AI systems are characterised by a significant degree of power imbalance  and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as  other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the  AI system is not trained with high quality data, does not meet adequate requirements in  terms of its performance, its accuracy or robustness, or is not properly designed and tested 
 
before being put on the market or otherwise put into service, it may single out people in a  discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of  important procedural fundamental rights, such as the right to an effective remedy and to a  fair trial as well as the right of defence and the presumption of innocence, could be  hampered, in particular, where such AI systems are not sufficiently transparent,  explainable and documented. It is therefore appropriate to classify as high-risk, insofar as  their use is permitted under relevant Union and national law, a number of AI systems  intended to be used in the law enforcement context where accuracy, reliability and  transparency is particularly important to avoid adverse impacts, retain public trust and  ensure accountability and effective redress. In view of the nature of the activities in  question and the risks relating thereto, those high-risk AI systems should include in  particular AI systems intended to be used by or on behalf of law enforcement authorities or  by Union agencies, offices or bodies in support of law enforcement authorities for  assessing the risk of a natural person to become a victim of criminal offences, as  polygraphs and similar tools , for the evaluation of the reliability of evidence in in the  course of investigation or prosecution of criminal offences, and, insofar not prohibited  under this regulation, for assessing the risk of a natural person of offending or reoffending  not solely based on profiling of natural persons nor based on assessing personality traits  and characteristics or past criminal behaviour of natural persons or groups, for profiling in  the course of detection, investigation or prosecution of criminal offences, . AI systems  specifically intended to be used for administrative proceedings by tax and customs  authorities as well as by financial intelligence units carrying out administrative tasks  analysing information pursuant to Union anti-money laundering legislation should not be  classified as high-risk AI systems used by law enforcement authorities for the purposes of  prevention, detection, investigation and prosecution of criminal offences. The use of AI  tools by law enforcement and authorities should not become a factor of inequality, or  exclusion. The impact of the use of AI tools on the defence rights of suspects should not be  ignored, notably the difficulty in obtaining meaningful information on the functioning of  these systems and the consequent difficulty in challenging their results in court, in  particular by individuals under investigation. 
(39) AI systems used in migration, asylum and border control management affect people who  are often in particularly vulnerable position and who are dependent on the outcome of the  actions of the competent public authorities. The accuracy, non-discriminatory nature and  transparency of the AI systems used in those contexts are therefore particularly important 
 
to guarantee the respect of the fundamental rights of the affected persons, notably their  rights to free movement, non-discrimination, protection of private life and personal data,  international protection and good administration. It is therefore appropriate to classify as  high-risk, insofar as their use is permitted under relevant Union and national law AI  systems intended to be used by or on behalf of competent public authorities or by Union  agencies, offices or bodies charged with tasks in the fields of migration, asylum and border  control management as polygraphs and similar tools, for assessing certain risks posed by  natural persons entering the territory of a Member State or applying for visa or asylum, for  assisting competent public authorities for the examination, including related assessment of  the reliability of evidence, of applications for asylum, visa and residence permits and  associated complaints with regard to the objective to establish the eligibility of the natural  persons applying for a status, for the purpose of detecting, recognising or identifying  natural persons in the context of migration, asylum and border control management with  the exception of travel documents. AI systems in the area of migration, asylum and border  control management covered by this Regulation should comply with the relevant  procedural requirements set by the Directive 2013/32/EU of the European Parliament and  of the Council20, the Regulation (EC) No 810/2009 of the European Parliament and of the  Council21 and other relevant legislation. The use of AI systems in migration, asylum and  border control management should in no circumstances be used by Member States or  Union institutions, agencies or bodies as a means to circumvent their international  obligations under the Convention of 28 July 1951 relating to the Status of Refugees as  amended by the Protocol of 31 January 1967, nor should they be used to in any way  infringe on the principle of non-refoulement, or deny safe and effective legal avenues into  the territory of the Union, including the right to international protection. 
(40) Certain AI systems intended for the administration of justice and democratic processes  should be classified as high-risk, considering their potentially significant impact on  democracy, rule of law, individual freedoms as well as the right to an effective remedy and  to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is  appropriate to qualify as high-risk AI systems intended to be used by a judicial authority or  on its behalf to assist judicial authorities in researching and interpreting facts and the law  and in applying the law to a concrete set of facts. AI systems intended to be used by    
20 Directive 2013/32/EU of the European Parliament and of the Council of 26 June 2013 on common procedures  for granting and withdrawing international protection (OJ L 180, 29.6.2013, p. 60). 
21 Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a  Community Code on Visas (Visa Code) (OJ L 243, 15.9.2009, p. 1).
 
alternative dispute resolution bodies for those purposes should also be considered high-risk  when the outcomes of the alternative dispute resolution proceedings produce legal effects  for the parties. The use of artificial intelligence tools can support the decision-making  power of judges or judicial independence, but should not replace it, as the final decision making must remain a human-driven activity and decision. Such qualification should not  extend, however, to AI systems intended for purely ancillary administrative activities that  do not affect the actual administration of justice in individual cases, such as anonymisation  or pseudonymisation of judicial decisions, documents or data, communication between  personnel, administrative tasks. 
(40a) Without prejudice to the rules provided for in [Regulation xxx on the transparency and  targeting of political advertising], and in order to address the risks of undue external  interference to the right to vote enshrined in Article 39 of the Charter, and of adverse  effects on democracy, and the rule of law, AI systems intended to be used to influence the  outcome of an election or referendum or the voting behaviour of natural persons in the  exercise of their vote in elections or referenda should be classified as high-risk AI systems  with the exception of AI systems whose output natural persons are not directly exposed to,  such as tools used to organise, optimise and structure political campaigns from an  administrative and logistical point of view.  
(41) The fact that an AI system is classified as a high-risk AI system under this Regulation  should not be interpreted as indicating that the use of the system is lawful under other acts  of Union law or under national law compatible with Union law, such as on the protection  of personal data, on the use of polygraphs and similar tools or other systems to detect the  emotional state of natural persons. Any such use should continue to occur solely in  accordance with the applicable requirements resulting from the Charter and from the  applicable acts of secondary Union law and national law. This Regulation should not be  understood as providing for the legal ground for processing of personal data, including  special categories of personal data, where relevant, unless it is specifically provided for  otherwise in this Regulation. 
(42) To mitigate the risks from high-risk AI systems placed on the market or put into service  and to ensure a high level of trustworthiness, certain mandatory requirements should apply  to high-risk AI systems, taking into account the intended purpose and the context of use of  the AI system and according to the risk management system to be established by the 
 
provider. The measures adopted by the providers to comply with the mandatory  requirements of this Regulation should take into account the generally acknowledge state  of the art on artificial intelligence, be proportionate and effective to meet the objectives of  this Regulation. Following the New Legislative Framework approach, as clarified in  Commission notice the ‘Blue Guide’ on the implementation of EU product rules 2022  (C/2022/3637), the general rule is that several pieces of the EU legislation may have to be  taken into consideration for one product, since the making available or putting into service  can only take place when the product complies with all applicable Union harmonisation  legislation. Hazards of AI systems covered by the requirements of this Regulation concern  different aspects than the existing Union harmonisation acts and therefore the requirements  of this Regulation would complement the existing body of the Union harmonisation acts.  For example, machinery or medical devices products incorporating an AI system might  present risks not addressed by the essential health and safety requirements set out in the  relevant Union harmonised legislation, as this sectoral legislation does not deal with risks  specific to AI systems. This calls for a simultaneous and complementary application of the  various legislative acts. To ensure consistency and avoid unnecessary administrative  burden or costs, providers of a product that contains one or more high-risk artificial  intelligence system, to which the requirements of this Regulation as well as requirements  of the Union harmonisation legislation listed in Annex II, Section A apply, should have a  flexibility on operational decisions on how to ensure compliance of a product that contains  one or more artificial intelligence systems with all applicable requirements of the Union  harmonised legislation in a best way. This flexibility could mean, for example a decision  by the provider to integrate a part of the necessary testing and reporting processes,  information and documentation required under this Regulation into already existing  documentation and procedures required under the existing Union harmonisation legislation  listed in Annex II, Section A. This however should not in any way undermine the  obligation of the provider to comply with all the applicable requirements. 
(42a) The risk management system should consist of a continuous, iterative process that is  planned and run throughout the entire lifecycle of a high-risk AI system. This process  should be aimed at identifying and mitigating the relevant risks of artificial intelligence  systems on health, safety and fundamental rights. The risk management system should be  regularly reviewed and updated to ensure its continuing effectiveness, as well as  justification and documentation of any significant decisions and actions taken subject to  this Regulation. This process should ensure that the provider identifies risks or adverse 
 
impacts and implements mitigation measures for the known and reasonably foreseeable  risks of artificial intelligence systems to the health, safety and fundamental rights in light  of its intended purpose and reasonably foreseeable misuse, including the possible risks  arising from the interaction between the AI system and the environment within which it  operates. The risk management system should adopt the most appropriate risk management  measures in the light of the state of the art in AI. When identifying the most appropriate  risk management measures, the provider should document and explain the choices made  and, when relevant, involve experts and external stakeholders. In identifying reasonably  foreseeable misuse of high-risk AI systems the provider should cover uses of the AI  systems which, while not directly covered by the intended purpose and provided for in the  instruction for use may nevertheless be reasonably expected to result from readily  predictable human behaviour in the context of the specific characteristics and use of the  particular AI system. Any known or foreseeable circumstances, related to the use of the  high-risk AI system in accordance with its intended purpose or under conditions of  reasonably foreseeable misuse, which may lead to risks to the health and safety or  fundamental rights should be included in the instructions for use provided by the provider.  This is to ensure that the deployer is aware and takes them into account when using the  high-risk AI system. Identifying and implementing risk mitigation measures for  foreseeable misuse under this Regulation should not require specific additional training  measures for the high-risk AI system by the provider to address them. The providers  however are encouraged to consider such additional training measures to mitigate  reasonable foreseeable misuses as necessary and appropriate.  
(43) Requirements should apply to high-risk AI systems as regards risk management, the  quality and relevance of data sets used, technical documentation and record-keeping,  transparency and the provision of information to deployers, human oversight, and  robustness, accuracy and cybersecurity. Those requirements are necessary to effectively  mitigate the risks for health, safety and fundamental rights, and no other less trade  restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade.  
(44) High quality data and access to high quality data plays a vital role in providing structure  and in ensuring the performance of many AI systems, especially when techniques  involving the training of models are used, with a view to ensure that the high-risk AI  system performs as intended and safely and it does not become a source of discrimination  prohibited by Union law. High quality datasets for training, validation and testing require 
 
the implementation of appropriate data governance and management practices. Datasets for  training, validation and testing, including the labels, should be relevant, sufficiently  representative, and to the best extent possible free of errors and complete in view of the  intended purpose of the system. In order to facilitate compliance with EU data protection  law, such as Regulation (EU) 2016/679, data governance and management practices  should include, in the case of personal data, transparency about the original purpose of the  data collection, The datasets should also have the appropriate statistical properties,  including as regards the persons or groups of persons in relation to whom the high-risk AI  system is intended to be used, with specific attention to the mitigation of possible biases in  the datasets, that are likely to affect the health and safety of persons, negatively impact  fundamental rights or lead to discrimination prohibited under Union law, especially where  data outputs influence inputs for future operations (‘feedback loops’) . Biases can for  example be inherent in underlying datasets, especially when historical data is being used,  or generated when the systems are implemented in real world settings. Results provided by  AI systems could be influenced by such inherent biases that are inclined to gradually  increase and thereby perpetuate and amplify existing discrimination, in particular for  persons belonging to certain vulnerable groups including racial or ethnic groups. The  requirement for the datasets to be to the best extent possible complete and free of errors  should not affect the use of privacy-preserving techniques in the context of the  development and testing of AI systems. In particular, datasets should take into account, to  the extent required by their intended purpose, the features, characteristics or elements that  are particular to the specific geographical, contextual, behavioural or functional setting  which the AI system is intended to be used. The requirements related to data governance  can be complied with by having recourse to third parties that offer certified compliance  services including verification of data governance, data set integrity, and data training,  validation and testing practices, as far as compliance with the data requirements of this  Regulation are ensured.  
(45) For the development and assessment of high-risk AI systems, certain actors, such as  providers, notified bodies and other relevant entities, such as digital innovation hubs,  testing experimentation facilities and researchers, should be able to access and use high  quality datasets within their respective fields of activities which are related to this  Regulation. European common data spaces established by the Commission and the  facilitation of data sharing between businesses and with government in the public interest  will be instrumental to provide trustful, accountable and non-discriminatory access to high 
 
quality data for the training, validation and testing of AI systems. For example, in health,  the European health data space will facilitate non-discriminatory access to health data and  the training of artificial intelligence algorithms on those datasets, in a privacy-preserving,  
secure, timely, transparent and trustworthy manner, and with an appropriate institutional  governance. Relevant competent authorities, including sectoral ones, providing or  supporting the access to data may also support the provision of high-quality data for the  training, validation and testing of AI systems.  
(45a) The right to privacy and to protection of personal data must be guaranteed throughout the  entire lifecycle of the AI system. In this regard, the principles of data minimisation and  data protection by design and by default, as set out in Union data protection law, are  applicable when personal data are processed. Measures taken by providers to ensure  compliance with those principles may include not only anonymisation and encryption, but  also the use of technology that permits algorithms to be brought to the data and allows  training of AI systems without the transmission between parties or copying of the raw or  structured data themselves, without prejudice to the requirements on data governance  provided for in this Regulation. 
(44c) In order to protect the right of others from the discrimination that might result from the  bias in AI systems, the providers should, exceptionally, to the extent that it is strictly  necessary for the purposes of ensuring bias detection and correction in relation to the high risk AI systems, subject to appropriate safeguards for the fundamental rights and freedoms  of natural persons and following the application of all applicable conditions laid down  under this Regulation in addition to the conditions laid down in Regulation (EU) 2016/679,  Directive (EU) 2016/680 and Regulation (EU) 2018/1725,be able to process also special  categories of personal data, as a matter of substantial public interest within the meaning of  Article 9(2)(g) of Regulation (EU) 2016/679 and Article 10(2)g) of Regulation (EU)  2018/1725. 
(46) Having comprehensible information on how high-risk AI systems have been developed  and how they perform throughout their lifetime is essential to enable traceability of those  systems, verify compliance with the requirements under this Regulation, as well as  monitoring of their operations and post market monitoring. This requires keeping records  and the availability of a technical documentation, containing information which is  necessary to assess the compliance of the AI system with the relevant requirements and 
 
facilitate post market monitoring. Such information should include the general  characteristics, capabilities and limitations of the system, algorithms, data, training, testing  and validation processes used as well as documentation on the relevant risk management  system and drawn in a clear and comprehensive form. The technical documentation should  be kept up to date, appropriately throughout the lifetime of the AI system. Furthermore,  high risk AI systems should technically allow for automatic recording of events (logs) over  the duration of the lifetime of the system. 
(47) To address concerns related to opacity and complexity of certain AI systems and help  deployers to fulfil their obligations under this Regulation, transparency should be required  for high-risk AI systems before they are placed on the market or put it into service. High risk AI systems should be designed in a manner to enable deployers to understand how the  AI system works, evaluate its functionality, and comprehend its strengths and limitations.  High-risk AI systems should be accompanied by appropriate information in the form of  instructions of use. Such information should include the characteristics, capabilities and  limitations of performance of the AI system. These would cover information on possible  known and foreseeable circumstances related to the use of the high-risk AI system,  including deployer action that may influence system behaviour and performance, under  which the AI system can lead to risks to health, safety, and fundamental rights, on the  changes that have been pre-determined and assessed for conformity by the provider and on  the relevant human oversight measures, including the measures to facilitate the  interpretation of the outputs of the AI system by the deployers. Transparency, including the  accompanying instructions for use, should assist deployers in the use of the system and  support informed decision making by them. Among others, deployers should be in a better  position to make the correct choice of the system they intend to use in the light of the  obligations applicable to them, be educated about the intended and precluded uses, and use  the AI system correctly and as appropriate. In order to enhance legibility and accessibility  of the information included in the instructions of use, where appropriate, illustrative  examples, for instance on the limitations and on the intended and precluded uses of the AI  system, should be included. Providers should ensure that all documentation, including the  instructions for use, contains meaningful, comprehensive, accessible and understandable  information, taking into account the needs and foreseeable knowledge of the target  deployers. Instructions for use should be made available in a language which can be easily  understood by target deployers, as determined by the Member State concerned. 
 
(48) High-risk AI systems should be designed and developed in such a way that natural persons  can oversee their functioning, ensure that they are used as intended and that their impacts  are addressed over the system’s lifecycle. For this purpose, appropriate human oversight  measures should be identified by the provider of the system before its placing on the  market or putting into service. In particular, where appropriate, such measures should  guarantee that the system is subject to in-built operational constraints that cannot be  overridden by the system itself and is responsive to the human operator, and that the  natural persons to whom human oversight has been assigned have the necessary  competence, training and authority to carry out that role. It is also essential, as appropriate,  to ensure that high-risk AI systems include mechanisms to guide and inform a natural  person to whom human oversight has been assigned to make informed decisions if, when  and how to intervene in order to avoid negative consequences or risks, or stop the system if  it does not perform as intended. Considering the significant consequences for persons in  case of incorrect matches by certain biometric identification systems, it is appropriate to  provide for an enhanced human oversight requirement for those systems so that no action  or decision may be taken by the deployer on the basis of the identification resulting from  the system unless this has been separately verified and confirmed by at least two natural  persons. Those persons could be from one or more entities and include the person  operating or using the system. This requirement should not pose unnecessary burden or  delays and it could be sufficient that the separate verifications by the different persons are  automatically recorded in the logs generated by the system. Given the specificities of the  areas of law enforcement, migration, border control and asylum, this requirement should  not apply in cases where Union or national law considers the application of this  requirement to be disproportionate.  
(49) High-risk AI systems should perform consistently throughout their lifecycle and meet an  appropriate level of accuracy, robustness and cybersecurity, in the light of their intended  purpose and in accordance with the generally acknowledged state of the art. The  Commission and relevant organisations and stakeholders are encouraged to take due  consideration of mitigation of risks and negative impacts of the AI system. The expected  level of performance metrics should be declared in the accompanying instructions of use.  Providers are urged to communicate this information to deployers in a clear and easily  understandable way, free of misunderstandings or misleading statements. The EU  legislation on legal metrology, including on Measuring Instruments Directive (MID) and  Non-automatic weighing instruments (NAWI) Directive, aims to ensure the accuracy of 
 
measurements and to help the transparency and fairness of commercial transactions. In this  context, in cooperation with relevant stakeholders and organisation, such as metrology and  benchmarking authorities, the Commission should encourage, as appropriate, the  development of benchmarks and measurement methodologies for AI systems. In doing so,  the Commission should take note and collaborate with international partners working on  metrology and relevant measurement indicators relating to Artificial Intelligence. 
(50) The technical robustness is a key requirement for high-risk AI systems. They should be  resilient in relation to harmful or otherwise undesirable behaviour that may result from  limitations within the systems or the environment in which the systems operate (e.g. errors,  faults, inconsistencies, unexpected situations). Therefore, technical and organisational  measures should be taken to ensure robustness of high-risk AI systems, for example by  designing and developing appropriate technical solutions to prevent or minimize harmful  or otherwise undesirable behaviour. Those technical solution may include for instance  mechanisms enabling the system to safely interrupt its operation (fail-safe plans) in the  presence of certain anomalies or when operation takes place outside certain predetermined  boundaries. Failure to protect against these risks could lead to safety impacts or negatively  affect the fundamental rights, for example due to erroneous decisions or wrong or biased  outputs generated by the AI system.  
(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts  to alter their use, behaviour, performance or compromise their security properties by  malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI  systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or  trained models (e.g. adversarial attacks or membership inference), or exploit vulnerabilities  in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of  cybersecurity appropriate to the risks, suitable measures, such as security controls, should  therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure. 
(51a) Without prejudice to the requirements related to robustness and accuracy set out in this  Regulation, high-risk AI systems which fall within the scope of the Regulation 2022/0272,  in accordance with Article 8 of the Regulation 2022/0272 may demonstrate compliance  with the cybersecurity requirement of this Regulation by fulfilling the essential  cybersecurity requirements set out in Article 10 and Annex I of the Regulation 
 
2022/0272.When high-risk AI systems fulfil the essential requirements of Regulation  2022/0272, they should be deemed compliant with the cybersecurity requirements set out  in this Regulation in so far as the achievement of those requirements is demonstrated in the  EU declaration of conformity or parts thereof issued under Regulation 2022/0272. For this  purpose, the assessment of the cybersecurity risks, associated to a product with digital  elements classified as high-risk AI system according to this Regulation, carried out under  Regulation 2022/0272, should consider risks to the cyber resilience of an AI system as  regards attempts by unauthorised third parties to alter its use, behaviour or performance,  including AI specific vulnerabilities such as data poisoning or adversarial attacks, as well  as, as relevant, risks to fundamental rights as required by this Regulation. The conformity  assessment procedure provided by this Regulation should apply in relation to the essential  cybersecurity requirements of a product with digital elements covered by Regulation  2022/0272 and classified as a high-risk AI system under this Regulation. However, this  rule should not result in reducing the necessary level of assurance for critical products with  digital elements covered by Regulation 2022/0272. Therefore, by way of derogation from  this rule, high-risk AI systems that fall within the scope of this Regulation and are also  qualified as important and critical products with digital elements pursuant to Regulation  2022/0272 and to which the conformity assessment procedure based on internal control  referred to in Annex VI of this Regulation applies, are subject to the conformity  assessment provisions of Regulation 2022/0272 insofar as the essential cybersecurity  requirements of Regulation 2022/0272 are concerned. In this case, for all the other aspects  covered by this Regulation the respective provisions on conformity assessment based on  internal control set out in Annex VI of this Regulation should apply. Building on the  knowledge and expertise of ENISA on the cybersecurity policy and tasks assigned to  ENISA under the Regulation 2019/1020 the European Commission should cooperate with  ENISA on issues related to cybersecurity of AI systems.  
(52) As part of Union harmonisation legislation, rules applicable to the placing on the market,  putting into service and use of high-risk AI systems should be laid down consistently with  Regulation (EC) No 765/2008 of the European Parliament and of the Council22 setting out  the requirements for accreditation and the market surveillance of products, Decision No  
  
22 Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out the  requirements for accreditation and market surveillance relating to the marketing of products and repealing  Regulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).
 
768/2008/EC of the European Parliament and of the Council23 on a common framework  for the marketing of products and Regulation (EU) 2019/1020 of the European Parliament  and of the Council24 on market surveillance and compliance of products (‘New Legislative  Framework for the marketing of products’). 
(53) It is appropriate that a specific natural or legal person, defined as the provider, takes the  responsibility for the placing on the market or putting into service of a high-risk AI system,  regardless of whether that natural or legal person is the person who designed or developed  the system.  
(53a) As signatories to the United Nations Convention on the Rights of Persons with Disabilities  (UNCRPD), the Union and the Member States are legally obliged to protect persons with  disabilities from discrimination and promote their equality, to ensure that persons with  disabilities have access, on an equal basis with others, to information and communications  technologies and systems, and to ensure respect for privacy for persons with disabilities.  Given the growing importance and use of AI systems, the application of universal design  principles to all new technologies and services should ensure full and equal access for  everyone potentially affected by or using AI technologies, including persons with  disabilities, in a way that takes full account of their inherent dignity and diversity. It is  therefore essential that Providers ensure full compliance with accessibility requirements,  including Directive (EU) 2016/2102 and Directive (EU) 2019/882. Providers should ensure  compliance with these requirements by design. Therefore, the necessary measures should  be integrated as much as possible into the design of the high-risk AI system. 
(54) The provider should establish a sound quality management system, ensure the  accomplishment of the required conformity assessment procedure, draw up the relevant  documentation and establish a robust post-market monitoring system. Providers of high risk AI systems that are subject to obligations regarding quality management systems  under relevant sectorial Union law should have the possibility to include the elements of  the quality management system provided for in this Regulation as part of the existing  quality management system provided for in that other sectorial Union legislation. The  
  
23 Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common  framework for the marketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, 13.8.2008,  p. 82). 
24 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market  surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No  765/2008 and (EU) No 305/2011 (Text with EEA relevance) (OJ L 169, 25.6.2019, p. 1–44).
 
complementarity between this Regulation and existing sectorial Union law should also be  taken into account in future standardization activities or guidance adopted by the  Commission. Public authorities which put into service high-risk AI systems for their own  use may adopt and implement the rules for the quality management system as part of the  quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the  public authority in question.  
(56) To enable enforcement of this Regulation and create a level-playing field for operators, and  taking into account the different forms of making available of digital products, it is  important to ensure that, under all circumstances, a person established in the Union can  provide authorities with all the necessary information on the compliance of an AI system.  Therefore, prior to making their AI systems available in the Union, providers established  outside the Union shall, by written mandate, appoint an authorised representative  established in the Union. This authorised representative plays a pivotal role in ensuring the  compliance of the high-risk AI systems placed on the market or put into service in the  Union by those providers who are not established in the Union and in serving as their  contact person established in the Union.  
(56a) In the light of the nature and complexity of the value chain for AI systems and in line with  New Legislative Framework principles, it is essential to ensure legal certainty and facilitate  the compliance with this Regulation. Therefore, it is necessary to clarify the role and the  specific obligations of relevant operators along the value chain, such as importers and  distributors who may contribute to the development of AI systems. In certain situations  those operators could act in more than one role at the same time and should therefore fulfil  cumulatively all relevant obligations associated with those roles. For example, an operator  could act as a distributor and an importer at the same time. 
(57) To ensure legal certainty, it is necessary to clarify that, under certain specific conditions,  any distributor, importer, deployer or other third-party should be considered a provider of a  high-risk AI system and therefore assume all the relevant obligations. This would be the  case if that party puts its name or trademark on a high-risk AI system already placed on the  market or put into service, without prejudice to contractual arrangements stipulating that  the obligations are allocated otherwise, or if that party make a substantial modification to a  high-risk AI system that has already been placed on the market or has already been put into 
 
service and in a way that it remains a high-risk AI system in accordance with Article 6, or  if it modifies the intended purpose of an AI system, including a general purpose AI system,  which has not been classified as high-risk and has already been placed on the market or  put into service, in a way that the AI system becomes a high-risk AI system in accordance  with Article 6. These provisions should apply without prejudice to more specific  provisions established in certain New Legislative Framework sectorial legislation with  which this Regulation should apply jointly. For example, Article 16, paragraph 2 of  Regulation 745/2017, establishing that certain changes should not be considered  modifications of a device that could affect its compliance with the applicable requirements,  should continue to apply to high-risk AI systems that are medical devices within the  meaning of that Regulation. 
(57a) General purpose AI systems may be used as high-risk AI systems by themselves or be  components of other high risk AI systems. Therefore, due to their particular nature and in  order to ensure a fair sharing of responsibilities along the AI value chain, the providers of  such systems should, irrespective of whether they may be used as high-risk AI systems as  such by other providers or as components of high-risk AI systems and unless provided  otherwise under this Regulation, closely cooperate with the providers of the respective  high-risk AI systems to enable their compliance with the relevant obligations under this  Regulation and with the competent authorities established under this Regulation.  
(57b) Where, under the conditions laid down in this Regulation, the provider that initially placed  the AI system on the market or put it into service should no longer be considered the  provider for the purposes of this Regulation, and when that provider has not expressly  excluded the change of the AI system into a high-risk AI system, the former provider  should nonetheless closely cooperate and make available the necessary information and  provide the reasonably expected technical access and other assistance that are required for  the fulfilment of the obligations set out in this Regulation, in particular regarding the  compliance with the conformity assessment of high-risk AI systems.  
(57c) In addition, where a high-risk AI system that is a safety component of a product which is  covered by a relevant New Legislative Framework sectorial legislation is not placed on the  market or put into service independently from the product, the product manufacturer as  defined under the relevant New Legislative Framework legislation should comply with the 
 
obligations of the provider established in this Regulation and notably ensure that the AI  system embedded in the final product complies with the requirements of this Regulation.  
(57d) Within the AI value chain multiple parties often supply AI systems, tools and services but  also components or processes that are incorporated by the provider into the AI system with  various objectives, including the model training, model retraining, model testing and  evaluation, integration into software, or other aspects of model development. These parties  have an important role in the value chain towards the provider of the high-risk AI system  into which their AI systems, tools, services, components or processes are integrated, and  should provide by written agreement this provider with the necessary information,  capabilities, technical access and other assistance based on the generally acknowledged  state of the art, in order to enable the provider to fully comply with the obligations set out  in this Regulation, without compromising their own intellectual property rights or trade  secrets. 
(57e) Third parties making accessible to the public tools, services, processes, or AI components  other than general-purpose AI models, shall not be mandated to comply with requirements  targeting the responsibilities along the AI value chain, in particular towards the provider  that has used or integrated them, when those tools, services, processes, or AI components  are made accessible under a free and open licence. Developers of free and open-source  tools, services, processes, or AI components other than general-purpose AI models should  be encouraged to implement widely adopted documentation practices, such as model cards  and data sheets, as a way to accelerate information sharing along the AI value chain,  allowing the promotion of trustworthy AI systems in the Union.  
(57f) The Commission could develop and recommend voluntary model contractual terms  between providers of high-risk AI systems and third parties that supply tools, services,  components or processes that are used or integrated in high-risk AI systems, to facilitate  the cooperation along the value chain. When developing voluntary model contractual  terms, the Commission should also take into account possible contractual requirements  applicable in specific sectors or business cases.  
(58) Given the nature of AI systems and the risks to safety and fundamental rights possibly  associated with their use, including as regards the need to ensure proper monitoring of the  performance of an AI system in a real-life setting, it is appropriate to set specific  responsibilities for deployers. Deployers should in particular take appropriate technical and 
 
organisational measures to ensure they use high-risk AI systems in accordance with the  instructions of use and certain other obligations should be provided for with regard to  monitoring of the functioning of the AI systems and with regard to record-keeping, as  appropriate. Furthermore, deployers should ensure that the persons assigned to implement  the instructions for use and human oversight as set out in this Regulation have the  necessary competence, in particular an adequate level of AI literacy, training and authority  to properly fulfil those tasks. These obligations should be without prejudice to other  deployer obligations in relation to high-risk AI systems under Union or national law. 
(58b) This Regulation is without prejudice to obligations for employers to inform or to inform  and consult workers or their representatives under Union or national law and practice,  including directive 2002/14/EC on a general framework for informing and consulting  employees, on decisions to put into service or use AI systems. It remains necessary to  ensure information of workers and their representatives on the planned deployment of  high-risk AI systems at the workplace in cases where the conditions for those information  or information and consultation obligations in other legal instruments are not fulfilled.  Moreover, such information right is ancillary and necessary to the objective of protecting  fundamental rights that underlies this Regulation. Therefore, an information requirement to  that effect should be laid down in this regulation, without affecting any existing rights of  workers. 
(58b) Whilst risks related to AI systems can result from the way such systems are designed, risks  can as well stem from how such AI systems are used. Deployers of high-risk AI system  therefore play a critical role in ensuring that fundamental rights are protected,  complementing the obligations of the provider when developing the AI system. Deployers  are best placed to understand how the high-risk AI system will be used concretely and can  therefore identify potential significant risks that were not foreseen in the development  phase, due to a more precise knowledge of the context of use, the people or groups of  people likely to be affected, including vulnerable groups. Deployers of high-risk AI  systems referred to in Annex III also play a critical role in informing natural persons and  should, when they make decisions or assist in making decisions related to natural persons,  where applicable, inform the natural persons that they are subject to the use of the high risk  AI system. This information should include the intended purpose and the type of decisions  it makes. The deployer should also inform the natural person about its right to an  explanation provided under this Regulation. With regard to high-risk AI systems used for 
 
law enforcement purposes, this obligation should be implemented in accordance with  Article 13 of Directive 2016/680.  
(58d) Any processing of biometric data involved in the use of AI systems for biometric  identification for the purpose of law enforcement needs to comply with Article 10 of  Directive (EU) 2016/680, that allows such processing only where strictly necessary,  subject to appropriate safeguards for the rights and freedoms of the data subject, and where  authorised by Union or Member State law. Such use, when authorized, also needs to  respect the principles laid down in Article 4 paragraph 1 of Directive (EU) 2016/680  including lawfulness, fairness and transparency, purpose limitation, accuracy and storage  limitation. 
(58e) Without prejudice to applicable Union law, notably the GDPR and Directive (EU)  2016/680 (the Law Enforcement Directive), considering the intrusive nature of post remote  biometric identification systems, the use of post remote biometric identification systems  shall be subject to safeguards. Post biometric identification systems should always be used  in a way that is proportionate, legitimate and strictly necessary, and thus targeted, in terms  of the individuals to be identified, the location, temporal scope and based on a closed  dataset of legally acquired video footage. In any case, post remote biometric identification  systems should not be used in the framework of law enforcement to lead to indiscriminate  surveillance. The conditions for post remote biometric identification should in any case not  provide a basis to circumvent the conditions of the prohibition and strict exceptions for real  time remote biometric identification. 
(58g) In order to efficiently ensure that fundamental rights are protected, deployers of high-risk  AI systems that are bodies governed by public law, or private operators providing public  services and operators deploying certain high-risk AI system referred to in Annex III, such  as banking or insurance entities, should carry out a fundamental rights impact assessment  prior to putting it into use. Services important for individuals that are of public nature may  also be provided by private entities. Private operators providing such services of public nature are linked to tasks in the public interest such as in the area of education, healthcare,  social services, housing, administration of justice. The aim of the fundamental rights  impact assessment is for the deployer to identify the specific risks to the rights of  individuals or groups of individuals likely to be affected, identify measures to be taken in  case of the materialisation of these risks. The impact assessment should apply to the first 
 
use of the high-risk AI system, and should be updated when the deployer considers that  any of the relevant factors have changed. The impact assessment should identify the  deployer’s relevant processes in which the high-risk AI system will be used in line with its  intended purpose, and should include a description of the period of time and frequency in  which the system is intended to be used as well as of specific categories of natural persons  and groups who are likely to be affected in the specific context of use. The assessment  should also include the identification of specific risks of harm likely to impact the  fundamental rights of these persons or groups. While performing this assessment, the  deployer should take into account information relevant to a proper assessment of impact,  including but not limited to the information given by the provider of the high-risk AI  system in the instructions for use. In light of the risks identified, deployers should  determine measures to be taken in case of the materialization of these risks, including for  example governance arrangements in that specific context of use, such as arrangements for  human oversight according to the instructions of use or, complaint handling and redress  procedures, as they could be instrumental in mitigating risks to fundamental rights in  concrete use-cases. After performing this impact assessment, the deployer should notify  the relevant market surveillance authority. Where appropriate, to collect relevant  information necessary to perform the impact assessment, deployers of high-risk AI system,  in particular when AI systems are used in the public sector, could involve relevant  stakeholders, including the representatives of groups of persons likely to be affected by the  AI system, independent experts, and civil society organisations in conducting such impact  assessments and designing measures to be taken in the case of materialization of the risks.  The AI Office should develop a template for a questionnaire in order to facilitate  compliance and reduce the administrative burden for deployers. 
(60a) The notion of general purpose AI models should be clearly defined and set apart from the  notion of AI systems to enable legal certainty. The definition should be based on the key  functional characteristics of a general-purpose AI model, in particular the generality and  the capability to competently perform a wide range of distinct tasks. These models are  typically trained on large amounts of data, through various methods, such as self supervised, unsupervised or reinforcement learning. General purpose AI models may be  placed on the market in various ways, including through libraries, application  programming interfaces (APIs), as direct download, or as physical copy. These models  may be further modified or fine-tuned into new models. Although AI models are essential  components of AI systems, they do not constitute AI systems on their own. AI models 
 
require the addition of further components, such as for example a user interface, to become  AI systems. AI models are typically integrated into and form part of AI systems. This  Regulation provides specific rules for general purpose AI models and for general purpose  AI models that pose systemic risks, which should apply also when these models are  integrated or form part of an AI system. It should be understood that the obligations for the  providers of general purpose AI models should apply once the general purpose AI models  are placed on the market. When the provider of a general purpose AI model integrates an  own model into its own AI system that is made available on the market or put into service,  that model should be considered as being placed on the market and, therefore, the  obligations in this Regulation for models should continue to apply in addition to those for  AI systems. The obligations foreseen for models should in any case not apply when an own  model is used for purely internal processes that are not essential for providing a product or  a service to third parties and the rights of natural persons are not affected. Considering  their potential significantly negative effects, the general-purpose AI models with systemic  risk should always be subject to the relevant obligations under this Regulation. The  definition should not cover AI models used before their placing on the market for the sole  purpose of research, development and prototyping activities. This is without prejudice to  the obligation to comply with this Regulation when, following such activities, a model is  placed on the market. 
(60b) Whereas the generality of a model could, among other criteria, also be determined by a  number of parameters, models with at least a billion of parameters and trained with a large  amount of data using self-supervision at scale should be considered as displaying  significant generality and competently performing a wide range of distinctive tasks.  
(60c) Large generative AI models are a typical example for a general-purpose AI model, given  that they allow for flexible generation of content (such as in the form of text, audio, images  or video) that can readily accommodate a wide range of distinctive tasks. 
(60d) When a general-purpose AI model is integrated into or forms part of an AI system, this  system should be considered a general-purpose AI system when, due to this integration,  this system has the capability to serve a variety of purposes. A general-purpose AI system  can be used directly, or it may be integrated into other AI systems. 
(60e) Providers of general purpose AI models have a particular role and responsibility in the AI  value chain, as the models they provide may form the basis for a range of downstream 
 
systems, often provided by downstream providers that necessitate a good understanding of  the models and their capabilities, both to enable the integration of such models into their  products, and to fulfil their obligations under this or other regulations. Therefore,  proportionate transparency measures should be foreseen, including the drawing up and  keeping up to date of documentation, and the provision of information on the general  purpose AI model for its usage by the downstream providers. Technical documentation  should be prepared and kept up to date by the general purpose AI model provider for the  purpose of making it available, upon request, to the AI Office and the national competent  authorities. The minimal set of elements contained in such documentations should be  outlined, respectively, in Annex (IXb) and Annex (IXa). The Commission should be  enabled to amend the Annexes by delegated acts in the light of the evolving technological  developments. 
(60i) Software and data, including models, released under a free and open-source licence that  allows them to be openly shared and where users can freely access, use, modify and  redistribute them or modified versions thereof, can contribute to research and innovation in  the market and can provide significant growth opportunities for the Union economy.  General purpose AI models released under free and open-source licences should be  considered to ensure high levels of transparency and openness if their parameters,  including the weights, the information on the model architecture, and the information on  model usage are made publicly available. The licence should be considered free and open source also when it allows users to run, copy, distribute, study, change and improve  software and data, including models under the condition that the original provider of the  model is credited, the identical or comparable terms of distribution are respected. 
(60i+1) Free and open-source AI components covers the software and data, including models and  general purpose AI models, tools, services or processes of an AI system. Free and open source AI components can be provided through different channels, including their  development on open repositories. For the purpose of this Regulation, AI components that  are provided against a price or otherwise monetised, including through the provision of  technical support or other services, including through a software platform, related to the AI  component, or the use of personal data for reasons other than exclusively for improving the  security, compatibility or interoperability of the software, with the exception of  transactions between micro enterprises, should not benefit from the exceptions provided to 
 
free and open source AI components. The fact of making AI components available through  open repositories should not, in itself, constitute a monetisation. 
(60f) The providers of general purpose AI models that are released under a free and open source  license, and whose parameters, including the weights, the information on the model  architecture, and the information on model usage, are made publicly available should be  subject to exceptions as regards the transparency-related requirements imposed on general  purpose AI models, unless they can be considered to present a systemic risk, in which case  the circumstance that the model is transparent and accompanied by an open source license  should not be considered a sufficient reason to exclude compliance with the obligations  under this Regulation. In any case, given that the release of general purpose AI models  under free and open source licence does not necessarily reveal substantial information on  the dataset used for the training or fine-tuning of the model and on how thereby the respect  of copyright law was ensured, the exception provided for general purpose AI models from  compliance with the transparency-related requirements should not concern the obligation  to produce a summary about the content used for model training and the obligation to put  in place a policy to respect Union copyright law in particular to identify and respect the  reservations of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790. 
(60i) General purpose models, in particular large generative models, capable of generating text,  images, and other content, present unique innovation opportunities but also challenges to  artists, authors, and other creators and the way their creative content is created, distributed,  used and consumed. The development and training of such models require access to vast  amounts of text, images, videos, and other data. Text and data mining techniques may be  used extensively in this context for the retrieval and analysis of such content, which may  be protected by copyright and related rights. Any use of copyright protected content  requires the authorization of the rightholder concerned unless relevant copyright  exceptions and limitations apply. Directive (EU) 2019/790 introduced exceptions and  limitations allowing reproductions and extractions of works or other subject matter, for the  purposes of text and data mining, under certain conditions. Under these rules, rightholders  may choose to reserve their rights over their works or other subject matter to prevent text  and data mining, unless this is done for the purposes of scientific research. Where the  rights to opt out has been expressly reserved in an appropriate manner, providers of  general-purpose AI models need to obtain an authorisation from rightholders if they want  to carry out text and data mining over such works. 
 
(60j) Providers that place general purpose AI models on the EU market should ensure  compliance with the relevant obligations in this Regulation. For this purpose, providers of  general purpose AI models should put in place a policy to respect Union law on copyright  and related rights, in particular to identify and respect the reservations of rights expressed  by rightholders pursuant to Article 4(3) of Directive (EU) 2019/790. Any provider placing  a general purpose AI model on the EU market should comply with this obligation,  regardless of the jurisdiction in which the copyright-relevant acts underpinning the training  of these general purpose AI models take place. This is necessary to ensure a level playing  field among providers of general purpose AI models where no provider should be able to  gain a competitive advantage in the EU market by applying lower copyright standards than  those provided in the Union.  
(60k) In order to increase transparency on the data that is used in the pre-training and training of  general purpose AI models, including text and data protected by copyright law, it is  adequate that providers of such models draw up and make publicly available a sufficiently  detailed summary of the content used for training the general purpose model. While taking  into due account the need to protect trade secrets and confidential business information,  this summary should be generally comprehensive in its scope instead of technically  detailed to facilitate parties with legitimate interests, including copyright holders, to  exercise and enforce their rights under Union law, for example by listing the main data  collections or sets that went into training the model, such as large private or public  databases or data archives, and by providing a narrative explanation about other data  sources used. It is appropriate for the AI Office to provide a template for the summary,  which should be simple, effective, and allow the provider to provide the required summary  in narrative form.  
(60ka) With regard to the obligations imposed on providers of general purpose AI models to put  in place a policy to respect Union copyright law and make publicly available a summary of  the content used for the training, the AI Office should monitor whether the provider has  fulfilled those obligations without verifying or proceeding to a work-by-work assessment  of the training data in terms of copyright compliance. This Regulation does not affect the  enforcement of copyright rules as provided for under Union law. 
(60g) Compliance with the obligations foreseen for the providers of general purpose AI models  should be commensurate and proportionate to the type of model provider, excluding the 
 
need for compliance for persons who develop or use models for non-professional or  scientific research purposes, who should nevertheless be encouraged to voluntarily comply  with these requirements. Without prejudice to Union Copyright law, compliance with these  obligations should take due account of the size of the provider and allow simplified ways  of compliance for SMEs including start-ups, that should not represent an excessive cost  and not discourage the use of such models. In case of a modification or fine-tuning of a  model, the obligations for providers should be limited to that modification or fine-tuning,  for example by complementing the already existing technical documentation with  information on the modifications, including new training data sources, as a means to  comply with the value chain obligations provided in this Regulation. 
(60m) General purpose AI models could pose systemic risks which include, but are not limited to,  any actual or reasonably foreseeable negative effects in relation to major accidents,  disruptions of critical sectors and serious consequences to public health and safety; any  actual or reasonably foreseeable negative effects on democratic processes, public and  economic security; the dissemination of illegal, false, or discriminatory content. Systemic  risks should be understood to increase with model capabilities and model reach, can arise  along the entire lifecycle of the model, and are influenced by conditions of misuse, model  reliability, model fairness and model security, the degree of autonomy of the model, its  access to tools, novel or combined modalities, release and distribution strategies, the  potential to remove guardrails and other factors. In particular, international approaches  have so far identified the need to devote attention to risks from potential intentional misuse  or unintended issues of control relating to alignment with human intent; chemical,  biological, radiological, and nuclear risks, such as the ways in which barriers to entry can  be lowered, including for weapons development, design acquisition, or use; offensive  cyber capabilities, such as the ways in vulnerability discovery, exploitation, or operational  use can be enabled; the effects of interaction and tool use, including for example the  capacity to control physical systems and interfere with critical infrastructure; risks from  models of making copies of themselves or “self-replicating” or training other models; the  ways in which models can give rise to harmful bias and discrimination with risks to  individuals, communities or societies; the facilitation of disinformation or harming privacy  with threats to democratic values and human rights; risk that a particular event could lead  to a chain reaction with considerable negative effects that could affect up to an entire city,  an entire domain activity or an entire community.
 
(60n) It is appropriate to establish a methodology for the classification of general purpose AI  models as general purpose AI model with systemic risks. Since systemic risks result from  particularly high capabilities, a general-purpose AI models should be considered to present  systemic risks if it has high-impact capabilities, evaluated on the basis of appropriate  technical tools and methodologies, or significant impact on the internal market due to its  reach. High-impact capabilities in general purpose AI models means capabilities that  match or exceed the capabilities recorded in the most advanced general-purpose AI  models. The full range of capabilities in a model could be better understood after its release  on the market or when users interact with the model. According to the state of the art at the  time of entry into force of this Regulation, the cumulative amount of compute used for the  training of the general purpose AI model measured in floating point operations (FLOPs) is  one of the relevant approximations for model capabilities. The amount of compute used for  training cumulates the compute used across the activities and methods that are intended to  enhance the capabilities of the model prior to deployment, such as pre-training, synthetic  data generation and fine-tuning. Therefore, an initial threshold of FLOPs should be set,  which, if met by a general-purpose AI model, leads to a presumption that the model is a  general-purpose AI model with systemic risks. This threshold should be adjusted over time  to reflect technological and industrial changes, such as algorithmic improvements or  increased hardware efficiency, and should be supplemented with benchmarks and  indicators for model capability. To inform this, the AI Office should engage with the  scientific community, industry, civil society and other experts. Thresholds, as well as tools  and benchmarks for the assessment of high-impact capabilities, should be strong predictors  of generality, its capabilities and associated systemic risk of general-purpose AI models,  and could take into taking into account the way the model will be placed on the market or  the number of users it may affect. To complement this system, there should be a possibility  for the Commission to take individual decisions designating a general-purpose AI model as  a general-purpose AI model with systemic risk if it is found that such model has  capabilities or impact equivalent to those captured by the set threshold. This decision  should be taken on the basis of an overall assessment of the criteria set out in Annex YY,  such as quality or size of the training data set, number of business and end users, its input  and output modalities, its degree of autonomy and scalability, or the tools it has access to.  Upon a reasoned request of a provider whose model has been designated as a general purpose AI model with systemic risk, the Commission should take the request into account 
 
and may decide to reassess whether the general-purpose AI model can still be considered  to present systemic risks. 
(60o) It is also necessary to clarify a procedure for the classification of a general purpose AI  model with systemic risks. A general purpose AI model that meets the applicable threshold  for high-impact capabilities should be presumed to be a general purpose AI models with  systemic risk. The provider should notify the AI Office at the latest two weeks after the  requirements are met or it becomes known that a general purpose AI model will meet the  requirements that lead to the presumption. This is especially relevant in relation to the  FLOP threshold because training of general purpose AI models takes considerable  planning which includes the upfront allocation of compute resources and, therefore,  providers of general purpose AI models are able to know if their model would meet the  threshold before the training is completed. In the context of this notification, the provider  should be able to demonstrate that because of its specific characteristics, a general purpose  AI model exceptionally does not present systemic risks, and that it thus should not be  classified as a general purpose AI model with systemic risks. This information is valuable  for the AI Office to anticipate the placing on the market of general purpose AI models with  systemic risks and the providers can start to engage with the AI Office early on. This is  especially important with regard to general-purpose AI models that are planned to be  released as open-source, given that, after open-source model release, necessary measures to  ensure compliance with the obligations under this Regulation may be more difficult to  implement. 
(60p) If the Commission becomes aware of the fact that a general purpose AI model meets the  requirements to classify as a general purpose model with systemic risk, which previously  had either not been known or of which the relevant provider has failed to notify the  Commission, the Commission should be empowered to designate it so. A system of  qualified alerts should ensure that the AI Office is made aware by the scientific panel of  general-purpose AI models that should possibly be classified as general purpose AI models  with systemic risk, in addition to the monitoring activities of the AI Office.  
(60q) The providers of general-purpose AI models presenting systemic risks should be subject, in  addition to the obligations provided for providers of general purpose AI models, to  obligations aimed at identifying and mitigating those risks and ensuring an adequate level  of cybersecurity protection, regardless of whether it is provided as a standalone model or 
 
embedded in an AI system or a product. To achieve these objectives, the Regulation should  require providers to perform the necessary model evaluations, in particular prior to its first  placing on the market, including conducting and documenting adversarial testing of  models, also, as appropriate, through internal or independent external testing. In addition,  providers of general-purpose AI models with systemic risks should continuously assess  and mitigate systemic risks, including for example by putting in place risk management  policies, such as accountability and governance processes, implementing post-market  monitoring, taking appropriate measures along the entire model’s lifecycle and cooperating  with relevant actors across the AI value chain.  
(60r) Providers of general purpose AI models with systemic risks should assess and mitigate  possible systemic risks. If, despite efforts to identify and prevent risks related to a general  purpose AI model that may present systemic risks, the development or use of the model  causes a serious incident, the general purpose AI model provider should without undue  delay keep track of the incident and report any relevant information and possible corrective  measures to the Commission and national competent authorities. Furthermore, providers  should ensure an adequate level of cybersecurity protection for the model and its physical  infrastructure, if appropriate, along the entire model lifecycle. Cybersecurity protection  related to systemic risks associated with malicious use of or attacks should duly consider  accidental model leakage, unsanctioned releases, circumvention of safety measures, and  defence against cyberattacks, unauthorised access or model theft. This protection could be  facilitated by securing model weights, algorithms, servers, and datasets, such as through  operational security measures for information security, specific cybersecurity policies,  adequate technical and established solutions, and cyber and physical access controls,  appropriate to the relevant circumstances and the risks involved.  
(60s) The AI Office should encourage and facilitate the drawing up, review and adaptation of  Codes of Practice, taking into account international approaches. All providers of general purpose AI models could be invited to participate. To ensure that the Codes of Practice  reflect the state of the art and duly take into account a diverse set of perspectives, the AI  Office should collaborate with relevant national competent authorities, and could, where  appropriate, consult with civil society organisations and other relevant stakeholders and  experts, including the Scientific Panel, for the drawing up of the Codes. Codes of Practice  should cover obligations for providers of general-purpose AI models and of general purpose models presenting systemic risks. In addition, as regards systemic risks, Codes of 
 
Practice should help to establish a risk taxonomy of the type and nature of the systemic  risks at Union level, including their sources. Codes of practice should also be focused on  specific risk assessment and mitigation measures. 
(60t) The Codes of Practice should represent a central tool for the proper compliance with the  obligations foreseen under this Regulation for providers of general-purpose AI models.  Providers should be able to rely on Codes of Practice to demonstrate compliance with the  obligations. By means of implementing acts, the Commission may decide to approve a  code of practice and give it a general validity within the Union, or, alternatively, to provide  common rules for the implementation of the relevant obligations, if, by the time the  Regulation becomes applicable, a Code of Practice cannot be finalised or is not deemed  adequate by the AI Office. Once a harmonised standard is published and assessed as  suitable to cover the relevant obligations by the AI Office, the compliance with a European  harmonised standard should grant providers the presumption of conformity. Providers of  general purpose AI models should furthermore be able to demonstrate compliance using  alternative adequate means, if codes of practice or harmonized standards are not available,  or they choose not to rely on those.  
(60u) This Regulation regulates AI systems and models by imposing certain requirements and  obligations for relevant market actors that are placing them on the market, putting into  service or use in the Union, thereby complementing obligations for providers of  intermediary services that embed such systems or models into their services regulated by  Regulation (EU) 2022/2065. To the extent that such systems or models are embedded into  designated very large online platforms or very large online search engines, they are subject  to the risk management framework provided for in Regulation (EU) 2022/2065.  Consequently, the corresponding obligations of the AI Act should be presumed to be  fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065  emerge and are identified in such models. Within this framework, providers of very large  online platforms and very large search engines are obliged to assess potential systemic  risks stemming from the design, functioning and use of their services, including how the  design of algorithmic systems used in the service may contribute to such risks, as well as  systemic risks stemming from potential misuses. Those providers are also obliged to take  appropriate mitigating measures in observance of fundamental rights.
 
(60aa) Considering the quick pace of innovation and the technological evolution of digital  services in scope of different instruments of Union law in particular having in mind the  usage and the perception of their recipients, the AI systems subject to this Regulation may  be provided as intermediary services or parts thereof within the meaning of Regulation  (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For  example, AI systems may be used to provide online search engines, in particular, to the  extent that an AI system such as an online chatbot performs searches of, in principle, all  websites, then incorporates the results into its existing knowledge and uses the updated  knowledge to generate a single output that combines different sources of information. 
(60v) Furthermore, obligations placed on providers and deployers of certain AI systems in this  Regulation to enable the detection and disclosure that the outputs of those systems are  artificially generated or manipulated are particularly relevant to facilitate the effective  implementation of Regulation (EU) 2022/2065. This applies in particular as regards the  obligations of providers of very large online platforms or very large online search engines  to identify and mitigate systemic risks that may arise from the dissemination of content that  has been artificially generated or manipulated, in particular risk of the actual or foreseeable  negative effects on democratic processes, civic discourse and electoral processes, including  through disinformation. 
(61) Standardisation should play a key role to provide technical solutions to providers to ensure  compliance with this Regulation, in line with the state of the art, to promote innovation as  well as competitiveness and growth in the single market. Compliance with harmonised  standards as defined in Regulation (EU) No 1025/2012 of the European Parliament and of  the Council25, which are normally expected to reflect the state of the art, should be a means  for providers to demonstrate conformity with the requirements of this Regulation. A  balanced representation of interests involving all relevant stakeholders in the development  of standards, in particular SME’s, consumer organisations and environmental and social  stakeholders in accordance with Article 5 and 6 of Regulation 1025/2012 should therefore  be encouraged. In order to facilitate compliance, the standardisation requests should be  issued by the Commission without undue delay. When preparing the standardisation  
request, the Commission should consult the AI advisory Forum and the Board in order to    
25 Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on  European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC,  94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the  European Parliament and of the Council and repealing Council Decision 87/95/EEC and Decision No  1673/2006/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12).
 
collect relevant expertise. However, in the absence of relevant references to harmonised  standards, the Commission should be able to establish, via implementing acts, and after  consultation of the AI Advisory forum, common specifications for certain requirements  under this Regulation. The common specification should be an exceptional fall back  
solution to facilitate the provider’s obligation to comply with the requirements of this  Regulation, when the standardisation request has not been accepted by any of the European  standardisation organisations, or when the relevant harmonized standards insufficiently  address fundamental rights concerns, or when the harmonised standards do not comply  with the request, or when there are delays in the adoption of an appropriate harmonised  standard. If such delay in the adoption of a harmonised standard is due to the technical  complexity of the standard in question, this should be considered by the Commission  before contemplating the establishment of common specifications. When developing  common specifications, the Commission is encouraged to cooperate with international  partners and international standardisation bodies.  
(61a) It is appropriate that, without prejudice to the use of harmonised standards and common  specifications, providers of high-risk AI system that has been trained and tested on data  reflecting the specific geographical, behavioural, contextual or functional setting within  which the AI system is intended to be used, should be presumed to be in compliance with  
the respective measure provided for under the requirement on data governance set out in  this Regulation. Without prejudice to the requirements related to robustness and accuracy  set out in this Regulation, in line with Article 54(3) of Regulation (EU) 2019/881 of the  European Parliament and of the Council, high-risk AI systems that have been certified or  for which a statement of conformity has been issued under a cybersecurity scheme  pursuant to that Regulation and the references of which have been published in the Official  Journal of the European Union should be presumed to be in compliance with the  cybersecurity requirement of this Regulation in so far as the cybersecurity certificate or  statement of conformity or parts thereof cover the cybersecurity requirement of this  Regulation This remains without prejudice to the voluntary nature of that cybersecurity  scheme. 
(62) In order to ensure a high level of trustworthiness of high-risk AI systems, those systems  should be subject to a conformity assessment prior to their placing on the market or putting  into service.
 
(63) It is appropriate that, in order to minimise the burden on operators and avoid any possible  duplication, for high-risk AI systems related to products which are covered by existing  Union harmonisation legislation following the New Legislative Framework approach, the  compliance of those AI systems with the requirements of this Regulation should be  assessed as part of the conformity assessment already foreseen under that legislation. The  applicability of the requirements of this Regulation should thus not affect the specific  logic, methodology or general structure of conformity assessment under the relevant  specific New Legislative Framework legislation.  
(64) Given the complexity of high-risk AI systems and the risks that are associated to them, it is  important to develop an adequate system of conformity assessment procedure for high risk  AI systems involving notified bodies, so called third party conformity assessment.  However, given the current experience of professional pre-market certifiers in the field of  product safety and the different nature of risks involved, it is appropriate to limit, at least in  an initial phase of application of this Regulation, the scope of application of third-party  conformity assessment for high-risk AI systems other than those related to products.  Therefore, the conformity assessment of such systems should be carried out as a general  rule by the provider under its own responsibility, with the only exception of AI systems  intended to be used for biometrics. 
(65) In order to carry out third-party conformity assessments when so required, notified bodies  should be notified under this Regulation by the national competent authorities, provided  they are compliant with a set of requirements, notably on independence, competence,  absence of conflicts of interests and suitable cybersecurity requirements. Notification of  those bodies should be sent by national competent authorities to the Commission and the  other Member States by means of the electronic notification tool developed and managed  by the Commission pursuant to Article R23 of Decision 768/2008.  
(65a) In line with Union commitments under the World Trade Organization Agreement on  Technical Barriers to Trade, it is adequate to facilitate the mutual recognition of  conformity assessment results produced by competent conformity assessment bodies,  independent of the territory in which they are established, provided that those conformity  assessment bodies established under the law of a third country meet the applicable  requirements of the Regulation and the Union has concluded an agreement to that extent.  In this context, the Commission should actively explore possible international instruments