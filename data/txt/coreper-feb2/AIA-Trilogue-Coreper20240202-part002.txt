for that purpose and in particular pursue the conclusion of mutual recognition agreements  with third countries. 
(66) In line with the commonly established notion of substantial modification for products  regulated by Union harmonisation legislation, it is appropriate that whenever a change  occurs which may affect the compliance of a high risk AI system with this Regulation (e.g.  change of operating system or software architecture), or when the intended purpose of the  system changes, that AI system should be considered a new AI system which should  undergo a new conformity assessment. However, changes occurring to the algorithm and  the performance of AI systems which continue to ‘learn’ after being placed on the market  or put into service (i.e. automatically adapting how functions are carried out) should not  constitute a substantial modification, provided that those changes have been pre determined by the provider and assessed at the moment of the conformity assessment. 
(67) High-risk AI systems should bear the CE marking to indicate their conformity with this  Regulation so that they can move freely within the internal market. For high-risk AI  systems embedded in a product, a physical CE marking should be affixed, and may be  complemented by a digital CE marking. For high-risk AI systems only provided digitally, a  digital CE marking should be used. Member States should not create unjustified obstacles  to the placing on the market or putting into service of high-risk AI systems that comply  with the requirements laid down in this Regulation and bear the CE marking. 
(68) Under certain conditions, rapid availability of innovative technologies may be crucial for  health and safety of persons, the protection of the environment and climate change and for  society as a whole. It is thus appropriate that under exceptional reasons of public security  or protection of life and health of natural persons, environmental protection and the  protection of key industrial and infrastructural assets, market surveillance authorities could  authorise the placing on the market or putting into service of AI systems which have not  undergone a conformity assessment. In a duly justified situations as provided under this  regulations, law enforcement authorities or civil protection authorities may put a specific  high-risk AI system into service without the authorisation of the market surveillance  authority, provided that such authorisation is requested during or after the use without  undue delay.  
(69) In order to facilitate the work of the Commission and the Member States in the artificial  intelligence field as well as to increase the transparency towards the public, providers of 
 
high-risk AI systems other than those related to products falling within the scope of  relevant existing Union harmonisation legislation, as well as providers who consider that  an AI system referred to in annex III is by derogation not high-risk, should be required to  register themselves and information about their AI system in a EU database, to be  established and managed by the Commission. Before using a high-risk AI system listed in  Annex III, deployers of high-risk AI systems that are public authorities, agencies or bodies,  shall register themselves in such database and select the system that they envisage to use..  Other deployers should be entitled to do so voluntarily. This section of the database should  be publicly accessible, free of charge, the information should be easily navigable,  understandable and machine-readable. The database should also be user-friendly, for  example by providing search functionalities, including through keywords, allowing the  general public to find relevant information included in Annex VIII and on the areas of risk  under Annex III to which the high-risk AI systems correspond. Any substantial  modification of high-risk AI systems should also be registered in the EU database. For  high risk AI systems in the area of law enforcement, migration, asylum and border control  management, the registration obligations should be fulfilled in a secure non-public section  of the database. Access to the secure non-public section should be strictly limited to the  Commission as well as to market surveillance authorities with regard to their national  section of that database. High risk AI systems in the area of critical infrastructure should  only be registered at national level. The Commission should be the controller of the EU  database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and  of the Council26. In order to ensure the full functionality of the database, when deployed,  the procedure for setting the database should include the elaboration of functional  specifications by the Commission and an independent audit report. The Commission  should take into account cybersecurity and hazard-related risks when carrying out its tasks  as data controller on the EU database. In order to maximise the availability and use of the  database by the public, the database, including the information made available through it,  should comply with requirements under the Directive 2019/882.  
(70) Certain AI systems intended to interact with natural persons or to generate content may  pose specific risks of impersonation or deception irrespective of whether they qualify as  high-risk or not. In certain circumstances, the use of these systems should therefore be  
  
26 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection  of natural persons with regard to the processing of personal data and on the free movement of such data, and  repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1).
 
subject to specific transparency obligations without prejudice to the requirements and  obligations for high-risk AI systems and subject to targeted exceptions to take into account  the special need of law enforcement. In particular, natural persons should be notified that  they are interacting with an AI system, unless this is obvious from the point of view of a  natural person who is reasonably well-informed, observant and circumspect taking into  account the circumstances and the context of use. When implementing such obligation, the  characteristics of individuals belonging to vulnerable groups due to their age or disability  should be taken into account to the extent the AI system is intended to interact with those  groups as well. Moreover, natural persons should be notified when they are exposed to  systems that, by processing their biometric data, can identify or infer the emotions or  intentions of those persons or assign them to specific categories. Such specific categories  can relate to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic  origin, personal preferences and interests. Such information and notifications should be  provided in accessible formats for persons with disabilities.  
(70a) A variety of AI systems can generate large quantities of synthetic content that becomes  increasingly hard for humans to distinguish from human-generated and authentic content.  The wide availability and increasing capabilities of those systems have a significant impact  on the integrity and trust in the information ecosystem, raising new risks of misinformation  and manipulation at scale, fraud, impersonation and consumer deception. In the light of  those impacts, the fast technological pace and the need for new methods and techniques to  trace origin of information, it is appropriate to require providers of those systems to embed  technical solutions that enable marking in a machine readable format and detection that the  output has been generated or manipulated by an AI system and not a human. Such  techniques and methods should be sufficiently reliable, interoperable, effective and robust  as far as this is technically feasible, taking into account available techniques or a  combination of such techniques, such as watermarks, metadata identifications,  cryptographic methods for proving provenance and authenticity of content, logging  methods, fingerprints or other techniques, as may be appropriate. When implementing this  obligation, providers should also take into account the specificities and the limitations of  the different types of content and the relevant technological and market developments in  the field, as reflected in the generally acknowledged state-of-the-art. Such techniques and  methods can be implemented at the level of the system or at the level of the model,  including general-purpose AI models generating content, thereby facilitating fulfilment of  this obligation by the downstream provider of the AI system. To remain proportionate, it is 
 
appropriate to envisage that this marking obligation should not cover AI systems  performing primarily an assistive function for standard editing or AI systems not  substantially altering the input data provided by the deployer or the semantics thereof.  
(70b) Further to the technical solutions employed by the providers of the system, deployers , who  use an AI system to generate or manipulate image, audio or video content that appreciably  resembles existing persons, places or events and would falsely appear to a person to be  authentic (‘deep fakes’), should also clearly and distinguishably disclose that the content  has been artificially created or manipulated by labelling the artificial intelligence output  accordingly and disclosing its artificial origin The compliance with this transparency  obligation should not be interpreted as indicating that the use of the system or its output  impedes the right to freedom of expression and the right to freedom of the arts and sciences  guaranteed in the Charter of Fundamental Rights of the EU, in particular where the content  is part of an evidently creative, satirical, artistic or fictional work or programme, subject to  appropriate safeguards for the rights and freedoms of third parties. In those cases, the  transparency obligation for deep fakes set out in this Regulation is limited to disclosure of  the existence of such generated or manipulated content in an appropriate manner that does  not hamper the display or enjoyment of the work, including its normal exploitation and  use, while maintaining the utility and quality of the work. In addition, it is also appropriate  to envisage a similar disclosure obligation in relation to AI-generated or manipulated text  to the extent it is published with the purpose of informing the public on matters of public  interest unless the AI-generated content has undergone a process of human review or  editorial control and a natural or legal person holds editorial responsibility for the  publication of the content.  
(70c) To ensure consistent implementation, it is appropriate to empower the Commission to  adopt implementing acts on the application of the provisions on the labelling and detection  of artificially generated or manipulated content. Without prejudice to the mandatory nature  and full applicability of these obligations, the Commission may also encourage and  facilitate the drawing up of codes of practice at Union level to facilitate the effective  implementation of the obligations regarding the detection and labelling of artificially  generated or manipulated content, including to support practical arrangements for making,  as appropriate, the detection mechanisms accessible and facilitating cooperation with other  actors in the value chain, disseminating content or checking its authenticity and  provenance to enable the public to effectively distinguish AI-generated content. 
 
(70d) The obligations placed on providers and deployers of certain AI systems in this Regulation  to enable the detection and disclosure that the outputs of those systems are artificially  generated or manipulated are particularly relevant to facilitate the effective implementation  of Regulation (EU) 2022/2065. This applies in particular as regards the obligations of  providers of very large online platforms or very large online search engines to identify and  mitigate systemic risks that may arise from the dissemination of content that has been  artificially generated or manipulated, in particular risk of the actual or foreseeable negative  effects on democratic processes, civic discourse and electoral processes, including through  disinformation. The requirement to label content generated by AI systems under this  Regulation is without prejudice to the obligation in Article 16(6) of Regulation 2022/2065  for providers of hosting services to process notices on illegal content received pursuant to  Article 16(1) and should not influence the assessment and the decision on the illegality of  the specific content. That assessment should be performed solely with reference to the  rules governing the legality of the content. 
(70e) The compliance with the transparency obligations for the AI systems coved by this  Regulation should not be interpreted as indicating that the use of the system or its output is  lawful under this Regulation or other Union and Member State law and should be without  prejudice to other transparency obligations for deployers of AI systems laid down in Union  or national law. 
(71) Artificial intelligence is a rapidly developing family of technologies that requires  regulatory oversight and a safe and controlled space for experimentation, while ensuring  responsible innovation and integration of appropriate safeguards and risk mitigation  measures. To ensure a legal framework that promotes innovation, is future-proof and  resilient to disruption, Member States should ensure that their national competent  authorities establish at least one artificial intelligence regulatory sandbox at national level  to facilitate the development and testing of innovative AI systems under strict regulatory  oversight before these systems are placed on the market or otherwise put into service.  Member States could also fulfil this obligation through participating in already existing  regulatory sandboxes or establishing jointly a sandbox with one or several Member States’  competent authorities, insofar as this participation provides equivalent level of national  coverage for the participating Member States. Regulatory sandboxes could be established  in physical, digital or hybrid form and may accommodate physical as well as digital 
 
products. Establishing authorities should also ensure that the regulatory sandboxes have the  adequate resources for their functioning, including financial and human resources. 
(72) The objectives of the AI regulatory sandboxes should be to foster AI innovation by  establishing a controlled experimentation and testing environment in the development and  pre-marketing phase with a view to ensuring compliance of the innovative AI systems with  this Regulation and other relevant Union and Member States legislation, to enhance legal  certainty for innovators and the competent authorities’ oversight and understanding of the  opportunities, emerging risks and the impacts of AI use, to facilitate regulatory learning for  authorities and companies, including with a view to future adaptions of the legal  framework, to support cooperation and the sharing of best practices with the authorities  involved in the AI regulatory sandbox, and to accelerate access to markets, including by  removing barriers for small and medium enterprises (SMEs), including start-ups.  Regulatory sandboxes should be widely available throughout the Union, and particular  attention should be given to their accessibility for SMEs, including startups. The  participation in the AI regulatory sandbox should focus on issues that raise legal  uncertainty for providers and prospective providers to innovate, experiment with AI in the  Union and contribute to evidence-based regulatory learning. The supervision of the AI  systems in the AI regulatory sandbox should therefore cover their development, training,  testing and validation before the systems are placed on the market or put into service, as  well as the notion and occurrence of substantial modification that may require a new  conformity assessment procedure. Any significant risks identified during the development  and testing of such AI systems should result in adequate mitigation and, failing that, in the  suspension of the development and testing process Where appropriate, national competent  authorities establishing AI regulatory sandboxes should cooperate with other relevant  authorities, including those supervising the protection of fundamental rights,, and could  allow for the involvement of other actors within the AI ecosystem such as national or  European standardisation organisations, notified bodies, testing and experimentation  facilities, research and experimentation labs, European Digital innovation hubs and  relevant stakeholder and civil society organisations. To ensure uniform implementation  across the Union and economies of scale, it is appropriate to establish common rules for  the regulatory sandboxes’ implementation and a framework for cooperation between the  relevant authorities involved in the supervision of the sandboxes. AI regulatory sandboxes  established under this Regulation should be without prejudice to other legislation allowing  for the establishment of other sandboxes aiming at ensuring compliance with legislation 
 
other that this Regulation. Where appropriate, relevant competent authorities in charge of  those other regulatory sandboxes should consider the benefits of using those sandboxes  also for the purpose of ensuring compliance of AI systems with this Regulation. Upon  agreement between the national competent authorities and the participants in the AI  regulatory sandbox, testing in real world conditions may also be operated and supervised in  the framework of the AI regulatory sandbox. 
(72a) This Regulation should provide the legal basis for the providers and prospective providers  in the AI regulatory sandbox to use personal data collected for other purposes for  developing certain AI systems in the public interest within the AI regulatory sandbox, only  under specified conditions, in line with Article 6(4) and 9(2)(g) of Regulation (EU)  2016/679, and Article 5, 6 and 10 of Regulation (EU) 2018/1725, and without prejudice to  Articles 4(2) and 10 of Directive (EU) 2016/680. All other obligations of data controllers  and rights of data subjects under Regulation (EU) 2016/679, Regulation (EU) 2018/1725  and Directive (EU) 2016/680 remain applicable. In particular, this Regulation should not  provide a legal basis in the meaning of Article 22(2)(b) of Regulation (EU) 2016/679 and  Article 24(2)(b) of Regulation (EU) 2018/1725. Providers and prospective providers in the  sandbox should ensure appropriate safeguards and cooperate with the competent  authorities, including by following their guidance and acting expeditiously and in good  faith to adequately mitigate any identified - significant risks to safety, health, and  fundamental rights that may arise during the development, testing and experimentation in  the sandbox.  
(72b) In order to accelerate the process of development and placing on the market of high-risk AI  systems listed in Annex III, it is important that providers or prospective providers of such  systems may also benefit from a specific regime for testing those systems in real world  conditions, without participating in an AI regulatory sandbox. However, in such cases and  taking into account the possible consequences of such testing on individuals, it should be  ensured that appropriate and sufficient guarantees and conditions are introduced by the  Regulation for providers or prospective providers. Such guarantees should include, among  others, requesting informed consent of natural persons to participate in testing in real world  conditions, with the exception of law enforcement in cases where the seeking of informed  consent would prevent the AI system from being tested. Consent of subjects to participate  in such testing under this Regulation is distinct from and without prejudice to consent of  data subjects for the processing of their personal data under the relevant data protection 
 
law. It is also important to minimise the risks and enable oversight by competent  authorities and therefore require prospective providers to have a real-world testing plan  submitted to competent market surveillance authority, register the testing in dedicated  sections in the EU-wide database subject to some limited exceptions, set limitations on the  period for which the testing can be done and require additional safeguards for persons  belonging to certain vulnerable groups as well as a written agreement defining the roles  and responsibilities of prospective providers and deployers and effective oversight by  competent personnel involved in the real world testing. Furthermore, it is appropriate to  envisage additional safeguards to ensure that the predictions, recommendations or  decisions of the AI system can be effectively reversed and disregarded and that personal  data is protected and is deleted when the subjects have withdrawn their consent to  participate in the testing without prejudice to their rights as data subjects under the EU data  protection law. As regards transfer of data, it is also appropriate to envisage that data  collected and processed for the purpose of the testing in real world conditions should only  be transferred to third countries outside the Union provided appropriate and applicable  safeguards under Union law are implemented, notably in accordance with bases for  transfer of personal data under Union law on data protection, while for non-personal data  appropriate safeguards are put in place in accordance with Union law, such as the Data  Governance Act and the Data Act. 
(72c) To ensure that Artificial Intelligence leads to socially and environmentally beneficial  outcomes, Member States are encouraged to support and promote research and  development of AI solutions in support of socially and environmentally beneficial  outcomes, such as AI-based solutions to increase accessibility for persons with disabilities,  tackle socio-economic inequalities, or meet environmental targets, by allocating sufficient  resources, including public and Union funding, and, where appropriate and provided that  the eligibility and selection criteria are fulfilled, considering in particular projects which  pursue such objectives. Such projects should be based on the principle of interdisciplinary  cooperation between AI developers, experts on inequality and non- discrimination,  accessibility, consumer, environmental, and digital rights, as well as academics. 
(73) In order to promote and protect innovation, it is important that the interests of SMEs,  including start-ups, that are providers or deployers of AI systems are taken into particular  account. To this objective, Member States should develop initiatives, which are targeted at  those operators, including on, awareness raising and information communication. Member 
 
States shall provide SME’s, including start-ups, having a registered office or a branch in  the Union, with priority access to the AI regulatory sandboxes provided that they fulfil the eligibility conditions and selection criteria and without precluding other providers and  prospective providers to access the sandboxes provided the same conditions and criteria are  fulfilled. Member States shall utilise existing channels and where appropriate, establish  new dedicated channels for communication with SMEs, start-ups, deployers other  innovators and, as appropriate, local public authorities, to support SMEs throughout their  development path by providing guidance and responding to queries about the  implementation of this Regulation. Where appropriate, these channels shall work together  to create synergies and ensure homogeneity in their guidance to SMEs including start-ups  and deployers. Additionally, Member States should facilitate the participation of SMEs and  other relevant stakeholders in the standardisation development processes. Moreover, the  specific interests and needs of SMEs including start-up providers should be taken into  account when Notified Bodies set conformity assessment fees. The Commission should  regularly assess the certification and compliance costs for SMEs including start-ups,  through transparent consultations deployers and should work with Member States to lower  such costs. For example, translation costs related to mandatory documentation and  communication with authorities may constitute a significant cost for providers and other  operators, notably those of a smaller scale. Member States should possibly ensure that one  of the languages determined and accepted by them for relevant providers’ documentation  and for communication with operators is one which is broadly understood by the largest  possible number of cross-border deployers. In order to address the specific needs of SMEs  including start-ups, the Commission should provide standardised templates for the areas  covered by this Regulation upon request of the AI Board. Additionally, the Commission  should complement Member States’ efforts by providing a single information platform  with easy-to-use information with regards to this Regulation for all providers and  deployers, by organising appropriate communication campaigns to raise awareness about  the obligations arising from this Regulation, and by evaluating and promoting the  convergence of best practices in public procurement procedures in relation to AI systems.  Medium-sized enterprises which recently changed from the small to medium-size category  within the meaning of the Annex to Recommendation 2003/361/EC (Article 16) should  have access to these support measures, as these new medium-sized enterprises may  sometimes lack the legal resources and training necessary to ensure proper understanding  and compliance with provisions.
 
(73a) In order to promote and protect innovation, the AI-on demand platform, all relevant EU  funding programmes and projects, such as Digital Europe Programme, Horizon Europe,  implemented by the Commission and the Member States at national or Union level should,  as appropriate, contribute to the achievement of the objectives of this Regulation. 
(74) In particular, in order to minimise the risks to implementation resulting from lack of  knowledge and expertise in the market as well as to facilitate compliance of providers,  notably SMEs, including start-ups, and notified bodies with their obligations under this  Regulation, the AI-on demand platform, the European Digital Innovation Hubs and the  Testing and Experimentation Facilities established by the Commission and the Member  States at national or EU level should contribute to the implementation of this Regulation.  Within their respective mission and fields of competence, they may provide in particular  technical and scientific support to providers and notified bodies. 
(74a) Moreover, in order to ensure proportionality considering the very small size of some  operators regarding costs of innovation, it is appropriate to allow microenterprises to fulfil  one of the most costly obligations, namely to establish a quality management system, in a  simplified manner which would reduce the administrative burden and the costs for those  enterprises without affecting the level of protection and the need for compliance with the  requirements for high-risk AI systems. The Commission should develop guidelines to  specify the elements of the quality management system to be fulfilled in this simplified  manner by microenterprises. 
(75) It is appropriate that the Commission facilitates, to the extent possible, access to Testing  and Experimentation Facilities to bodies, groups or laboratories established or accredited  pursuant to any relevant Union harmonisation legislation and which fulfil tasks in the  context of conformity assessment of products or devices covered by that Union  harmonisation legislation. This is notably the case for expert panels, expert laboratories  and reference laboratories in the field of medical devices pursuant to Regulation (EU)  2017/745 and Regulation (EU) 2017/746. 
(75a) This Regulation should establish a governance framework that both allows to coordinate  and support the application of this Regulation at national level, as well as build capabilities  at Union level and integrate stakeholders in the field of artificial intelligence. The effective  implementation and enforcement of this Regulation require a governance framework that  allows to coordinate and build up central expertise at Union level. The Commission has 
 
established the AI Office by Commission decision of […], which has as its mission to  develop Union expertise and capabilities in the field of artificial intelligence and to  contribute to the implementation of Union legislation on artificial intelligence. Member  States should facilitate the tasks of the AI Office with a view to support the development of  Union expertise and capabilities at Union level and to strengthen the functioning of the  digital single market. Furthermore, a European Artificial Intelligence Board composed of  representatives of the Member States, a scientific panel to integrate the scientific  community and an advisory forum to contribute stakeholder input to the implementation of  this Regulation, both at national and Union level, should be established. The development  of Union expertise and capabilities should also include making use of existing resources  and expertise, notably through synergies with structures built up in the context of the  Union level enforcement of other legislation and synergies with related initiatives at Union  level, such as the EuroHPC Joint Undertaking and the AI Testing and Experimentation  Facilities under the Digital Europe Programme. 
(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation  a European Artificial Intelligence Board should be established. The Board should reflect  the various interests of the AI eco-system and be composed of representatives of the  Member States. The Board should be responsible for a number of advisory tasks, including  issuing opinions, recommendations, advice or contributing to guidance on matters related  to the implementation of this Regulation, including on enforcement matters, technical  specifications or existing standards regarding the requirements established in this  Regulation and providing advice to the Commission and the Member States and their  national competent authorities on specific questions related to artificial intelligence. In  order to give some flexibility to Member States in the designation of their representatives  in the AI Board, such representatives may be any persons belonging to public entities who  should have the relevant competences and powers to facilitate coordination at national  level and contribute to the achievement of the Board's tasks. The Board should establish  two standing sub-groups to provide a platform for cooperation and exchange among  market surveillance authorities and notifying authorities on issues related respectively to  market surveillance and notified bodies. The standing subgroup for market surveillance  should act as the Administrative Cooperation Group (ADCO) for this Regulation in the  meaning of Article 30 of Regulation (EU) 2019/1020. In line with the role and tasks of the  Commission pursuant to Article 33 of Regulation (EU) 2019/1020, the Commission should  support the activities of the standing subgroup for market surveillance by undertaking 
 
market evaluations or studies, notably with a view to identifying aspects of this Regulation  requiring specific and urgent coordination among market surveillance authorities. The  Board may establish other standing or temporary sub-groups as appropriate for the purpose  of examining specific issues. The Board should also cooperate, as appropriate, with  relevant EU bodies, expert groups and networks active in the context of relevant EU  legislation, including in particular those active under relevant EU regulation on data,  digital products and services. 
(76x) With a view to ensure the involvement of stakeholders in the implementation and  application of this Regulation, an advisory forum should be established to advise and  provide technical expertise to the Board and the Commission. To ensure a varied and  balanced stakeholder representation between commercial and non-commercial interest and,  within the category of commercial interests, with regards to SMEs and other undertakings,  the advisory forum should comprise inter alia industry, start-ups, SMEs, academia, civil  society, including social partners, as well as the Fundamental Rights Agency, European  Union Agency for Cybersecurity, the European Committee for Standardization (CEN), the  European Committee for Electrotechnical Standardization (CENELEC) and the European  Telecommunications Standards Institute (ETSI). 
(76y) To support the implementation and enforcement of this Regulation, in particular the  monitoring activities of the AI Office as regards general-purpose AI models, a scientific  panel of independent experts should be established. The independent experts constituting  the scientific panel should be selected on the basis of up-to-date scientific or technical  expertise in the field of artificial intelligence and should perform their tasks with  impartiality, objectivity and ensure the confidentiality of information and data obtained in  carrying out their tasks and activities. To allow reinforcing national capacities necessary  for the effective enforcement of this Regulation, Member States should be able to request  support from the pool of experts constituting the scientific panel for their enforcement  activities. 
(76a) In order to support adequate enforcement as regards AI systems and reinforce the  capacities of the Member States, EU AI testing support structures should be established  and made available to the Member States. 
(77) Member States hold a key role in the application and enforcement of this Regulation. In  this respect, each Member State should designate at least one notifying authority and at 
 
least one market surveillance authority as national competent authorities for the purpose of  supervising the application and implementation of this Regulation. Member States may  decide to appoint any kind of public entity to perform the tasks of the national competent  authorities within the meaning of this Regulation, in accordance with their specific national  organisational characteristics and needs. In order to increase organisation efficiency on the  side of Member States and to set a single point of contact vis-à-vis the public and other  counterparts at Member State and Union levels, each Member State should designate a  market surveillance authority to act as single point of contact. 
(77a) The national competent authorities should exercise their powers independently, impartially  and without bias, so as to safeguard the principles of objectivity of their activities and tasks  and to ensure the application and implementation of this Regulation. The members of these  authorities should refrain from any action incompatible with their duties and should be  
subject to confidentiality rules under this Regulation.  
(78) In order to ensure that providers of high-risk AI systems can take into account the  experience on the use of high-risk AI systems for improving their systems and the design  and development process or can take any possible corrective action in a timely manner, all  providers should have a post-market monitoring system in place. Where relevant, post market monitoring should include an analysis of the interaction with other AI systems  including other devices and software. Post-market monitoring should not cover sensitive  operational data of deployers which are law enforcement authorities. This system is also  key to ensure that the possible risks emerging from AI systems which continue to ‘learn’  after being placed on the market or put into service can be more efficiently and timely  addressed. In this context, providers should also be required to have a system in place to  report to the relevant authorities any serious incidents resulting from the use of their AI  systems, meaning incident or malfunctioning leading to death or serious damage to health,  serious and irreversible disruption of the management and operation of critical  infrastructure, breaches of obligations under Union law intended to protect fundamental  rights or serious damage to property or the environment.  
(79) In order to ensure an appropriate and effective enforcement of the requirements and  obligations set out by this Regulation, which is Union harmonisation legislation, the  system of market surveillance and compliance of products established by Regulation (EU)  2019/1020 should apply in its entirety. Market surveillance authorities designated pursuant 
 
to this Regulation should have all enforcement powers under this Regulation and  Regulation (EU) 2019/1020 and should exercise their powers and carry out their duties  independently, impartially and without bias. Although the majority of AI systems are not  subject to specific requirements and obligations under this Regulation, market surveillance  authorities may take measures in relation to all AI systems when they present a risk in  accordance with this Regulation. Due to the specific nature of Union institutions, agencies  and bodies falling within the scope of this Regulation, it is appropriate to designate the  European Data Protection Supervisor as a competent market surveillance authority for  them. This should be without prejudice to the designation of national competent authorities  by the Member States. Market surveillance activities should not affect the ability of the  supervised entities to carry out their tasks independently, when such independence is  required by Union law. 
(79a) This Regulation is without prejudice to the competences, tasks, powers and independence  of relevant national public authorities or bodies which supervise the application of Union  law protecting fundamental rights, including equality bodies and data protection  authorities. Where necessary for their mandate, those national public authorities or bodies  should also have access to any documentation created under this Regulation. A specific  safeguard procedure should be set for ensuring adequate and timely enforcement against  AI systems presenting a risk to health, safety and fundamental rights. The procedure for  such AI systems presenting a risk should be applied to high-risk AI systems presenting a  risk, prohibited systems which have been placed on the market, put into service or used in  violation of the prohibited practices laid down in this Regulation and AI systems which  have been made available in violation of the transparency requirements laid down in this  Regulation and present a risk. 
(80) Union legislation on financial services includes internal governance and risk management  rules and requirements which are applicable to regulated financial institutions in the course  of provision of those services, including when they make use of AI systems. In order to  ensure coherent application and enforcement of the obligations under this Regulation and  relevant rules and requirements of the Union financial services legislation, the competent  authorities for the supervision and enforcement of the financial services legislation, notably  competent authorities as defined in Directive 2009/138/EC, Directive (EU) 2016/97,  Directive 2013/36/EU Regulation (EU) No 575/2013, Directive 2008/48/EC and Directive  2014/17/EU of the European Parliament and of the Council, should be designated, within 
 
their respective competences, as competent authorities for the purpose of supervising the  implementation of this Regulation, including for market surveillance activities, as regards  AI systems provided or used by regulated and supervised financial institutions unless  Member States decide to designate another authority to fulfil these market surveillance  tasks. Those competent authorities should have all powers under this Regulation and  Regulation (EU) 2019/1020 on market surveillance to enforce the requirements and  obligations of this Regulation, including powers to carry our ex post market surveillance  activities that can be integrated, as appropriate, into their existing supervisory mechanisms  and procedures under the relevant Union financial services legislation. It is appropriate to  envisage that, when acting as market surveillance authorities under this Regulation, the  national authorities responsible for the supervision of credit institutions regulated under  Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism  (SSM) established by Council Regulation No 1024/2013, should report, without delay, to  the European Central Bank any information identified in the course of their market  surveillance activities that may be of potential interest for the European Central Bank’s  prudential supervisory tasks as specified in that Regulation. To further enhance the  consistency between this Regulation and the rules applicable to credit institutions regulated  under Directive 2013/36/EU of the European Parliament and of the Council27, it is also  appropriate to integrate some of the providers’ procedural obligations in relation to risk  management, post marketing monitoring and documentation into the existing obligations  and procedures under Directive 2013/36/EU. In order to avoid overlaps, limited  derogations should also be envisaged in relation to the quality management system of  providers and the monitoring obligation placed on deployers of high-risk AI systems to the  extent that these apply to credit institutions regulated by Directive 2013/36/EU. The same  regime should apply to insurance and re-insurance undertakings and insurance holding  companies under Directive 2009/138/EU (Solvency II) and the insurance intermediaries  under Directive 2016/97/EU and other types of financial institutions subject to  requirements regarding internal governance, arrangements or processes established  pursuant to the relevant Union financial services legislation to ensure consistency and  equal treatment in the financial sector. 
  
27 Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity  of credit institutions and the prudential supervision of credit institutions and investment firms, amending  Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, 27.6.2013, p. 338).
 
(80-x) Each market surveillance authority for high-risk AI systems listed in point 1 of Annex III  insofar as these systems are used for law enforcement purposes and for purposes listed in  points 6, 7 and 8 of Annex III should have effective investigative and corrective powers,  including at least the power to obtain access to all personal data that are being processed  and to all information necessary for the performance of its tasks. The market surveillance  authorities should be able to exercise their powers by acting with complete independence.  Any limitations of their access to sensitive operational data under this Regulation should  be without prejudice to the powers conferred to them by Directive 2016/680. No exclusion  on disclosing data to national data protection authorities under this Regulation should  affect the current or future powers of those authorities beyond the scope of this Regulation. 
(80x) The market surveillance authorities of the Member States and the Commission should be  able to propose joint activities, including joint investigations, to be conducted by market  surveillance authorities or market surveillance authorities jointly with the Commission,  that have the aim of promoting compliance, identifying non-compliance, raising awareness  and providing guidance in relation to this Regulation with respect to specific categories of  high-risk AI systems that are found to present a serious risk across several Member States.  Joint activities to promote compliance should be carried out in accordance with Article 9 of  the 2019/1020. The AI Office should provide coordination support for joint investigations.  
(80y) It is necessary to clarify the responsibilities and competences on national and Union level  as regards AI systems that are built on general-purpose AI models. To avoid overlapping  competences, where an AI system is based on a general-purpose AI model and the model  and system are provided by the same provider, the supervision should take place at Union  level through the AI Office, which should have the powers of a market surveillance  authority within the meaning of Regulation (EU) 2019/1020 for this purpose. In all other  cases, national market surveillance authorities remain responsible for the supervision of AI  systems. However, for general-purpose AI systems that can be used directly by deployers  for at least one purpose that is classified as high-risk, market surveillance authorities  should cooperate with the AI Office to carry out evaluations of compliance and inform the  Board and other market surveillance authorities accordingly. Furthermore, market  surveillance authorities should be able to request assistance from the AI Office where the  market surveillance authority is unable to conclude an investigation on a high-risk AI  system because of its inability to access certain information related to the general-purpose  AI model on which the high-risk AI system is built. In such cases, the procedure regarding 
 
mutual assistance in cross-border cases in Chapter VI of Regulation (EU) 2019/1020  should apply by analogy. 
(80z) To make best use of the centralised Union expertise and synergies at Union level, the  powers of supervision and enforcement of the obligations on providers of general-purpose  AI models should be a competence of the Commission. The Commission should entrust the  implementation of these tasks to the AI Office, without prejudice to the powers of  organisation of the Commission and the division of competences between member States  and the Union based on the Treaties. The AI Office should be able to carry out all  necessary actions to monitor the effective implementation of this Regulation as regards  general-purpose AI models. It should be able to investigate possible infringements of the  rules on providers of general-purpose AI models both on its own initiative, following the  results of its monitoring activities, or upon request from market surveillance authorities in  line with the conditions set out in this Regulation. To support effective monitoring of the  AI Office, it should provide for the possibility that downstream providers lodge complaints  about possible infringements of the rules on providers of general purpose AI models. 
(80z+1) With a view to complement the governance systems for general-purpose AI models, the  scientific panel should support the monitoring activities of the AI Office and may, in  certain cases, provide qualified alerts to the AI Office which trigger follow-ups such as  investigations. This should be the case where the scientific panel has reason to suspect that  a general-purpose AI model poses a concrete and identifiable risk at Union level.  Furthermore, this should be the case where the scientific panel has reason to suspect that a  general-purpose AI model meets the criteria that would lead to a classification as general purpose AI model with systemic risk. To equip the scientific panel with the information  necessary for the performance of these tasks, there should be a mechanism whereby the  scientific panel can request the Commission to require documentation or information from  a provider.  
(80z+2) The AI Office should be able to take the necessary actions to monitor the effective  implementation of and compliance with the obligations for providers of general purpose AI  models laid down in this Regulation. The AI Office should be able to investigate possible  infringements in accordance with the powers provided for in this Regulation, including by  requesting documentation and information, by conducting evaluations, as well as by  requesting measures from providers of general purpose AI models. In the conduct of 
 
evaluations, in order to make use of independent expertise, the AI Office should be able to  involve independent experts to carry out the evaluations on its behalf. Compliance with the  obligations should be enforceable, inter alia, through requests to take appropriate measures,  including risk mitigation measures in case of identified systemic risks as well as restricting  the making available on the market, withdrawing or recalling the model. As a safeguard in  
case needed beyond the procedural rights provided for in this Regulation, providers of  general-purpose AI models should have the procedural rights provided for in Article 18 of  Regulation (EU) 2019/1020, which should apply by analogy, without prejudice to more  specific procedural rights provided for by this Regulation.  
(81) The development of AI systems other than high-risk AI systems in accordance with the  requirements of this Regulation may lead to a larger uptake of ethical and trustworthy  artificial intelligence in the Union. Providers of non-high-risk AI systems should be  encouraged to create codes of conduct, including related governance mechanisms, intended  to foster the voluntary application of some or all of the mandatory requirements applicable  to high-risk AI systems, adapted in light of the intended purpose of the systems and the  lower risk involved and taking into account the available technical solutions and industry  best practices such as model and data cards. Providers and, as appropriate, deployers of all  AI systems, high-risk or not, and models should also be encouraged to apply on a  voluntary basis additional requirements related, for example, to the elements of the  European ethic guidelines for trustworthy AI, environmental sustainability, AI literacy  measures, inclusive and diverse design and development of AI systems, including attention  to vulnerable persons and accessibility to persons with disability, stakeholders’  participation with the involvement as appropriate, of relevant stakeholders such as business  and civil society organisations, academia and research organisations, trade unions and  consumer protection organisation in the design and development of AI systems, and  diversity of the development teams, including gender balance. To ensure that the voluntary  codes of conduct are effective, they should be based on clear objectives and key  performance indicators to measure the achievement of those objectives. They should be  also developed in an inclusive way, as appropriate, with the involvement of relevant  stakeholders such as business and civil society organisations, academia and research  organisations, trade unions and consumer protection organisation. The Commission may  develop initiatives, including of a sectorial nature, to facilitate the lowering of technical  barriers hindering cross-border exchange of data for AI development, including on data  access infrastructure, semantic and technical interoperability of different types of data.
 
(82) It is important that AI systems related to products that are not high-risk in accordance with  this Regulation and thus are not required to comply with the requirements set out for high risk AI systems are nevertheless safe when placed on the market or put into service. To  contribute to this objective, Regulation (EU) 2023/988 of the European Parliament and of  the Council28 would apply as a safety net. 
(83) In order to ensure trustful and constructive cooperation of competent authorities on Union  and national level, all parties involved in the application of this Regulation should respect  the confidentiality of information and data obtained in carrying out their tasks, in  accordance with Union or national law. They should carry out their tasks and activities in  such a manner as to protect, in particular, intellectual property rights, confidential business information and trade secrets, the effective implementation of this Regulation, public and  national security interests, the integrity of criminal or administrative proceedings, and the  integrity of classified information. 
(84) Compliance with this Regulation should be enforceable by means of the imposition of  penalties and other enforcement measures. Member States should take all necessary  measures to ensure that the provisions of this Regulation are implemented, including by  laying down effective, proportionate and dissuasive penalties for their infringement, and in  respect of the ne bis in idem principle. In order to strengthen and harmonise administrative  penalties for infringement of this Regulation, the upper limits for setting the administrative  fines for certain specific infringements should be laid down. When assessing the amount of  the fines, Member States should, in each individual case, take into account all relevant  circumstances of the specific situation, with due regard in particular to the nature, gravity  and duration of the infringement and of its consequences and to the provider’s size, in  particular if the provider is an SME including a start-up. The European Data Protection  Supervisor should have the power to impose fines on Union institutions, agencies and  bodies falling within the scope of this Regulation. 
(84a) Compliance with the obligations on providers of general-purpose AI models imposed  under this Regulation should be enforceable among others by means of fines. To that end,  appropriate levels of fines should also be laid down for infringement of those obligations,  
  
28 Regulation (EU) 2023/988 of the European Parliament and of the Council of of 10 May 2023 on general  product safety, amending Regulation (EU) No 1025/2012 of the European Parliament and of the Council and  Directive (EU) 2020/1828 of the European Parliament and the Council, and repealing Directive 2001/95/EC of  the European Parliament and of the Council and Council Directive 87/357/EEC (Text with EEA relevance) (OJ  L 135, 23.5.2023, p. 1–51).
 
including the failure to comply with measures requested by the Commission in accordance  with this Regulation, subject to appropriate limitation periods in accordance with the  principle of proportionality. All decisions taken by the Commission under this Regulation  are subject to review by the Court of Justice of the European Union in accordance with the  TFEU. 
(84aa) Union and national law already provides effective remedies to natural and legal persons  whose rights and freedoms are adversely affected by the use of AI systems. Without  prejudice to those remedies, any natural or legal person having grounds to consider that  there has been an infringement of the provisions of this Regulation should be entitled to  lodge a complaint to the relevant market surveillance authority or the AI Office where  applicable. 
(84b) Affected persons should have the right to request an explanation when a decision is taken  by the deployer with the output from certain high-risk systems as provided for in this  Regulation as the main basis and which produces legal effects or similarly significantly  affects him or her in a way that they consider to adversely impact their health, safety or  fundamental rights. This explanation should be a clear and meaningful and should provide  a basis for affected persons to exercise their rights. This should not apply to the use of AI  systems for which exceptions or restrictions follow from Union or national law and should  apply only to the extent this right is not already provided for under Union legislation. 
(84c) Persons acting as ‘whistle-blowers’ on the breaches of this Regulation should be afforded  the protection guaranteed by Union legislation on the protection of persons who report  breaches of law. Therefore, Directive (EU) 2019/1937 should apply to the reporting of  breaches of this Regulation and the protection of persons reporting such breaches. 
(85) In order to ensure that the regulatory framework can be adapted where necessary, the  power to adopt acts in accordance with Article 290 TFEU should be delegated to the  Commission to amend the Union harmonisation legislation listed in Annex II, the high-risk  AI systems listed in Annex III, the provisions regarding technical documentation listed in  Annex IV, the content of the EU declaration of conformity in Annex V, the provisions  regarding the conformity assessment procedures in Annex VI and VII, the provisions  establishing the high-risk AI systems to which the conformity assessment procedure based  on assessment of the quality management system and assessment of the technical  documentation should apply, the threshold as well as to supplement benchmarks and 
 
indicators in the rules for classification of general-purpose AI models with systemic risk,  the criteria for the designation of general-purpose AI models with systemic risk in Annex  IXc, the technical documentation for providers of general-purpose AI models in Annex  VIIIb and the transparency information for providers of general-purpose AI models in  Annex VIIIc. It is of particular importance that the Commission carry out appropriate  consultations during its preparatory work, including at expert level, and that those  consultations be conducted in accordance with the principles laid down in the  Interinstitutional Agreement of 13 April 2016 on Better Law-Making1. In particular, to  ensure equal participation in the preparation of delegated acts, the European Parliament  and the Council receive all documents at the same time as Member States’ experts, and  their experts systematically have access to meetings of Commission expert groups dealing  with the preparation of delegated acts. 
(85a) Given the rapid technological developments and the required technical expertise in the  effective application of this Regulation, the Commission should evaluate and review this  Regulation by three years after the date of entry into application and every four years  thereafter and report to the European Parliament and the Council. In addition, taking into  account the implications for the scope of this Regulation, the Commission should carry out  an assessment of the need to amend the list in Annex III and the list of prohibited practices  once a year. Moreover, by two years after entry into application and every four years  thereafter, the Commission should evaluate and report to the European Parliament and to  the Council on the need to amend the high-risk areas in Annex III, the AI systems within  the scope of the transparency obligations in Title IV, the effectiveness of the supervision  and governance system and the progress on the development of standardisation  deliverables on energy efficient development of general-purpose AI models, including the  need for further measures or actions. Finally, within two years after the entry into  application and every three years thereafter, the Commission should evaluate the impact  and effectiveness of voluntary codes of conducts to foster the application of the  requirements set out in Title III, Chapter 2, for systems other than high-risk AI systems and  possibly other additional requirements for such AI systems. 
(86) In order to ensure uniform conditions for the implementation of this Regulation,  implementing powers should be conferred on the Commission. Those powers should be  exercised in accordance with Regulation (EU) No 182/2011 of the European Parliament  and of the Council1.
 
(87) Since the objective of this Regulation cannot be sufficiently achieved by the Member  States and can rather, by reason of the scale or effects of the action, be better achieved at  Union level, the Union may adopt measures in accordance with the principle of  subsidiarity as set out in Article 5 TEU. In accordance with the principle of proportionality  as set out in that Article, this Regulation does not go beyond what is necessary in order to  achieve that objective. 
(87a) In order to ensure legal certainty, ensure an appropriate adaptation period for operators and  avoid disruption to the market, including by ensuring continuity of the use of AI systems, it  is appropriate that this Regulation applies to the high-risk AI systems that have been placed  on the market or put into service before the general date of application thereof, only if,  
from that date, those systems are subject to significant changes in their design or intended  purpose. It is appropriate to clarify that, in this respect, the concept of significant change  should be understood as equivalent in substance to the notion of substantial modification,  which is used with regard only to high-risk AI systems as defined in this Regulation. By  way of exception and in light of public accountability, operators of AI systems which are  components of the large-scale IT systems established by the legal acts listed in Annex IX  and operators of high-risk AI systems that are intended to be used by public authorities  should take the necessary steps to comply with the requirements of this Regulation by end  of 2030 and by four years after the entry into application respectively. 
(87b) Providers of high-risk AI systems are encouraged to start to comply, on voluntary basis,  with the relevant obligations foreseen under this Regulation already during the transitional  period. 
(88) This Regulation should apply from … [OP – please insert the date established in Art. 85].  However, taking into account the unacceptable risk associated with the use of AI in certain  ways, the prohibitions should apply already from … [OP – please insert the date – 6  months after entry into force of this Regulation]. While the full effect of these prohibitions  follows with the establishment of the governance and enforcement of this Regulation,  anticipating the application of the prohibitions is important to take account of unacceptable  risk and has effect on other procedures, such as in civil law. Moreover, the infrastructure  related to the governance and the conformity assessment system should be operational  before [OP – please insert the date established in Art. 85], therefore the provisions on  notified bodies and governance structure should apply from … [OP – please insert the date 
 
– twelve months following the entry into force of this Regulation]. Given the rapid pace of  technological advancements and adoption of general-purpose AI models, obligations for  providers of general purpose AI models should apply within 12 months from the date of  entry into force. Codes of Practice should be ready at the latest 3 months before the entry  into application of the relevant provisions, to enable providers to demonstrate compliance  in time. The AI Office should ensure that classification rules and procedures are up to date  in light of technological developments. In addition, Member States should lay down and  notify to the Commission the rules on penalties, including administrative fines, and ensure  that they are properly and effectively implemented by the date of application of this  Regulation. Therefore, the provisions on penalties should apply from [OP – please insert  the date – twelve months following the entry into force of this Regulation]. 
(89) The European Data Protection Supervisor and the European Data Protection Board were  consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and delivered an  opinion on 18 June 2021. 
TITLE I 
GENERAL PROVISIONS 
Article 1 
Subject matter 
1. The purpose of this Regulation is to improve the functioning of the internal market and  promoting the uptake of human centric and trustworthy artificial intelligence, while  ensuring a high level of protection of health, safety, fundamental rights enshrined in the  Charter, including democracy, rule of law and environmental protection against harmful  effects of artificial intelligence systems in the Union and supporting innovation. 
2. This Regulation lays down: 
(a) harmonised rules for the placing on the market, the putting into service and the use of  artificial intelligence systems (‘AI systems’) in the Union;
 
(b) prohibitions of certain artificial intelligence practices; 
(c) specific requirements for high-risk AI systems and obligations for operators of such  systems; 
(d) harmonised transparency rules for certain AI systems; 
(da) harmonised rules for the placing on the market of general-purpose AI models; (e) rules on market monitoring, market surveillance governance and enforcement; (ea) measures to support innovation, with a particular focus on SMEs, including start-ups. 
Article 2 
Scope 
1. This Regulation applies to: 
(a) providers placing on the market or putting into service AI systems or placing on the  market general-purpose AI models in the Union, irrespective of whether those  providers are established or who are located within the Union or in a third country; 
(b) deployers of AI systems that have their place of establishment or who are located  within the Union; 
(c) providers and deployers of AI systems that have their place of establishment or who  are located in a third country, where the output produced by the system is used in the  Union; 
(ca) importers and distributors of AI systems; 
(cb) product manufacturers placing on the market or putting into service an AI system  together with their product and under their own name or trademark; 
(cc) authorised representatives of providers, which are not established in the Union. (cc) affected persons that are located in the Union.
 
2. For AI systems classified as high-risk AI systems in accordance with Articles 6(1) and 6(2)  related to products covered by Union harmonisation legislation listed in Annex II, section  B only Article 84 of this Regulation shall apply. Article 53 shall apply only insofar as the  requirements for high-risk AI systems under this Regulation have been integrated under  that Union harmonisation legislation. 
3. This Regulation shall not apply to areas outside the scope of EU law and in any event shall  not affect the competences of the Member States concerning national security, regardless  of the type of entity entrusted by the Member States to carry out the tasks in relation to  those competences.  
This Regulation shall not apply to AI systems if and insofar placed on the market, put into  service, or used with or without modification of such systems exclusively for military,  defence or national security purposes, regardless of the type of entity carrying out those  activities.  
This Regulation shall not apply to AI systems which are not placed on the market or put  into service in the Union, where the output is used in the Union exclusively for military,  defence or national security purposes, regardless of the type of entity carrying out those  activities.  
4. This Regulation shall not apply to public authorities in a third country nor to international  organisations falling within the scope of this Regulation pursuant to paragraph 1, where  those authorities or organisations use AI systems in the framework of international  cooperation or agreements for law enforcement and judicial cooperation with the Union or  with one or more Member States, under the condition that this third country or  international organisations provide adequate safeguards with respect to the protection of  fundamental rights and freedoms of individuals. 
5. This Regulation shall not affect the application of the provisions on the liability of  intermediary service providers set out in Chapter II, Section 4 of Directive 2000/31/EC of  the European Parliament and of the Council29 [as to be replaced by the corresponding  provisions of the Digital Services Act].  
  
29 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of  information society services, in particular electronic commerce, in the Internal Market ('Directive on electronic  commerce') (OJ L 178, 17.7.2000, p. 1).
 
5a. This Regulation shall not apply to AI systems and models, including their output,  specifically developed and put into service for the sole purpose of scientific research and  development. 
5a. Union law on the protection of personal data, privacy and the confidentiality of  communications applies to personal data processed in connection with the rights and  obligations laid down in this Regulation. This Regulation shall not affect Regulations (EU)  2016/679 and (EU) 2018/1725 and Directives 2002/58/EC and (EU) 2016/680, without  prejudice to arrangements provided for in Article 10(5) and Article 54 of this Regulation. 
5b. This Regulation shall not apply to any research, testing and development activity regarding  AI systems or models prior to being placed on the market or put into service; those  activities shall be conducted respecting applicable Union law. The testing in real world  conditions shall not be covered by this exemption. 
5b. This Regulation is without prejudice to the rules laid down by other Union legal acts  related to consumer protection and product safety. 
5c. This Regulation shall not apply to obligations of deployers who are natural persons using  AI systems in the course of a purely personal non-professional activity. 
5e. This Regulation shall not preclude Member States or the Union from maintaining or  introducing laws, regulations or administrative provisions which are more favourable to  workers in terms of protecting their rights in respect of the use of AI systems by  employers, or to encourage or allow the application of collective agreements which are  more favourable to workers.  
5g. The obligations laid down in this Regulation shall not apply to AI systems released under  free and open source licences unless they are placed on the market or put into service as  high-risk AI systems or an AI system that falls under Title II and IV.  
Article 3 
Definitions 
For the purpose of this Regulation, the following definitions apply:
 
(1) ‘AI system‘ is a machine-based system designed to operate with varying levels of  autonomy and that may exhibit adaptiveness after deployment and that, for explicit  or implicit objectives, infers, from the input it receives, how to generate outputs such  as predictions, content, recommendations, or decisions that can influence physical or  virtual environments; 
(1a) ‘risk’ means the combination of the probability of an occurrence of harm and the  severity of that harm; 
(2) ‘provider’ means a natural or legal person, public authority, agency or other body  that develops an AI system or a general purpose AI model or that has an AI system  or a general purpose AI model developed and places them on the market or puts the  system into service under its own name or trademark, whether for payment or free of  charge; 
(4) ‘deployer means any natural or legal person, public authority, agency or other body  using an AI system under its authority except where the AI system is used in the  course of a personal non-professional activity; 
(5) ‘authorised representative’ means any natural or legal person located or established  in the Union who has received and accepted a written mandate from a provider of an  AI system or a general-purpose AI model to, respectively, perform and carry out on  its behalf the obligations and procedures established by this Regulation; 
(6) ‘importer’ means any natural or legal person located or established in the Union that  places on the market an AI system that bears the name or trademark of a natural or  legal person established outside the Union; 
(7) ‘distributor’ means any natural or legal person in the supply chain, other than the  provider or the importer, that makes an AI system available on the Union market; 
(8) ‘operator’ means the provider, the product manufacturer, the deployer, the authorised  representative, the importer or the distributor; 
(9) ‘placing on the market’ means the first making available of an AI system or a general  purpose AI model on the Union market;
 
(10) ‘making available on the market’ means any supply of an AI system or a general  purpose AI model for distribution or use on the Union market in the course of a  commercial activity, whether in return for payment or free of charge; 
(11) ‘putting into service’ means the supply of an AI system for first use directly to the  deployer or for own use in the Union for its intended purpose; 
(12) ‘intended purpose’ means the use for which an AI system is intended by the provider,  including the specific context and conditions of use, as specified in the information  supplied by the provider in the instructions for use, promotional or sales materials  and statements, as well as in the technical documentation; 
(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in  accordance with its intended purpose, but which may result from reasonably  foreseeable human behaviour or interaction with other systems, including other AI  systems; 
(14) ‘safety component of a product or system’ means a component of a product or of a  system which fulfils a safety function for that product or system, or the failure or  malfunctioning of which endangers the health and safety of persons or property; 
(15) ‘instructions for use’ means the information provided by the provider to inform the user of in particular an AI system’s intended purpose and proper use; 
(16) ‘recall of an AI system’ means any measure aimed at achieving the return to the  provider or taking it out of service or disabling the use of an AI system made  available to deployers; 
(17) ‘withdrawal of an AI system’ means any measure aimed at preventing an AI system  in the supply chain being made available on the market; 
(18) ‘performance of an AI system’ means the ability of an AI system to achieve its  intended purpose; 
(19) ‘notifying authority’ means the national authority responsible for setting up and  carrying out the necessary procedures for the assessment, designation and  notification of conformity assessment bodies and for their monitoring;
 
(20) ‘conformity assessment’ means the process of demonstrating whether the  requirements set out in Title III, Chapter 2 of this Regulation relating to a high-risk  AI system have been fulfilled; 
(21) ‘conformity assessment body’ means a body that performs third-party conformity  assessment activities, including testing, certification and inspection; 
(22) ‘notified body’ means a conformity assessment body notified in accordance with this  Regulation and other relevant Union harmonisation legislation; 
(23) ‘substantial modification’ means a change to the AI system after its placing on the  market or putting into service which is not foreseen or planned in the initial  conformity assessment by the provider and as a result of which the compliance of the  AI system with the requirements set out in Title III, Chapter 2 of this Regulation is  affected or results in a modification to the intended purpose for which the AI system  has been assessed; 
(24) ‘CE marking of conformity’ (CE marking) means a marking by which a provider  indicates that an AI system is in conformity with the requirements set out in Title III,  Chapter 2 of this Regulation and other applicable Union legislation harmonising the  conditions for the marketing of products (‘Union harmonisation legislation’)  providing for its affixing; 
(25) ‘post-market monitoring system’ means all activities carried out by providers of AI  systems to collect and review experience gained from the use of AI systems they  place on the market or put into service for the purpose of identifying any need to  immediately apply any necessary corrective or preventive actions; 
(26) ‘market surveillance authority’ means the national authority carrying out the  activities and taking the measures pursuant to Regulation (EU) 2019/1020; 
(27) ‘harmonised standard’ means a European standard as defined in Article 2(1)(c) of  Regulation (EU) No 1025/2012; 
(28) ‘common specification’ means a set of technical specifications, as defined in point 4  of Article 2 of Regulation (EU) No 1025/2012 providing means to comply with  certain requirements established under this Regulation;
 
(29) ‘training data’ means data used for training an AI system through fitting its learnable  parameters; 
(30) ‘validation data’ means data used for providing an evaluation of the trained AI  system and for tuning its non-learnable parameters and its learning process, among  other things, in order to prevent underfitting or overfitting; whereas the validation  dataset is a separate dataset or part of the training dataset, either as a fixed or variable  split; 
(31) ‘testing data’ means data used for providing an independent evaluation of the AI  system in order to confirm the expected performance of that system before its placing  on the market or putting into service; 
(32) ‘input data’ means data provided to or directly acquired by an AI system on the basis  of which the system produces an output; 
(33) ‘biometric data’ means personal data resulting from specific technical processing  relating to the physical, physiological or behavioural characteristics of a natural  person, such as facial images or dactyloscopic data; 
(33a) ‘biometric identification’ means the automated recognition of physical,  physiological, behavioural, and psychological human features for the purpose of  establishing an individual’s identity by comparing biometric data of that individual to  stored biometric data of individuals in a database; 
(33c) ‘biometric verification’ means the automated verification of the identity of natural  persons by comparing biometric data of an individual to previously provided  biometric data (one-to-one verification, including authentication); 
(33d) ‘special categories of personal data’ means the categories of personal data referred to  in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680  and Article 10(1) of Regulation (EU) 2018/1725; 
(33e) ‘sensitive operational data’ means operational data related to activities of prevention,  detection, investigation and prosecution of criminal offences, the disclosure of which  can jeopardise the integrity of criminal proceedings;
 
(34) ‘emotion recognition system’ means an AI system for the purpose of identifying or  inferring emotions or intentions of natural persons on the basis of their biometric  data; 
(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning  natural persons to specific categories on the basis of their biometric data unless  ancillary to another commercial service and strictly necessary for objective technical  reasons;  
(36) ‘remote biometric identification system’ means an AI system for the purpose of  identifying natural persons, without their active involvement, typically at a distance  through the comparison of a person’s biometric data with the biometric data  contained in a reference database; 
(37) ‘‘real-time’ remote biometric identification system’ means a remote biometric  identification system whereby the capturing of biometric data, the comparison and  the identification all occur without a significant delay. This comprises not only  instant identification, but also limited short delays in order to avoid circumvention; 
(38) ‘‘post’ remote biometric identification system’ means a remote biometric  identification system other than a ‘real-time’ remote biometric identification system; 
(39) ‘publicly accessible space’ means any publicly or privately owned physical place  accessible to an undetermined number of natural persons, regardless of whether  certain conditions for access may apply, and regardless of the potential capacity  restrictions; 
(40) ‘law enforcement authority’ means: 
(a) any public authority competent for the prevention, investigation, detection or  prosecution of criminal offences or the execution of criminal penalties,  including the safeguarding against and the prevention of threats to public  security; or 
(b) any other body or entity entrusted by Member State law to exercise public  authority and public powers for the purposes of the prevention, investigation,  detection or prosecution of criminal offences or the execution of criminal 
 
penalties, including the safeguarding against and the prevention of threats to  public security; 
(41) ‘law enforcement’ means activities carried out by law enforcement authorities or on  their behalf for the prevention, investigation, detection or prosecution of criminal  offences or the execution of criminal penalties, including the safeguarding against  and the prevention of threats to public security; 
(42) ‘Artificial Intelligence Office’ means the Commission’s function of contributing to  the implementation, monitoring and supervision of AI systems, general purpose AI  models and AI governance. References in this Regulation to the Artificial  Intelligence office shall be understood as references to the Commission; 
(43) ‘national competent authority’ means any of the following: the notifying authority  and the market surveillance authority. As regards AI systems put into service or used  by EU institutions, agencies, offices and bodies, any reference to national competent  authorities or market surveillance authorities in this Regulation shall be understood  as referring to the European Data Protection Supervisor; 
(44) ‘serious incident’ means any incident or malfunctioning of an AI system that directly  or indirectly leads to any of the following: 
(a) the death of a person or serious damage to a person’s health; 
(b) a serious and irreversible disruption of the management and operation of  critical infrastructure; 
(ba) breach of obligations under Union law intended to protect fundamental rights; (bb) serious damage to property or the environment. 
(44a) 'personal data' means personal data as defined in Article 4, point (1) of Regulation  (EU) 2016/679; 
(44c) ‘non-personal data’ means data other than personal data as defined in point (1) of  Article 4 of Regulation (EU) 2016/679; 
(be) ‘profiling’ means any form of automated processing of personal data as defined in  point (4) of Article 4 of Regulation (EU) 2016/679; or in the case of law enforcement 
 
authorities – in point 4 of Article 3 of Directive (EU) 2016/680 or, in the case of  Union institutions, bodies, offices or agencies, in point 5 Article 3 of Regulation  (EU) 2018/1725; 
(bf) ‘real world testing plan’ means a document that describes the objectives,  methodology, geographical, population and temporal scope, monitoring, organisation  and conduct of testing in real world conditions; 
(44 eb)‘sandbox plan’ means a document agreed between the participating provider and the  competent authority describing the objectives, conditions, timeframe, methodology  and requirements for the activities carried out within the sandbox; 
(bg) 'AI regulatory sandbox’ means a concrete and controlled framework set up by a  competent authority which offers providers or prospective providers of AI systems  the possibility to develop, train, validate and test, where appropriate in real world  conditions, an innovative AI system, pursuant to a sandbox plan for a limited time  under regulatory supervision; 
(bh) ‘AI literacy’ refers to skills, knowledge and understanding that allows providers,  users and affected persons, taking into account their respective rights and obligations  in the context of this Regulation, to make an informed deployment of AI systems, as  well as to gain awareness about the opportunities and risks of AI and possible harm it  can cause;  
(bi) ‘testing in real world conditions’ means the temporary testing of an AI system for its  intended purpose in real world conditions outside of a laboratory or otherwise  simulated environment with a view to gathering reliable and robust data and to  
assessing and verifying the conformity of the AI system with the requirements of this  Regulation; testing in real world conditions shall not be considered as placing the AI  system on the market or putting it into service within the meaning of this Regulation,  provided that all conditions under Article 53 or Article 54a are fulfilled; 
(bj) ‘subject’ for the purpose of real world testing means a natural person who  participates in testing in real world conditions; 
(bk) ‘informed consent’ means a subject's freely given, specific, unambiguous and  voluntary expression of his or her willingness to participate in a particular testing in 
 
real world conditions, after having been informed of all aspects of the testing that are  relevant to the subject's decision to participate;  
(bl) "deep fake" means AI generated or manipulated image, audio or video content that  resembles existing persons, objects, places or other entities or events and would  falsely appear to a person to be authentic or truthful; 
(44e) ‘widespread infringement’ means any act or omission contrary to Union law that  protects the interest of individuals: 
(a) which has harmed or is likely to harm the collective interests of individuals  residing in at least two Member States other than the Member State, in which: 
(i) the act or omission originated or took place; 
(ii) the provider concerned, or, where applicable, its authorised  
representative is established; or 
(iii) the deployer is established, when the infringement is committed by the  deployer; 
(b) which protects the interests of individuals, that have caused, cause or are likely  to cause harm to the collective interests of individuals and that have common  features, including the same unlawful practice, the same interest being  infringed and that are occurring concurrently, committed by the same operator,  in at least three Member States; 
(44h) ‘critical infrastructure’ means an asset, a facility, equipment, a network or a system,  or a part of thereof, which is necessary for the provision of an essential service  within the meaning of Article 2(4) of Directive (EU) 2022/2557; 
(44b) ‘general purpose AI model’ means an AI model, including when trained with a large  amount of data using self-supervision at scale, that displays significant generality and  is capable to competently perform a wide range of distinct tasks regardless of the  way the model is placed on the market and that can be integrated into a variety of  downstream systems or applications. This does not cover AI models that are used  before release on the market for research, development and prototyping activities;
 
(44c) ‘high-impact capabilities’ in general purpose AI models means capabilities that  match or exceed the capabilities recorded in the most advanced general purpose AI  models; 
(44d) ‘systemic risk at Union level’ means a risk that is specific to the high-impact  capabilities of general-purpose AI models, having a significant impact on the internal  market due to its reach, and with actual or reasonably foreseeable negative effects on  public health, safety, public security, fundamental rights, or the society as a whole,  that can be propagated at scale across the value chain; 
(44e) ‘general purpose AI system’ means an AI system which is based on a general  purpose AI model, that has the capability to serve a variety of purposes, both for  direct use as well as for integration in other AI systems; 
(44f) ’floating-point operation’ means any mathematical operation or assignment  involving floating-point numbers, which are a subset of the real numbers typically  represented on computers by an integer of fixed precision scaled by an integer  exponent of a fixed base; 
(44g) ‘downstream provider’ means a provider of an AI system, including a general purpose AI system, which integrates an AI model, regardless of whether the model is  provided by themselves and vertically integrated or provided by another entity based  on contractual relations. 
Article 4b 
AI literacy 
Providers and deployers of AI systems shall take measures to ensure, to their best extent, a  sufficient level of AI literacy of their staff and other persons dealing with the operation and  use of AI systems on their behalf, taking into account their technical knowledge,  experience, education and training and the context the AI systems are to be used in, and  considering the persons or groups of persons on which the AI systems are to be used.
 
TITLE II 
PROHIBITED ARTIFICIAL INTELLIGENCE PRACTICES 
Article 5 
Prohibited Artificial Intelligence Practices 
1. The following artificial intelligence practices shall be prohibited: 
(a) the placing on the market, putting into service or use of an AI system that deploys  subliminal techniques beyond a person’s consciousness or purposefully manipulative  or deceptive techniques, with the objective to or the effect of materially distorting a  person’s or a group of persons’ behaviour by appreciably impairing the person’s  ability to make an informed decision, thereby causing the person to take a decision  that that person would not have otherwise taken in a manner that causes or is likely  to cause that person, another person or group of persons significant harm; 
(b) the placing on the market, putting into service or use of an AI system that exploits  any of the vulnerabilities of a person or a specific group of persons due to their age,  disability or a specific social or economic situation, with the objective to or the effect  of materially distorting the behaviour of that person or a person pertaining to that  group in a manner that causes or is reasonably likely to cause that person or another  person significant harm; 
(ba) the placing on the market or putting into service for this specific purpose, or use of  biometric categorisation systems that categorise individually natural persons based  on their biometric data to deduce or infer their race, political opinions, trade union  membership, religious or philosophical beliefs, sex life or sexual orientation.  
This prohibition does not cover any labelling or filtering of lawfully acquired  biometric datasets, such as images, based on biometric data or categorizing of  biometric data in the area of law enforcement;  
(c) the placing on the market, putting into service or use of AI systems for the evaluation  or classification of natural persons or groups thereof over a certain period of time  based on their social behaviour or known, inferred or predicted personal or 
 
personality characteristics, with the social score leading to either or both of the  following: 
(i) detrimental or unfavourable treatment of certain natural persons or whole  groups thereof in social contexts that are unrelated to the contexts in which the  data was originally generated or collected; 
(ii) detrimental or unfavourable treatment of certain natural persons or groups  thereof that is unjustified or disproportionate to their social behaviour or its  gravity; 
(d) the use of ‘real-time’ remote biometric identification systems in publicly accessible  spaces for the purpose of law enforcement unless and in as far as such use is strictly  necessary for one of the following objectives: 
(i) the targeted search for specific victims of abduction, trafficking in human  beings and sexual exploitation of human beings as well as search for missing  persons; 
(ii) the prevention of a specific, substantial and imminent threat to the life or  physical safety of natural persons or a genuine and present or genuine and  foreseeable threat of a terrorist attack;  
(iii) the localisation or identification of a person suspected of having committed a  criminal offence, for the purposes of conducting a criminal investigation,  prosecution or executing a criminal penalty for offences, referred to in Annex  IIa and punishable in the Member State concerned by a custodial sentence or a  detention order for a maximum period of at least four years. This paragraph is  without prejudice to the provisions in Article 9 of the GDPR for the processing  of biometric data for purposes other than law enforcement. 
(da) the placing on the market, putting into service for this specific purpose, or use of an  AI system for making risk assessments of natural persons in order to assess or predict  the risk of a natural person to commit a criminal offence, based solely on the  profiling of a natural person or on assessing their personality traits and  characteristics. This prohibition shall not apply to AI systems used to support the 
 
human assessment of the involvement of a person in a criminal activity, which is  already based on objective and verifiable facts directly linked to a criminal activity;  
(db) the placing on the market, putting into service for this specific purpose, or use of AI  systems that create or expand facial recognition databases through the untargeted  scraping of facial images from the internet or CCTV footage; 
(dc) the placing on the market, putting into service for this specific purpose, or use of AI  systems to infer emotions of a natural person in the areas of workplace and education  institutions except in cases where the use of the AI system is intended to be put in  place or into the market for medical or safety reasons. 
1a. This Article shall not affect the prohibitions that apply where an artificial intelligence  practice infringes other Union law. 
2. The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces  for the purpose of law enforcement for any of the objectives referred to in paragraph 1  point (d) shall only be deployed for the purposes under paragraph 1, point (d) to confirm  the specifically targeted individual’s identity and it shall take into account the following  elements:  
(a) the nature of the situation giving rise to the possible use, in particular the seriousness,  probability and scale of the harm caused in the absence of the use of the system;  
(b) the consequences of the use of the system for the rights and freedoms of all persons  concerned, in particular the seriousness, probability and scale of those consequences.  
In addition, the use of ‘real-time’ remote biometric identification systems in publicly  accessible spaces for the purpose of law enforcement for any of the objectives referred to in  paragraph 1 point (d) shall comply with necessary and proportionate safeguards and  conditions in relation to the use in accordance with national legislations authorizing the use  thereof, in particular as regards the temporal, geographic and personal limitations. The use of  the ‘real-time’ remote biometric identification system in publicly accessible spaces shall only  be authorised if the law enforcement authority has completed a fundamental rights impact  assessment as provided for in Article 29a and has registered the system in the database  according to Article 51. However, in duly justified cases of urgency, the use of the system 
 
may be commenced without the registration, provided that the registration is completed  without undue delay. 
3. As regards paragraphs 1, point (d) and 2, each use for the purpose of law enforcement of a  ‘real-time’ remote biometric identification system in publicly accessible spaces shall be  subject to a prior authorisation granted by a judicial authority or an independent  administrative authority whose decision is binding of the Member State in which the use is  to take place, issued upon a reasoned request and in accordance with the detailed rules of  national law referred to in paragraph 4. However, in a duly justified situation of urgency,  the use of the system may be commenced without an authorisation provided that such  authorisation shall be requested without undue delay, at the latest within 24 hours. If such  authorisation is rejected, its use shall be stopped with immediate effect and all the data, as  well as the results and outputs of this use shall be immediately discarded and deleted.  
The competent judicial authority or an independent administrative authority whose  decision is binding shall only grant the authorisation where it is satisfied, based on  objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote  biometric identification system at issue is necessary for and proportionate to achieving one  of the objectives specified in paragraph 1, point (d), as identified in the request and, in  particular, remains limited to what is strictly necessary concerning the period of time as  well as geographic and personal scope. In deciding on the request, the competent judicial  authority or an independent administrative authority whose decision is binding shall take  into account the elements referred to in paragraph 2. It shall be ensured that no decision  that produces an adverse legal effect on a person may be taken by the judicial authority or  an independent administrative authority whose decision is binding solely based on the  output of the remote biometric identification system.  
3a. Without prejudice to paragraph 3, each use of a ‘real-time’ remote biometric identification  system in publicly accessible spaces for law enforcement purposes shall be notified to the  relevant market surveillance authority and the national data protection authority in  accordance with the national rules referred to in paragraph 4. The notification shall as a  minimum contain the information specified under paragraph 5 and shall not include  sensitive operational data.  
4. A Member State may decide to provide for the possibility to fully or partially authorise the  use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for 
 
the purpose of law enforcement within the limits and under the conditions listed in  paragraphs 1, point (d), 2 and 3. Member States concerned shall lay down in their national  law the necessary detailed rules for the request, issuance and exercise of, as well as  supervision and reporting relating to, the authorisations referred to in paragraph 3. Those  rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d),  including which of the criminal offences referred to in point (iii) thereof, the competent  authorities may be authorised to use those systems for the purpose of law enforcement.  Member States shall notify those rules to the Commission at the latest 30 days following  the adoption thereof. Member States may introduce, in accordance with Union law, more  restrictive laws on the use of remote biometric identification systems.  
5. National market surveillance authorities and the national data protection authorities of  Member States that have been notified of the use of ‘real-time’ remote biometric  identification systems in publicly accessible spaces for law enforcement purposes pursuant  to paragraph 3a shall submit to the Commission annual reports on such use. For that  purpose, the Commission shall provide Member States and national market surveillance  and data protection authorities with a template, including information on the number of the  decisions taken by competent judicial authorities or an independent administrative  authority whose decision is binding upon requests for authorisations in accordance with  paragraph 3 and their result. 
6. The Commission shall publish annual reports on the use of ‘real-time’ remote biometric  identification systems in publicly accessible spaces for law enforcement purposes based on  aggregated data in Member States based on the annual reports referred to in paragraph 5,  which shall not include sensitive operational data of the related law enforcement activities.
 
TITLE III 
HIGH-RISK AI SYSTEMS 
Chapter 1 
CLASSIFICATION OF AI SYSTEMS AS HIGH-RISK 
Article 6 
Classification rules for high-risk AI systems 
1. Irrespective of whether an AI system is placed on the market or put into service  independently from the products referred to in points (a) and (b), that AI system shall be  considered high-risk where both of the following conditions are fulfilled: 
(a) the AI system is intended to be used as a safety component of a product, or the AI  system is itself a product, covered by the Union harmonisation legislation listed in  Annex II; 
(b) the product whose safety component pursuant to point (a) is the AI system, or the AI  system itself as a product, is required to undergo a third-party conformity  
assessment, with a view to the placing on the market or putting into service of that  product pursuant to the Union harmonisation legislation listed in Annex II. 
2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in  Annex III shall also be considered high-risk. 
2a. By derogation from paragraph 2 AI systems shall not be considered as high risk if they do  not pose a significant risk of harm, to the health, safety or fundamental rights of natural  persons, including by not materially influencing the outcome of decision making. This  shall be the case if one or more of the following criteria are fulfilled: 
(a) the AI system is intended to perform a narrow procedural task; 
(b) the AI system is intended to improve the result of a previously completed human  activity; 
 
(c) the AI system is intended to detect decision-making patterns or deviations from prior  decision-making patterns and is not meant to replace or influence the previously  completed human assessment, without proper human review; or 
(d) the AI system is intended to perform a preparatory task to an assessment relevant for  the purpose of the use cases listed in Annex III. 
Notwithstanding first subparagraph of this paragraph, an AI system shall always be  considered high-risk if the AI system performs profiling of natural persons. 
2b. A provider who considers that an AI system referred to in Annex III is not high-risk shall  document its assessment before that system is placed on the market or put into service.  Such provider shall be subject to the registration obligation set out in Article 51(1a). Upon  request of national competent authorities, the provider shall provide the documentation of  the assessment. 
2c. The Commission shall, after consulting the AI Board, and no later than 18 months after the  entry into force of this Regulation, provide guidelines specifying the practical  implementation of this article completed by a comprehensive list of practical examples of  high risk and non-high risk use cases on AI systems in accordance with the conditions set  out in Article 82a. 
2d. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend the criteria laid down in points (a) to (d) of the first subparagraph of paragraph 2a.  
The Commission may adopt delegated acts adding new criteria to those laid down in points  (a) to (d) of the first subparagraph of paragraph 2a, or modifying them, only where there is  concrete and reliable evidence of the existence of AI systems that fall under the scope of  Annex III but that do not pose a significant risk of harm to the health, safety and  fundamental rights. 
The Commission shall adopt delegated acts deleting any of the criteria laid down in the  first subparagraph of paragraph 2a where there is concrete and reliable evidence that this is  necessary for the purpose of maintaining the level of protection of health, safety and  fundamental rights in the Union.
 
Any amendment to the criteria laid down in points (a) to (d) set out in the first  subparagraph of paragraph 2a shall not decrease the overall level of protection of health,  safety and fundamental rights in the Union. 
When adopting the delegated acts, the Commission shall ensure consistency with the  delegated acts adopted pursuant to Article 7(1) and shall take account of market and  technological developments. 
Article 7 
Amendments to Annex III 
1. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend Annex III by adding or modifying use cases of high-risk AI systems where both of  the following conditions are fulfilled: 
(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 of  Annex III; 
(b) the AI systems pose a risk of harm to health and safety, or an adverse impact on  fundamental rights, and that risk is equivalent to or greater than the risk of harm or of  adverse impact posed by the high-risk AI systems already referred to in Annex III.  
2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm  to the health and safety or a risk of adverse impact on fundamental rights that is equivalent  to or greater than the risk of harm posed by the high-risk AI systems already referred to in  Annex III, the Commission shall take into account the following criteria: 
(a) the intended purpose of the AI system; 
(b) the extent to which an AI system has been used or is likely to be used; 
(ba) the nature and amount of the data processed and used by the AI system, in particular  whether special categories of personal data are processed; 
(bb) the extent to which the AI system acts autonomously and the possibility for a human  to override a decision or recommendations that may lead to potential harm;
 
(c) the extent to which the use of an AI system has already caused harm to health and  safety, has had an adverse impact on fundamental rights or has given rise to  significant concerns in relation to the likelihood of such harm or adverse impact, as  demonstrated for example by reports or documented allegations submitted to national  competent authorities or by other reports, as appropriate;  
(d) the potential extent of such harm or such adverse impact, in particular in terms of its  intensity and its ability to affect a plurality of persons or to disproportionately affect  a particular group of persons; 
(e) the extent to which potentially harmed or adversely impacted persons are dependent  on the outcome produced with an AI system, in particular because for practical or  legal reasons it is not reasonably possible to opt-out from that outcome; 
(f) the extent to which there is an imbalance of power, or the potentially harmed or  adversely impacted persons are in a vulnerable position in relation to the user of an  AI system, in particular due to status, authority, knowledge, economic or social  circumstances, or age; 
(g) the extent to which the outcome produced involving an AI system is easily corrigible  or reversible, taking into account the technical solutions available to correct or  reverse, whereby outcomes having and adverse impact on health, safety, fundamental  rights, shall not be considered as easily corrigible or reversible; 
(gb) the magnitude and likelihood of benefit of the deployment of the AI system for  individuals, groups, or society at large, including possible improvements in product  safety; 
(h) the extent to which existing Union legislation provides for: 
(i) effective measures of redress in relation to the risks posed by an AI system,  with the exclusion of claims for damages; 
(ii) effective measures to prevent or substantially minimise those risks. 
2a. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend the list in Annex III by removing high-risk AI systems where both of the following  conditions are fulfilled:
 
(a) the high-risk AI system(s) concerned no longer pose any significant risks to  fundamental rights, health or safety, taking into account the criteria listed in  paragraph 2; 
(b) the deletion does not decrease the overall level of protection of health, safety and fundamental rights under Union law. 
Chapter 2 
REQUIREMENTS FOR HIGH-RISK AI SYSTEMS 
Article 8 
Compliance with the requirements 
1. High-risk AI systems shall comply with the requirements established in this Chapter,  taking into account its intended purpose as well as the generally acknowledged state of the  art on AI and AI related technologies. The risk management system referred to in Article 9  shall be taken into account when ensuring compliance with those requirements. 
2a. Where a product contains an artificial intelligence system, to which the requirements of  this Regulation as well as requirements of the Union harmonisation legislation listed in  Annex II, Section A apply, providers shall be responsible for ensuring that their product is  fully compliant with all applicable requirements required under the Union harmonisation  legislation. In ensuring the compliance of high-risk AI systems referred in paragraph 1  with the requirements set out in Chapter 2 of this Title, and in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice to  integrate, as appropriate, the necessary testing and reporting processes, information and  documentation they provide with regard to their product into already existing  documentation and procedures required under the Union harmonisation legislation listed in  Annex II, Section A.
 
Article 9 
Risk management system 
1. A risk management system shall be established, implemented, documented and maintained  in relation to high-risk AI systems. 
2. The risk management system shall be understood as a continuous iterative process planned  and run throughout the entire lifecycle of a high-risk AI system, requiring regular  systematic review and updating. It shall comprise the following steps: 
(a) identification and analysis of the known and the reasonably foreseeable risks that the  high-risk AI system can pose to the health, safety or fundamental rights when the  high-risk AI system is used in accordance with its intended purpose; 
(b) estimation and evaluation of the risks that may emerge when the high-risk AI system  is used in accordance with its intended purpose and under conditions of reasonably  foreseeable misuse; 
(c) evaluation of other possibly arising risks based on the analysis of data gathered from  the post-market monitoring system referred to in Article 61; 
(d) adoption of appropriate and targeted risk management measures designed to address  the risks identified pursuant to point a of this paragraph in accordance with the  provisions of the following paragraphs. 
2a. The risks referred to in this paragraph shall concern only those which may be reasonably  mitigated or eliminated through the development or design of the high-risk AI system, or  the provision of adequate technical information.  
3. The risk management measures referred to in paragraph 2, point (d) shall give due  consideration to the effects and possible interaction resulting from the combined  application of the requirements set out in this Chapter 2, with a view to minimising risks  more effectively while achieving an appropriate balance in implementing the measures to  fulfil those requirements.
 
4. The risk management measures referred to in paragraph 2, point (d) shall be such that  relevant residual risk associated with each hazard as well as the overall residual risk of the  high-risk AI systems is judged to be acceptable. 
In identifying the most appropriate risk management measures, the following shall be  ensured: 
(a) elimination or reduction of identified risks and evaluated pursuant to paragraph 2 as  far as technically feasible through adequate design and development of the high-risk  AI system;  
(b) where appropriate, implementation of adequate mitigation and control measures  addressing risks that cannot be eliminated; 
(c) provision of the required information pursuant to Article 13, referred to in paragraph  2, point (b) of this Article, and, where appropriate, training to deployers. 
With a view to eliminating or reducing risks related to the use of the high-risk AI system,  due consideration shall be given to the technical knowledge, experience, education,  training to be expected by the deployer and the presumable context in which the system is  intended to be used. 
5. High-risk AI systems shall be tested for the purposes of identifying the most appropriate  and targeted risk management measures. Testing shall ensure that high-risk AI systems  perform consistently for their intended purpose and they are in compliance with the  requirements set out in this Chapter. 
6. Testing procedures may include testing in real world conditions in accordance with Article  54a. 
7. The testing of the high-risk AI systems shall be performed, as appropriate, at any point in  time throughout the development process, and, in any event, prior to the placing on the  market or the putting into service. Testing shall be made against prior defined metrics and  probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI  system. 
8. When implementing the risk management system described in paragraphs 1 to 6, providers  shall give consideration to whether in view of its intended purpose the high-risk AI system 
 
is likely to adversely impact persons under the age of 18 and, as appropriate, other  vulnerable groups of people. 
9. For providers of high-risk AI systems that are subject to requirements regarding internal  risk management processes under relevant sectorial Union law, the aspects described in  paragraphs 1 to 8 may be part of or combined with the risk management procedures  established pursuant to that law. 
Article 10 
Data and data governance 
1. High-risk AI systems which make use of techniques involving the training of models with  data shall be developed on the basis of training, validation and testing data sets that meet  the quality criteria referred to in paragraphs 2 to 5 whenever such datasets are used. 
2. Training, validation and testing data sets shall be subject to appropriate data governance  and management practices appropriate for the intended purpose of the AI system. Those  practices shall concern in particular: 
(a) the relevant design choices; 
(aa) data collection processes and origin of data, and in the case of personal data, the  original purpose of data collection; 
(c) relevant data preparation processing operations, such as annotation, labelling,  cleaning, updating, enrichment and aggregation; 
(d) the formulation of assumptions, notably with respect to the information that the data  are supposed to measure and represent; 
(e) an assessment of the availability, quantity and suitability of the data sets that are  needed; 
(f) examination in view of possible biases that are likely to affect the health and safety  of persons, negatively impact fundamental rights or lead to discrimination prohibited  under Union law, especially where data outputs influence inputs for future  
operations;
 
(fa) appropriate measures to detect, prevent and mitigate possible biases identified  according to point (f); 
(g) the identification of relevant data gaps or shortcomings that prevent compliance with  this Regulation, and how those gaps and shortcomings can be addressed. 
3. Training, validation and testing datasets shall be relevant, sufficiently representative, and  to the best extent possible, free of errors and complete in view of the intended purpose.  They shall have the appropriate statistical properties, including, where applicable, as  regards the persons or groups of persons in relation to whom the high-risk AI system is  intended to be used. These characteristics of the data sets may be met at the level of  individual data sets or a combination thereof. 
4. Datasets shall take into account, to the extent required by the intended purpose, the  characteristics or elements that are particular to the specific geographical, contextual,  behavioural or functional setting within which the high-risk AI system is intended to be  used. 
5. To the extent that it is strictly necessary for the purposes of ensuring bias detection and  correction in relation to the high-risk AI systems in accordance with the second paragraph,  point f and fa, the providers of such systems may exceptionally process special categories  of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of  Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to  appropriate safeguards for the fundamental rights and freedoms of natural persons. In  addition to provisions set out in the Regulation (EU) 2016/679, Directive (EU) 2016/680  and Regulation (EU) 2018/1725, all the following conditions shall apply in order for such  processing to occur: 
(a) the bias detection and correction cannot be effectively fulfilled by processing other  data, including synthetic or anonymised data; 
(b) the special categories of personal data processed for the purpose of this paragraph are  subject to technical limitations on the re-use of the personal data and state of the art  security and privacy-preserving measures, including pseudonymisation; 
(c) the special categories of personal data processed for the purpose of this paragraph are subject to measures to ensure that the personal data processed are secured, protected, 
 
subject to suitable safeguards, including strict controls and documentation of the  access, to avoid misuse and ensure only authorised persons have access to those  personal data with appropriate confidentiality obligations; 
(d) the special categories of personal data processed for the purpose of this paragraph are  not to be transmitted, transferred or otherwise accessed by other parties; 
(e) the special categories of personal data processed for the purpose of this paragraph are  deleted once the bias has been corrected or the personal data has reached the end of  its retention period, whatever comes first; 
(f) the records of processing activities pursuant to Regulation (EU) 2016/679, Directive  (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification why the  processing of special categories of personal data was strictly necessary to detect and  correct biases and this objective could not be achieved by processing other data. 
6. For the development of high-risk AI systems not using techniques involving the training of  models, paragraphs 2 to 5 shall apply only to the testing data sets. 
Article 11 
Technical documentation  
1. The technical documentation of a high-risk AI system shall be drawn up before that system  is placed on the market or put into service and shall be kept up-to date. 
The technical documentation shall be drawn up in such a way to demonstrate that the high risk AI system complies with the requirements set out in this Chapter and provide national  competent authorities and notified bodies with the necessary information in a clear and  comprehensive form to assess the compliance of the AI system with those requirements. It  shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups,  may provide the elements of the technical documentation specified in Annex IV in a  simplified manner. For this purpose, the Commission shall establish a simplified technical  documentation form targeted at the needs of small and micro enterprises. Where an SME,  including start-ups, opts to provide the information required in Annex IV in a simplified  manner, it shall use the form referred to in this paragraph. Notified bodies shall accept the  form for the purpose of conformity assessment.
 
2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II,  section A apply, is placed on the market or put into service one single technical  documentation shall be drawn up containing all the information set out in paragraph 1 as  well as the information required under those legal acts. 
3. The Commission is empowered to adopt delegated acts in accordance with Article 73 to  amend Annex IV where necessary to ensure that, in the light of technical progress, the  technical documentation provides all the necessary information to assess the compliance of  the system with the requirements set out in this Chapter. 
Article 12 
Record-keeping 
1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’)  over the duration of the lifetime of the system. 
2. In order to ensure a level of traceability of the AI system’s functioning that is appropriate  to the intended purpose of the system, logging capabilities shall enable the recording of  events relevant for: 
(i) identification of situations that may result in the AI system presenting a risk within the  meaning of Article 65(1) or in a substantial modification;  
(ii) facilitation of the post-market monitoring referred to in Article 61; and  
(iii) monitoring of the operation of high-risk AI systems referred to in Article 29(4). 
4. For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging  capabilities shall provide, at a minimum: 
(a) recording of the period of each use of the system (start date and time and end date  and time of each use); 
(b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match;
 
(d) the identification of the natural persons involved in the verification of the results, as  referred to in Article 14 (5). 
Article 13 
Transparency and provision of information to deployers 
1. High-risk AI systems shall be designed and developed in such a way to ensure that their  operation is sufficiently transparent to enable deployers to interpret the system’s output and  use it appropriately. An appropriate type and degree of transparency shall be ensured with  a view to achieving compliance with the relevant obligations of the provider and deployer  set out in Chapter 3 of this Title. 
2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital  format or otherwise that include concise, complete, correct and clear information that is  relevant, accessible and comprehensible to users. 
3. The instructions for use shall contain at least the following information: 
(a) the identity and the contact details of the provider and, where applicable, of its  authorised representative; 
(b) the characteristics, capabilities and limitations of performance of the high-risk AI  system, including: 
(i) its intended purpose; 
(ii) the level of accuracy, including its metrics, robustness and cybersecurity  referred to in Article 15 against which the high-risk AI system has been tested  and validated and which can be expected, and any known and foreseeable  
circumstances that may have an impact on that expected level of accuracy,  
robustness and cybersecurity; 
(iii) any known or foreseeable circumstance, related to the use of the high-risk AI  system in accordance with its intended purpose or under conditions of  
reasonably foreseeable misuse, which may lead to risks to the health and safety  or fundamental rights referred to in Article 9(2);
 
(iiia) where applicable, the technical capabilities and characteristics of the AI system  to provide information that is relevant to explain its output;  
(iv) when appropriate, its performance regarding specific persons or groups of  persons on which the system is intended to be used; 
(v) when appropriate, specifications for the input data, or any other relevant  information in terms of the training, validation and testing data sets used,  
taking into account the intended purpose of the AI system; 
(va) where applicable, information to enable deployers to interpret the system’s  output and use it appropriately. 
(c) the changes to the high-risk AI system and its performance which have been pre determined by the provider at the moment of the initial conformity assessment, if  any; 
(d) the human oversight measures referred to in Article 14, including the technical  measures put in place to facilitate the interpretation of the outputs of AI systems by  the deployers; 
(e) the computational and hardware resources needed, the expected lifetime of the high risk AI system and any necessary maintenance and care measures, including their  frequency, to ensure the proper functioning of that AI system, including as regards  software updates; 
(ea) where relevant, a description of the mechanisms included within the AI system that  allows users to properly collect, store and interpret the logs in accordance with  Article 12. 
Article 14 
Human oversight 
1. High-risk AI systems shall be designed and developed in such a way, including with  appropriate human-machine interface tools, that they can be effectively overseen by natural  persons during the period in which the AI system is in use.
 
2. Human oversight shall aim at preventing or minimising the risks to health, safety or  fundamental rights that may emerge when a high-risk AI system is used in accordance with  its intended purpose or under conditions of reasonably foreseeable misuse, in particular  when such risks persist notwithstanding the application of other requirements set out in this  Chapter. 
3. The oversight measures shall be commensurate to the risks, level of autonomy and context  of use of the AI system and shall be ensured through either one or all of the following  types of measures: 
(a) measures identified and built, when technically feasible, into the high-risk AI system  by the provider before it is placed on the market or put into service;  
(b) measures identified by the provider before placing the high-risk AI system on the  market or putting it into service and that are appropriate to be implemented by the  user. 
4. For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be  provided to the user in such a way that natural persons to whom human oversight is  assigned are enabled, as appropriate and proportionate to the circumstances: 
(a) to properly understand the relevant capacities and limitations of the high-risk AI  system and be able to duly monitor its operation, also in view of detecting and  addressing anomalies, dysfunctions and unexpected performance; 
(b) to remain aware of the possible tendency of automatically relying or over-relying on  the output produced by a high-risk AI system (‘automation bias’), in particular for  high-risk AI systems used to provide information or recommendations for decisions  to be taken by natural persons; 
(c) to correctly interpret the high-risk AI system’s output, taking into account for  example the interpretation tools and methods available; 
(d) to decide, in any particular situation, not to use the high-risk AI system or otherwise  disregard, override or reverse the output of the high-risk AI system;
 
(e) to intervene on the operation of the high-risk AI system or interrupt, the system  through a "stop" button or a similar procedure that allows the system to come to a  halt in a safe state. 
5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in  paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the  deployer on the basis of the identification resulting from the system unless this has been  separately verified and confirmed by at least two natural persons with the necessary  competence, training and authority.  
The requirement for a separate verification by at least two natural persons shall not apply  to high risk AI systems used for the purpose of law enforcement, migration, border control  or asylum, in cases where Union or national law considers the application of this  requirement to be disproportionate. 
Article 15 
Accuracy, robustness and cybersecurity 
1. High-risk AI systems shall be designed and developed in such a way that they achieve an  appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in  those respects throughout their lifecycle. 
1a. To address the technical aspects of how to measure the appropriate levels of accuracy and  robustness set out in paragraph 1 of this Article and any other relevant performance  metrics, the Commission shall, in cooperation with relevant stakeholder and organisations  such as metrology and benchmarking authorities, encourage as appropriate, the  development of benchmarks and measurement methodologies. 
2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be  declared in the accompanying instructions of use. 
3. High-risk AI systems shall be as resilient as possible regarding errors, faults or  inconsistencies that may occur within the system or the environment in which the system  operates, in particular due to their interaction with natural persons or other systems.  Technical and organisational measures shall be taken towards this regard.
 
The robustness of high-risk AI systems may be achieved through technical redundancy  solutions, which may include backup or fail-safe plans. 
High-risk AI systems that continue to learn after being placed on the market or put into  service shall be developed in such a way to eliminate or reduce as far as possible the risk of  possibly biased outputs influencing input for future operations (‘feedback loops’) are duly  addressed with appropriate mitigation measures. 
4. High-risk AI systems shall be resilient as regards to attempts by unauthorised third parties  to alter their use, outputs or performance by exploiting the system vulnerabilities. 
The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall  be appropriate to the relevant circumstances and the risks. 
The technical solutions to address AI specific vulnerabilities shall include, where  appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying  to manipulate the training dataset (‘data poisoning’), or pre-trained components used in  training (‘model poisoning’), inputs designed to cause the model to make a mistake  (‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws. 
Chapter 3 
OBLIGATIONS OF PROVIDERS AND DEPLOYERS OF HIGH RISK AI SYSTEMS AND OTHER PARTIES 
Article 16 
Obligations of providers of high-risk AI systems  
Providers of high-risk AI systems shall: 
(a) ensure that their high-risk AI systems are compliant with the requirements set out in  Chapter 2 of this Title; 
(aa) indicate their name, registered trade name or registered trade mark, the address at  which they can be contacted on the high-risk AI system or, where that is not possible,  on its packaging or its accompanying documentation, as applicable;
 
(b) have a quality management system in place which complies with Article 17; (c) keep the documentation referred to in Article 18; 
(d) when under their control, keep the logs automatically generated by their high-risk AI  systems as referred to in Article 20; 
(e) ensure that the high-risk AI system undergoes the relevant conformity assessment  procedure as referred to in Article 43, prior to its placing on the market or putting  into service; 
(ea) draw up an EU declaration of conformity in accordance with Article 48; 
(eb) affix the CE marking to the high-risk AI system to indicate conformity with this  Regulation, in accordance with Article 49; 
(f) comply with the registration obligations referred to in Article 51(1); 
(g) take the necessary corrective actions and provide information as required in Article  21; 
(j) upon a reasoned request of a national competent authority, demonstrate the  conformity of the high-risk AI system with the requirements set out in Chapter 2 of  this Title; 
(ja) ensure that the high-risk AI system complies with accessibility requirements, in  accordance with Directive 2019/882 on accessibility requirements for products and  services and Directive 2016/2102 on the accessibility of the websites and mobile  applications of public sector bodies. 
Article 17 
Quality management system  
1. Providers of high-risk AI systems shall put a quality management system in place that  ensures compliance with this Regulation. That system shall be documented in a systematic  and orderly manner in the form of written policies, procedures and instructions, and shall  include at least the following aspects:
 
(a) a strategy for regulatory compliance, including compliance with conformity  assessment procedures and procedures for the management of modifications to the  high-risk AI system; 
(b) techniques, procedures and systematic actions to be used for the design, design  control and design verification of the high-risk AI system; 
(c) techniques, procedures and systematic actions to be used for the development,  quality control and quality assurance of the high-risk AI system; 
(d) examination, test and validation procedures to be carried out before, during and after  the development of the high-risk AI system, and the frequency with which they have  to be carried out; 
(e) technical specifications, including standards, to be applied and, where the relevant  harmonised standards are not applied in full, or do not cover all of the relevant  requirements set out in Chapter II of this Title, the means to be used to ensure that  the high-risk AI system complies with those requirements; 
(f) systems and procedures for data management, including data acquisition, data  collection, data analysis, data labelling, data storage, data filtration, data mining, data  aggregation, data retention and any other operation regarding the data that is  performed before and for the purposes of the placing on the market or putting into  service of high-risk AI systems; 
(g) the risk management system referred to in Article 9; 
(h) the setting-up, implementation and maintenance of a post-market monitoring system,  in accordance with Article 61; 
(i) procedures related to the reporting of a serious incident in accordance with Article  62; 
(j) the handling of communication with national competent authorities, other relevant  authorities, including those providing or supporting the access to data, notified  bodies, other operators, customers or other interested parties;
 
(k) systems and procedures for record keeping of all relevant documentation and  information; 
(l) resource management, including security of supply related measures; 
(m) an accountability framework setting out the responsibilities of the management and  other staff with regard to all aspects listed in this paragraph. 
2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the size  of the provider’s organisation. Providers shall in any event respect the degree of rigour and  the level of protection required to ensure compliance of their AI systems with this  Regulation. 
2a. For providers of high-risk AI systems that are subject to obligations regarding quality  management systems or their equivalent function under relevant sectorial Union law, the  aspects described in paragraph 1 may be part of the quality management systems pursuant  to that law. 
3. For providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation, the  obligation to put in place a quality management system with the exception of paragraph 1,  points (g), (h) and (i) shall be deemed to be fulfilled by complying with the rules on  internal governance arrangements or processes pursuant to the relevant Union financial  services legislation. In that context, any harmonised standards referred to in Article 40 of  this Regulation shall be taken into account. 
Article 18 
Documentation keeping 
1. The provider shall, for a period ending 10 years after the AI system has been placed on the  market or put into service, keep at the disposal of the national competent authorities: 
(a) the technical documentation referred to in Article 11; 
(b) the documentation concerning the quality management system referred to in Article  17;
 
(c) the documentation concerning the changes approved by notified bodies where  applicable; 
(d) the decisions and other documents issued by the notified bodies where applicable; (e) the EU declaration of conformity referred to in Article 48. 
1a. Each Member State shall determine conditions under which the documentation referred to  in paragraph 1 remains at the disposal of the national competent authorities for the period  indicated in that paragraph for the cases when a provider or its authorised representative  established on its territory goes bankrupt or ceases its activity prior to the end of that  period. 
2. Providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the technical documentation as part of the documentation kept under the relevant  Union financial services legislation. 
Article 20 
Automatically generated logs 
1. Providers of high-risk AI systems shall keep the logs, referred to in Article 12(1),  automatically generated by their high-risk AI systems, to the extent such logs are under  their control. Without prejudice to applicable Union or national law, the logs shall be kept  for a period appropriate to the intended purpose of the high-risk AI system, of at least 6  months, unless provided otherwise in applicable Union or national law, in particular in  Union law on the protection of personal data. 
2. Providers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the logs automatically generated by their high-risk AI systems as part of the  documentation kept under the relevant financial service legislation.
 
Article 21 
Corrective actions and duty of information 
Providers of high-risk AI systems which consider or have reason to consider that a high risk AI system which they have placed on the market or put into service is not in  conformity with this Regulation shall immediately take the necessary corrective actions to  bring that system into conformity, to withdraw it, to disable it, or to recall it, as  appropriate. They shall inform the distributors of the high-risk AI system in question and,  where applicable, the deployers, the authorised representative and importers accordingly.  
Where the high-risk AI system presents a risk within the meaning of Article 65(1) and the  provider becomes aware of that risk, they shall immediately investigate the causes, in  collaboration with the reporting deployer, where applicable, and inform the market  surveillance authorities of the Member States in which they made the high-risk AI system  available and, where applicable, the notified body that issued a certificate for the high-risk  AI system in accordance with Article 44, in particular, of the nature of the non-compliance  and of any relevant corrective action taken. 
Article 23 
Cooperation with competent authorities 
1. Providers of high-risk AI systems shall, upon a reasoned request by a competent authority,  provide that authority all the information and documentation necessary to demonstrate the  conformity of the high-risk AI system with the requirements set out in Chapter 2 of this  Title, in a language which can be easily understood by the authority in an official Union  language determined by the Member State concerned. 
1a. Upon a reasoned request by a national competent authority providers shall also give the  requesting national competent authority, as applicable, access to the logs referred to in  Article 12(1) automatically generated by the high-risk AI system to the extent such logs are  under their control. 
1b. Any information obtained by a national competent authority pursuant to the provisions of  this Article shall be treated in compliance with the confidentiality obligations set out in  Article 70.
 
Article 25 
Authorised representatives 
1. Prior to making their systems available on the Union market providers established outside  the Union shall, by written mandate, appoint an authorised representative which is  established in the Union. 
1b. The provider shall enable its authorised representative to perform its tasks under this  Regulation. 
2. The authorised representative shall perform the tasks specified in the mandate received  from the provider. It shall provide a copy of the mandate to the market surveillance  authorities upon request, in one of the official languages of the institution of the Union  determined by the national competent authority. For the purpose of this Regulation, the  mandate shall empower the authorised representative to carry out the following tasks: 
(-a) verify that the EU declaration of conformity and the technical documentation have  been drawn up and that an appropriate conformity assessment procedure has been  carried out by the provider; 
(a) keep at the disposal of the national competent authorities and national authorities  referred to in Article 63(7), for a period ending 10 years after the high-risk AI system  has been placed on the market or put into service, the contact details of the provider  by which the authorised representative has been appointed, a copy of the EU  declaration of conformity, the technical documentation and, if applicable, the  certificate issued by the notified body; 
(b) provide a national competent authority, upon a reasoned request, with all the  information and documentation, including that kept according to point (a), necessary  to demonstrate the conformity of a high-risk AI system with the requirements set out  in Chapter 2 of this Title, including access to the logs, as referred to in Article 12(1),  automatically generated by the high-risk AI system to the extent such logs are under  the control of the provider;
 
(c) cooperate with competent authorities, upon a reasoned request, on any action the  latter takes in relation to the high-risk AI system, in particular to reduce and mitigate  the risks posed by the high-risk AI system; 
(ca) where applicable, comply with the registration obligations referred in Article 51(1),  or, if the registration is carried out by the provider itself, ensure that the information  referred to in [point 3] of Annex VIII is correct. 
The mandate shall empower the authorised representative to be addressed, in addition to or  instead of the provider, by the competent authorities, on all issues related to ensuring  compliance with this Regulation.  
2b. The authorised representative shall terminate the mandate if it considers or has reason to  consider that the provider acts contrary to its obligations under this Regulation. In such a  case, it shall also immediately inform the market surveillance authority of the Member  State in which it is established, as well as, where applicable, the relevant notified body,  about the termination of the mandate and the reasons thereof. 
Article 26 
Obligations of importers 
1. Before placing a high-risk AI system on the market, importers of such system shall ensure  that such a system is in conformity with this Regulation by verifying that: 
(a) the relevant conformity assessment procedure referred to in Article 43 has been  carried out by the provider of that AI system; 
(b) the provider has drawn up the technical documentation in accordance with Article 11  and Annex IV; 
(c) the system bears the required CE conformity marking and is accompanied by the EU  declaration of conformity and instructions of use; 
(ca) the provider has appointed an authorised representative in accordance with Article  25(1).
 
2. Where an importer has sufficient reason to consider that a high-risk AI system is not in  conformity with this Regulation, or is falsified, or accompanied by falsified  documentation, it shall not place that system on the market until that AI system has been brought into conformity. Where the high-risk AI system presents a risk within the meaning  of Article 65(1), the importer shall inform the provider of the AI system, the authorised  representatives and the market surveillance authorities to that effect.  
3. Importers shall indicate their name, registered trade name or registered trademark, and the  address at which they can be contacted on the high-risk AI system and on its packaging or  its accompanying documentation, where applicable. 
4. Importers shall ensure that, while a high-risk AI system is under their responsibility, where  applicable, storage or transport conditions do not jeopardise its compliance with the  requirements set out in Chapter 2 of this Title. 
4a. Importers shall keep, for a period ending 10 years after the AI system has been placed on  the market or put into service, a copy of the certificate issued by the notified body, where  applicable, of the instructions for use and of the EU declaration of conformity. 
5. Importers shall provide national competent authorities, upon a reasoned request, with all  the necessary information and documentation including that kept in accordance with  paragraph 4a to demonstrate the conformity of a high-risk AI system with the requirements  set out in Chapter 2 of this Title in a language which can be easily understood by them. To  this purpose they shall also ensure that the technical documentation can be made available  to those authorities.  
5a. Importers shall cooperate with national competent authorities on any action those  authorities take, in particular to reduce and mitigate the risks posed by the high-risk AI  system. 
Article 27 
Obligations of distributors 
1. Before making a high-risk AI system available on the market, distributors shall verify that  the high-risk AI system bears the required CE conformity marking, that it is accompanied  by a copy of EU declaration of conformity and instruction of use, and that the provider and 
 
the importer of the system, as applicable, have complied with their obligations set out in  Article 16, point (aa) and (b) and 26(3) respectively. 
2. Where a distributor considers or has reason to consider, on the basis of the information in  its possession, that a high-risk AI system is not in conformity with the requirements set out  in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market  until that system has been brought into conformity with those requirements. Furthermore,  where the system presents a risk within the meaning of Article 65(1), the distributor shall  inform the provider or the importer of the system, as applicable, to that effect.  
3. Distributors shall ensure that, while a high-risk AI system is under their responsibility,  where applicable, storage or transport conditions do not jeopardise the compliance of the  system with the requirements set out in Chapter 2 of this Title. 
4. A distributor that considers or has reason to consider, on the basis of the information in its  possession, that a high-risk AI system which it has made available on the market is not in  conformity with the requirements set out in Chapter 2 of this Title shall take the corrective  actions necessary to bring that system into conformity with those requirements, to  withdraw it or recall it or shall ensure that the provider, the importer or any relevant  operator, as appropriate, takes those corrective actions. Where the high-risk AI system  presents a risk within the meaning of Article 65(1), the distributor shall immediately  inform the provider or importer of the system and the national competent authorities of the  Member States in which it has made the product available to that effect, giving details, in  particular, of the non-compliance and of any corrective actions taken.  
5. Upon a reasoned request from a national competent authority, distributors of the high-risk  AI system shall provide that authority with all the information and documentation  regarding its activities as described in paragraph 1 to 4 necessary to demonstrate the  conformity of a high-risk system with the requirements set out in Chapter 2 of this Title.  
5a. Distributors shall cooperate with national competent authorities on any action those  authorities take in relation to an AI system, of which they are the distributor, in particular  to reduce or mitigate the risk posed by the high-risk AI system.
 
Article 28 
Responsibilities along the AI value chain  
1. Any distributor, importer, deployer or other third-party shall be considered a provider of a  high-risk AI system for the purposes of this Regulation and shall be subject to the  obligations of the provider under Article 16, in any of the following circumstances: 
(a) they put their name or trademark on a high-risk AI system already placed on the  market or put into service, without prejudice to contractual arrangements stipulating  that the obligations are allocated otherwise; 
(b) they make a substantial modification to a high-risk AI system that has already been  placed on the market or has already been put into service and in a way that it remains  a high-risk AI system in accordance with Article 6; 
(ba) they modify the intended purpose of an AI system, including a general purpose AI  system, which has not been classified as high-risk and has already been placed on the  market or put into service in such manner that the AI system becomes a high risk AI  system in accordance with Article 6. 
2. Where the circumstances referred to in paragraph 1, point (a) to (ba) occur, the provider  that initially placed the AI system on the market or put it into service shall no longer be considered a provider of that specific AI system for the purposes of this Regulation. This  former provider shall closely cooperate and shall make available the necessary information  and provide the reasonably expected technical access and other assistance that are required  for the fulfilment of the obligations set out in this Regulation, in particular regarding the  compliance with the conformity assessment of high-risk AI systems. This paragraph shall  not apply in the cases where the former provider has expressly excluded the change of its  system into a high-risk system and therefore the obligation to hand over the  documentation. 
2a. For high-risk AI systems that are safety components of products to which the legal acts  listed in Annex II, section A apply, the manufacturer of those products shall be considered  the provider of the high-risk AI system and shall be subject to the obligations under Article  16 under either of the following scenarios:
 
(i) the high-risk AI system is placed on the market together with the product under the  name or trademark of the product manufacturer;  
(ii) the high-risk AI system is put into service under the name or trademark of the product  manufacturer after the product has been placed on the market. 
2b. The provider of a high risk AI system and the third party that supplies an AI system, tools,  services, components, or processes that are used or integrated in a high-risk AI system  shall, by written agreement, specify the necessary information, capabilities, technical  access and other assistance based on the generally acknowledged state of the art, in order  to enable the provider of the high risk AI system to fully comply with the obligations set  out in this Regulation. This obligation shall not apply to third parties making accessible to  the public tools, services, processes, or AI components other than general-purpose AI  models under a free and open licence. 
The AI Office may develop and recommend voluntary model contractual terms between  providers of high-risk AI systems and third parties that supply tools, services, components  or processes that are used or integrated in high-risk AI systems. When developing  voluntary model contractual terms, the AI Office shall take into account possible  contractual requirements applicable in specific sectors or business cases. The model  contractual terms shall be published and be available free of charge in an easily usable  electronic format. 
2b. Paragraphs 2 and 2a are without prejudice to the need to respect and protect intellectual  property rights and confidential business information or trade secrets in accordance with  Union and national law. 
Article 29 
Obligations of deployers of high-risk AI systems 
1. Deployers of high-risk AI systems shall take appropriate technical and organisational  measures to ensure they use such systems in accordance with the instructions of use  accompanying the systems, pursuant to paragraphs 2 and 5 of this Article.
 
1a. To the extent deployers exercise control over the high-risk AI system, they shall ensure  that the natural persons assigned to ensure human oversight of the high-risk AI systems  have the necessary competence, training and authority as well as the necessary support.  
2. The obligations in paragraph 1 and 1a, are without prejudice to other deployer obligations  under Union or national law and to the deployer’s discretion in organising its own  resources and activities for the purpose of implementing the human oversight measures  indicated by the provider. 
3. Without prejudice to paragraph 1 and 1a, to the extent the deployer exercises control over  the input data, that deployer shall ensure that input data is relevant and sufficiently  representative in view of the intended purpose of the high-risk AI system. 
4. Deployers shall monitor the operation of the high-risk AI system on the basis of the  instructions of use and when relevant, inform providers in accordance with Article 61.  When they have reasons to consider that the use in accordance with the instructions of use  may result in the AI system presenting a risk within the meaning of Article 65(1) they  shall, without undue delay, inform the provider or distributor and relevant market  surveillance authority and suspend the use of the system. They shall also immediately  inform first the provider, and then the importer or distributor and relevant market  surveillance authorities when they have identified any serious incident. If the deployer is  not able to reach the provider, Article 62 shall apply mutatis mutandis. This obligation  shall not cover sensitive operational data of deployers of AI systems which are law  enforcement authorities. 
For deployers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation, the  monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by  complying with the rules on internal governance arrangements, processes and mechanisms  pursuant to the relevant financial service legislation. 
5. Deployers of high-risk AI systems shall keep the logs automatically generated by that  high-risk AI system to the extent such logs are under their control for a period appropriate  to the intended purpose of the high-risk AI system, of at least six months, unless provided  otherwise in applicable Union or national law, in particular in Union law on the protection  of personal data.
 
Deployers that are financial institutions subject to requirements regarding their internal  governance, arrangements or processes under Union financial services legislation shall  maintain the logs as part of the documentation kept pursuant to the relevant Union  financial service legislation. 
(a) Prior to putting into service or use a high-risk AI system at the workplace, deployers  who are employers shall inform workers representatives and the affected workers  that they will be subject to the system. This information shall be provided, where  applicable, in accordance with the rules and procedures laid down in Union and  national law and practice on information of workers and their representatives. 
(b) Deployers of high-risk AI systems that are public authorities or Union institutions,  bodies, offices and agencies shall comply with the registration obligations referred to  in Article 51. When they find that the system that they envisage to use has not been  registered in the EU database referred to in Article 60 they shall not use that system  and shall inform the provider or the distributor. 
6. Where applicable, deployers of high-risk AI systems shall use the information provided  under Article 13 to comply with their obligation to carry out a data protection impact  assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU)  2016/680.  
6a. Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for the  targeted search of a person convicted or suspected of having committed a criminal offence,  the deployer of an AI system for post-remote biometric identification shall request an  authorisation, prior, or without undue delay and no later than 48 hours, by a judicial  authority or an administrative authority whose decision is binding and subject to judicial  review, for the use of the system, except when the system is used for the initial  identification of a potential suspect based on objective and verifiable facts directly linked  to the offence. Each use shall be limited to what is strictly necessary for the investigation  of a specific criminal offence. 
If the requested authorisation provided for in the first subparagraph of this paragraph is  rejected, the use of the post remote biometric identification system linked to that  authorisation shall be stopped with immediate effect and the personal data linked to the use  of the system for which the authorisation was requested shall be deleted.
 
In any case, such AI system for post remote biometric identification shall not be used for  law enforcement purposes in an untargeted way, without any link to a criminal offence, a  criminal proceeding, a genuine and present or genuine and foreseeable threat of a criminal  offence or the search for a specific missing person. 
It shall be ensured that no decision that produces an adverse legal effect on a person may  be taken by the law enforcement authorities solely based on the output of these post remote  biometric identification systems. 
This paragraph is without prejudice to the provisions of Article 10 of the Directive (EU)  2016/680 and Article 9 of the GDPR for the processing of biometric data. 
Regardless of the purpose or deployer, each use of these systems shall be documented in  the relevant police file and shall be made available to the relevant market surveillance  authority and the national data protection authority upon request, excluding the disclosure  of sensitive operational data related to law enforcement. This subparagraph shall be  without prejudice to the powers conferred by the Directive 2016/680 to supervisory  authorities. 
Deployers shall, in addition, submit annual reports to the relevant market surveillance and  national data protection authorities on the uses of post-remote biometric identification  systems, excluding the disclosure of sensitive operational data related to law enforcement.  The reports can be aggregated to cover several deployments in one operation.  
Member States may introduce, in accordance with Union law, more restrictive laws on the  use of post remote biometric identification systems. 
6b. Without prejudice to Article 52, deployers of high-risk AI systems referred to in Annex III  that make decisions or assist in making decisions related to natural persons shall inform the  natural persons that they are subject to the use of the high-risk AI system. For high risk AI  systems used for law enforcement purposes Article 13 of Directive 2016/680 shall apply. 
6c. Deployers shall cooperate with the relevant national competent authorities on any action  those authorities take in relation with the high-risk system in order to implement this  Regulation. 
Article 29a