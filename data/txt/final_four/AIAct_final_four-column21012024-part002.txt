
TTTXX TABLE: 1 XXTTT
== ROW 1.0 ==
-- COL 1.0.0 --

-- COL 1.0.1 --

-- COL 1.0.2 --

-- COL 1.0.3 --
Commission Proposal 
-- COL 1.0.4 --
EP Mandate 
-- COL 1.0.5 --
Council Mandate 
-- COL 1.0.6 --
Draft Agreement
-- COL 1.0.7 --

-- COL 1.0.8 --

== ROW 1.1 ==
-- COL 1.1.0 --

-- COL 1.1.1 --

-- COL 1.1.2 --

-- COL 1.1.3 --

-- COL 1.1.4 --

-- COL 1.1.5 --

-- COL 1.1.6 --

-- COL 1.1.7 --

-- COL 1.1.8 --

== ROW 1.2 ==
-- COL 1.2.0 --

-- COL 1.2.1 --

-- COL 1.2.2 --
Recital 12f
-- COL 1.2.3 --

-- COL 1.2.4 --

-- COL 1.2.5 --

-- COL 1.2.6 --

-- COL 1.2.7 --

-- COL 1.2.8 --

== ROW 1.3 ==
-- COL 1.3.0 --
G 
-- COL 1.3.1 --

-- COL 1.3.2 --
22f 
-- COL 1.3.3 --

-- COL 1.3.4 --

-- COL 1.3.5 --

-- COL 1.3.6 --

-- COL 1.3.7 --
G
-- COL 1.3.8 --

== ROW 1.4 ==
-- COL 1.4.0 --

-- COL 1.4.1 --

-- COL 1.4.2 --
Recital 12g
-- COL 1.4.3 --

-- COL 1.4.4 --

-- COL 1.4.5 --

-- COL 1.4.6 --

-- COL 1.4.7 --

-- COL 1.4.8 --

== ROW 1.5 ==
-- COL 1.5.0 --
G 
-- COL 1.5.1 --

-- COL 1.5.2 --
22g 
-- COL 1.5.3 --

-- COL 1.5.4 --

-- COL 1.5.5 --

-- COL 1.5.6 --

-- COL 1.5.7 --
G
-- COL 1.5.8 --

== ROW 1.6 ==
-- COL 1.6.0 --

-- COL 1.6.1 --

-- COL 1.6.2 --
Recital 12h
-- COL 1.6.3 --

-- COL 1.6.4 --

-- COL 1.6.5 --

-- COL 1.6.6 --

-- COL 1.6.7 --

-- COL 1.6.8 --

== ROW 1.7 ==
-- COL 1.7.0 --
G 
-- COL 1.7.1 --

-- COL 1.7.2 --
22h 
-- COL 1.7.3 --

-- COL 1.7.4 --

-- COL 1.7.5 --

-- COL 1.7.6 --

-- COL 1.7.7 --
G
-- COL 1.7.8 --

== ROW 1.8 ==
-- COL 1.8.0 --

-- COL 1.8.1 --

-- COL 1.8.2 --
Recital 13
-- COL 1.8.3 --

-- COL 1.8.4 --

-- COL 1.8.5 --

-- COL 1.8.6 --

-- COL 1.8.7 --

-- COL 1.8.8 --

== ROW 1.9 ==
-- COL 1.9.0 --
G 
-- COL 1.9.1 --

-- COL 1.9.2 --
23
-- COL 1.9.3 --
(13) In order to ensure a consistent  and high level of protection of public  interests as regards health, safety and  fundamental rights, common  
normative standards for all high-risk  AI systems should be established.  Those standards should be consistent  with the Charter of fundamental  rights of the European Union (the  Charter) and should be non 
discriminatory and in line with the  Union’s international trade  
commitments.
-- COL 1.9.4 --
(13) In order to ensure a consistent  and high level of protection of public  interests as regards health, safety and  fundamental rights as well as  democracy and rule of law and the  environment, common normative  standards for all high-risk AI  
systems should be established.  Those standards should be consistent  with the Charter, the European  Green Deal, the Joint Declaration  on Digital of fundamental Rights of  the European Union (the  
Charterand the Ethics Guidelines  for Trustworthy Artificial  
Intelligence (AI) of the High-Level  Expert Group on Artificial  
Intelligence, and should be non discriminatory and in line with the  Union’s international trade  
commitments.
-- COL 1.9.5 --
(13) In order to ensure a consistent  and high level of protection of public  interests as regards health, safety and  fundamental rights, common  
normative standards for all high-risk  AI systems should be established.  Those standards should be consistent  with the Charter of fundamental  rights of the European Union (the  Charter) and should be non 
discriminatory and in line with the  Union’s international trade  
commitments.
-- COL 1.9.6 --

-- COL 1.9.7 --
G
-- COL 1.9.8 --


TTTXX TABLE: 2 XXTTT
== ROW 2.0 ==
-- COL 2.0.0 --

-- COL 2.0.1 --

-- COL 2.0.2 --

-- COL 2.0.3 --
Commission Proposal 
-- COL 2.0.4 --
EP Mandate 
-- COL 2.0.5 --
Council Mandate 
-- COL 2.0.6 --
Draft Agreement
-- COL 2.0.7 --

-- COL 2.0.8 --

== ROW 2.1 ==
-- COL 2.1.0 --

-- COL 2.1.1 --

-- COL 2.1.2 --

-- COL 2.1.3 --

-- COL 2.1.4 --

-- COL 2.1.5 --

-- COL 2.1.6 --

-- COL 2.1.7 --

-- COL 2.1.8 --

== ROW 2.2 ==
-- COL 2.2.0 --

-- COL 2.2.1 --

-- COL 2.2.2 --
Recital 14
-- COL 2.2.3 --

-- COL 2.2.4 --

-- COL 2.2.5 --

-- COL 2.2.6 --

-- COL 2.2.7 --

-- COL 2.2.8 --

== ROW 2.3 ==
-- COL 2.3.0 --
G 
-- COL 2.3.1 --

-- COL 2.3.2 --
24
-- COL 2.3.3 --
(14) In order to introduce a  
proportionate and effective set of  binding rules for AI systems, a  clearly defined risk-based approach  should be followed. That approach  should tailor the type and content of  such rules to the intensity and scope  of the risks that AI systems can  generate. It is therefore necessary to  prohibit certain artificial intelligence  practices, to lay down requirements  for high-risk AI systems and  
obligations for the relevant  
operators, and to lay down  
transparency obligations for certain  AI systems.
-- COL 2.3.4 --
(14) In order to introduce a  
proportionate and effective set of  binding rules for AI systems, a  clearly defined risk-based approach  should be followed. That approach  should tailor the type and content of  such rules to the intensity and scope  of the risks that AI systems can  generate. It is therefore necessary to  prohibit certain unacceptable  artificial intelligence practices, to lay  down requirements for high-risk AI  systems and obligations for the  relevant operators, and to lay down  transparency obligations for certain  AI systems.
-- COL 2.3.5 --
(14) In order to introduce a  
proportionate and effective set of  binding rules for AI systems, a  clearly defined risk-based approach  should be followed. That approach  should tailor the type and content of  such rules to the intensity and scope  of the risks that AI systems can  generate. It is therefore necessary to  prohibit certain artificial intelligence  practices, to lay down requirements  for high-risk AI systems and  
obligations for the relevant  
operators, and to lay down  
transparency obligations for certain  AI systems.
-- COL 2.3.6 --
(14) In order to introduce a  
proportionate and effective set of  binding rules for AI systems, a  clearly defined risk-based approach  should be followed. That approach  should tailor the type and content of  such rules to the intensity and scope  of the risks that AI systems can  generate. It is therefore necessary to  prohibit certain unacceptable  artificial intelligence practices, to lay  down requirements for high-risk AI  systems and obligations for the  relevant operators, and to lay down  transparency obligations for certain  AI systems.
-- COL 2.3.7 --
G
-- COL 2.3.8 --

== ROW 2.4 ==
-- COL 2.4.0 --

-- COL 2.4.1 --

-- COL 2.4.2 --
Recital 14a
-- COL 2.4.3 --

-- COL 2.4.4 --

-- COL 2.4.5 --

-- COL 2.4.6 --

-- COL 2.4.7 --

-- COL 2.4.8 --

== ROW 2.5 ==
-- COL 2.5.0 --
G 
-- COL 2.5.1 --

-- COL 2.5.2 --
24a
-- COL 2.5.3 --

-- COL 2.5.4 --

-- COL 2.5.5 --

-- COL 2.5.6 --
(14a) While the risk-based  
approach is the basis for a  
proportionate and effective set of  binding rules, it is important to  recall the 2019 Ethics Guidelines  for Trustworthy AI developed by the  independent High-Level Expert  Group on AI (HLEG) appointed by  the Commission. In those  
Guidelines the HLEG developed  seven non-binding ethical  
principles for AI which should help  ensure that AI is trustworthy and 
-- COL 2.5.7 --
G
-- COL 2.5.8 --


TTTXX TABLE: 3 XXTTT
== ROW 3.0 ==
-- COL 3.0.0 --

-- COL 3.0.1 --

-- COL 3.0.2 --

-- COL 3.0.3 --
Commission Proposal 
-- COL 3.0.4 --
EP Mandate 
-- COL 3.0.5 --
Council Mandate 
-- COL 3.0.6 --
Draft Agreement
-- COL 3.0.7 --

-- COL 3.0.8 --

== ROW 3.1 ==
-- COL 3.1.0 --

-- COL 3.1.1 --

-- COL 3.1.2 --

-- COL 3.1.3 --

-- COL 3.1.4 --

-- COL 3.1.5 --

-- COL 3.1.6 --
ethically sound. The seven  
principles include: human agency  and oversight; technical robustness  and safety; privacy and data  
governance; transparency;  
diversity, non-discrimination and  fairness; societal and  
environmental well-being and  accountability. Without prejudice to  the legally binding requirements of  this Regulation and any other  applicable Union law, these  
Guidelines contribute to the design  of a coherent, trustworthy and  human-centric Artificial  
Intelligence, in line with the  
Charter and with the values on  which the Union is founded.  According to the Guidelines of  HLEG, human agency and  
oversight means that AI systems  are developed and used as a tool  that serves people, respects human  dignity and personal autonomy, and  that is functioning in a way that  can be appropriately controlled and  overseen by humans. Technical  robustness and safety means that  AI systems are developed and used  in a way that allows robustness in  case of problems and resilience  against attempts to alter the use or  performance of the AI system so as  to allow unlawful use by third  parties, and minimise unintended  harm . Privacy and data  
governance means that AI systems 
-- COL 3.1.7 --

-- COL 3.1.8 --


TTTXX TABLE: 4 XXTTT
== ROW 4.0 ==
-- COL 4.0.0 --

-- COL 4.0.1 --

-- COL 4.0.2 --

-- COL 4.0.3 --
Commission Proposal 
-- COL 4.0.4 --
EP Mandate 
-- COL 4.0.5 --
Council Mandate 
-- COL 4.0.6 --
Draft Agreement
-- COL 4.0.7 --

-- COL 4.0.8 --

== ROW 4.1 ==
-- COL 4.1.0 --

-- COL 4.1.1 --

-- COL 4.1.2 --

-- COL 4.1.3 --

-- COL 4.1.4 --

-- COL 4.1.5 --

-- COL 4.1.6 --
are developed and used in  
compliance with existing privacy  and data protection rules, while  processing data that meets high  standards in terms of quality and  integrity. Transparency means that  AI systems are developed and used  in a way that allows appropriate  traceability and explainability,  while making humans aware that  they communicate or interact with  an AI system, as well as duly  informing deployers of the  
capabilities and limitations of that  AI system and affected persons  about their rights. Diversity, non discrimination and fairness means  that AI systems are developed and  used in a way that includes diverse  actors and promotes equal access,  gender equality and cultural  diversity, while avoiding  
discriminatory impacts and unfair  biases that are prohibited by Union  or national law. Social and  
environmental well-being means  that AI systems are developed and  used in a sustainable and  
environmentally friendly manner as  well as in a way to benefit all  human beings, while monitoring  and assessing the long-term impacts  on the individual, society and  democracy. The application of these  principles should be translated,  when possible, in the design and  use of AI models. They should in 
-- COL 4.1.7 --

-- COL 4.1.8 --


TTTXX TABLE: 5 XXTTT
== ROW 5.0 ==
-- COL 5.0.0 --

-- COL 5.0.1 --

-- COL 5.0.2 --

-- COL 5.0.3 --
Commission Proposal 
-- COL 5.0.4 --
EP Mandate 
-- COL 5.0.5 --
Council Mandate 
-- COL 5.0.6 --
Draft Agreement
-- COL 5.0.7 --

-- COL 5.0.8 --

== ROW 5.1 ==
-- COL 5.1.0 --

-- COL 5.1.1 --

-- COL 5.1.2 --

-- COL 5.1.3 --

-- COL 5.1.4 --

-- COL 5.1.5 --

-- COL 5.1.6 --
any case serve as a basis for the  drafting of codes of conduct under  this Regulation. All stakeholders,  including industry, academia, civil  society and standardisation  
organisations, are encouraged to  take into account as appropriate the  ethical principles for the  
development of voluntary best  practices and standards. 
-- COL 5.1.7 --

-- COL 5.1.8 --

== ROW 5.2 ==
-- COL 5.2.0 --

-- COL 5.2.1 --

-- COL 5.2.2 --
Recital 15
-- COL 5.2.3 --

-- COL 5.2.4 --

-- COL 5.2.5 --

-- COL 5.2.6 --

-- COL 5.2.7 --

-- COL 5.2.8 --

== ROW 5.3 ==
-- COL 5.3.0 --
G 
-- COL 5.3.1 --

-- COL 5.3.2 --
25
-- COL 5.3.3 --
(15) Aside from the many beneficial  uses of artificial intelligence, that  technology can also be misused and  provide novel and powerful tools for  manipulative, exploitative and social  control practices. Such practices are  particularly harmful and should be  prohibited because they contradict  Union values of respect for human  dignity, freedom, equality,  
democracy and the rule of law and  Union fundamental rights, including  the right to non-discrimination, data  protection and privacy and the rights  of the child.
-- COL 5.3.4 --
(15) Aside from the many beneficial  uses of artificial intelligence, that  technology can also be misused and  provide novel and powerful tools for  manipulative, exploitative and social  control practices. Such practices are  particularly harmful and abusive and  should be prohibited because they  contradict Union values of respect  for human dignity, freedom,  
equality, democracy and the rule of  law and Union fundamental rights,  including the right to non 
discrimination, data protection and  privacy and the rights of the child.
-- COL 5.3.5 --
(15) Aside from the many beneficial  uses of artificial intelligence, that  technology can also be misused and  provide novel and powerful tools for  manipulative, exploitative and social  control practices. Such practices are  particularly harmful and should be  prohibited because they contradict  Union values of respect for human  dignity, freedom, equality,  
democracy and the rule of law and  Union fundamental rights, including  the right to non-discrimination, data  protection and privacy and the rights  of the child.
-- COL 5.3.6 --
(15) Aside from the many beneficial  uses of artificial intelligence, that  technology can also be misused and  provide novel and powerful tools for  manipulative, exploitative and social  control practices. Such practices are  particularly harmful and abusive and  should be prohibited because they  contradict Union values of respect  for human dignity, freedom,  
equality, democracy and the rule of  law and Union fundamental rights,  including the right to non 
discrimination, data protection and  privacy and the rights of the child.
-- COL 5.3.7 --
G
-- COL 5.3.8 --

== ROW 5.4 ==
-- COL 5.4.0 --

-- COL 5.4.1 --

-- COL 5.4.2 --
Recital 16
-- COL 5.4.3 --

-- COL 5.4.4 --

-- COL 5.4.5 --

-- COL 5.4.6 --

-- COL 5.4.7 --

-- COL 5.4.8 --

== ROW 5.5 ==
-- COL 5.5.0 --
G 
-- COL 5.5.1 --

-- COL 5.5.2 --
26
-- COL 5.5.3 --
(16) The placing on the market,  putting into service or use of certain  AI systems intended to distort  human behaviour, whereby physical 
-- COL 5.5.4 --
(16) The placing on the market,  putting into service or use of certain  AI systems intended to distortwith  the objective to or the effect of 
-- COL 5.5.5 --
(16) AI-enabled manipulative  techniques can be used to persuade  persons to engage in unwanted  behaviours, or to deceive them by 
-- COL 5.5.6 --
(16) AI-enabled manipulative  techniques can be used to persuade  persons to engage in unwanted  behaviours, or to deceive them by 
-- COL 5.5.7 --
G
-- COL 5.5.8 --


TTTXX TABLE: 6 XXTTT
== ROW 6.0 ==
-- COL 6.0.0 --

-- COL 6.0.1 --

-- COL 6.0.2 --

-- COL 6.0.3 --
Commission Proposal 
-- COL 6.0.4 --
EP Mandate 
-- COL 6.0.5 --
Council Mandate 
-- COL 6.0.6 --
Draft Agreement
-- COL 6.0.7 --

-- COL 6.0.8 --

== ROW 6.1 ==
-- COL 6.1.0 --

-- COL 6.1.1 --

-- COL 6.1.2 --

-- COL 6.1.3 --
or psychological harms are likely to  occur, should be forbidden. Such AI  systems deploy subliminal  
components individuals cannot  perceive or exploit vulnerabilities of  children and people due to their age,  physical or mental incapacities.  They do so with the intention to  materially distort the behaviour of a  person and in a manner that causes  or is likely to cause harm to that or  another person. The intention may  not be presumed if the distortion of  human behaviour results from  factors external to the AI system  which are outside of the control of  the provider or the user. Research  for legitimate purposes in relation to  such AI systems should not be  stifled by the prohibition, if such  research does not amount to use of  the AI system in human-machine  relations that exposes natural  
persons to harm and such research is  carried out in accordance with  recognised ethical standards for  scientific research.
-- COL 6.1.4 --
materially distorting human  
behaviour, whereby physical or  psychological harms are likely to  occur, should be forbidden. This  limitation should be understood to  include neuro-technologies assisted  by AI systems that are used to  monitor, use, or influence neural  data gathered through brain computer interfaces insofar as they  are materially distorting the  
behaviour of a natural person in a  manner that causes or is likely to  cause that person or another person  significant harm. Such AI systems  deploy subliminal components  individuals cannot perceive or  exploit vulnerabilities of children  and peopleindividuals and specific  groups of persons due to their  known or predicted personality  traits, age, physical or mental  incapacities, social or economic  situation. They do so with the  intention to or the effect of  
materially distortdistorting the  behaviour of a person and in a  manner that causes or is likely to  cause significant harm to that or  another person or groups of persons,  including harms that may be  accumulated over time. The  
intention to distort the behaviour  may not be presumed if the  
distortion of human behaviour  results from factors external to the  AI system which are outside of the 
-- COL 6.1.5 --
nudging them into decisions in a  way that subverts and impairs their  autonomy, decision-making and  free choices. The placing on the  market, putting into service or use of  certain AI systems intended to  distortmaterially distorting human  behaviour, whereby physical or  psychological harms are likely to  occur, are particularly dangerous  and should therefore be forbidden.  Such AI systems deploy subliminal  components individualssuch as  audio, image, video stimuli that  persons cannot perceive as those  stimuli are beyond human  
perception or other subliminal  techniques that subvert or impair  person’s autonomy, decision making or free choices in ways that  people are not consciously aware  of, or even if aware not able to  control or resist, for example in  cases of machine-brain interfaces  or virtual reality. In addition, AI  systems may also otherwise or  exploit vulnerabilities of children  and peoplea specific group of  persons due to their age, physical or  mental incapacities. They do  
sodisability within the meaning of  Directive (EU) 2019/882, or a  specific social or economic  
situation that is likely to make those  persons more vulnerable to  
exploitation such as persons living  in extreme poverty, ethnic or 
-- COL 6.1.6 --
nudging them into decisions in a  way that subverts and impairs their  autonomy, decision-making and  free choices. The placing on the  market, putting into service or use of  certain AI systems intended to  distortwith the objective to or the  effect of materially distorting human behaviour, whereby  
significant harms, in particular  having sufficiently important  adverse impacts on physical,  psychological health or financial  interests or psychological harms are  likely to occur, are particularly  dangerous and should therefore be  forbidden. Such AI systems deploy  subliminal components  
individualssuch as audio, image,  video stimuli that persons cannot  perceive as those stimuli are beyond  human perception or other  
manipulative or deceptive  
techniques that subvert or impair  person’s autonomy, decision making or free choices in ways that  people are not consciously aware  of, or even if aware they are still  deceived or not able to control or  resist. This could be for example,  facilitated by machine-brain  interfaces or virtual reality as they  allow for a higher degree of control  of what stimuli are presented to  persons, insofar as they may be  materially distorting their  
behaviour in a significantly 
-- COL 6.1.7 --

-- COL 6.1.8 --


TTTXX TABLE: 7 XXTTT
== ROW 7.0 ==
-- COL 7.0.0 --

-- COL 7.0.1 --

-- COL 7.0.2 --

-- COL 7.0.3 --
Commission Proposal 
-- COL 7.0.4 --
EP Mandate 
-- COL 7.0.5 --
Council Mandate 
-- COL 7.0.6 --
Draft Agreement
-- COL 7.0.7 --

-- COL 7.0.8 --

== ROW 7.1 ==
-- COL 7.1.0 --

-- COL 7.1.1 --

-- COL 7.1.2 --

-- COL 7.1.3 --

-- COL 7.1.4 --
control of the provider or the user,  such as factors that may not be  reasonably foreseen and mitigated  by the provider or the deployer of  the AI system. In any case, it is not  necessary for the provider or the  deployer to have the intention to  cause the significant harm, as long  as such harm results from the  manipulative or exploitative AI enabled practices. The prohibitions  for such AI practices is  
complementary to the provisions  contained in Directive 2005/29/EC,  according to which unfair  
commercial practices are  
prohibited, irrespective of whether  they carried out having recourse to  AI systems or otherwise. In such  setting, lawful commercial  
practices, for example in the field of  advertising, that are in compliance  with Union law should not in  themselves be regarded as violating  prohibition. Research for legitimate  purposes in relation to such AI  systems should not be stifled by the  prohibition, if such research does not  amount to use of the AI system in  human-machine relations that  exposes natural persons to harm and  such research is carried out in  accordance with recognised ethical  standards for scientific research and  on the basis of specific informed  consent of the individuals that are  exposed to them or, where 
-- COL 7.1.5 --
religious minorities. Such AI  systems can be placed on the  market, put into service or used with the intention to objective to or  the effect of materially  
distortdistorting the behaviour of a  person and in a manner that causes  or is reasonably likely to cause  physical or phycological harm to  that or another person or groups of  persons, including harms that may  be accumulated over time. The  intention to distort the behaviour  may not be presumed if the  
distortion of human behaviour results from factors external to the  AI system which are outside of the  control of the provider or the user,  meaning factors that may not be  reasonably foreseen and mitigated  by the provider or the user of the AI  system. In any case, it is not  
necessary for the provider or the  user to have the intention to cause  the physical or psychological harm,  as long as such harm results from  the manipulative or exploitative AI enabled practices. The prohibitions  for such AI practices are  
complementary to the provisions  contained in Directive 2005/29/EC,  notably that unfair commercial  practices leading to economic or  financial harms to consumers are  prohibited under all circumstances,  irrespective of whether they are put  in place through AI systems or 
-- COL 7.1.6 --
harmful manner.  
In addition, AI systems may also  otherwise or exploit vulnerabilities  of children and peoplea person or a  specific group of persons due to  their age, physical or mental  
incapacities. They do sodisability  within the meaning of Directive  (EU) 2019/882, or a specific social  or economic situation that is likely  to make those persons more  
vulnerable to exploitation such as  persons living in extreme poverty,  ethnic or religious minorities. Such  AI systems can be placed on the  market, put into service or used with the intention to objective to or  the effect of materially  
distortdistorting the behaviour of a  person and in a manner that causes  or is reasonably likely to cause  significant harm to that or another  person or groups of persons,  
including harms that may be  accumulated over time and should  therefore be prohibited. The  
intention to distort the behaviour  may not be presumed if the  
distortion of human behaviour results from factors external to the  AI system which are outside of the  control of the provider or the user.  Research for legitimate purposes in  relation to such AI systems should  not be stifled by the prohibition, if  such research does not amount to  use of the AI system in human-
-- COL 7.1.7 --

-- COL 7.1.8 --


TTTXX TABLE: 8 XXTTT
== ROW 8.0 ==
-- COL 8.0.0 --

-- COL 8.0.1 --

-- COL 8.0.2 --

-- COL 8.0.3 --
Commission Proposal 
-- COL 8.0.4 --
EP Mandate 
-- COL 8.0.5 --
Council Mandate 
-- COL 8.0.6 --
Draft Agreement
-- COL 8.0.7 --

-- COL 8.0.8 --

== ROW 8.1 ==
-- COL 8.1.0 --

-- COL 8.1.1 --

-- COL 8.1.2 --

-- COL 8.1.3 --

-- COL 8.1.4 --
applicable, of their legal guardian. 
-- COL 8.1.5 --
otherwise. The prohibitions of  manipulative and exploitative  practices in this Regulation should  not affect lawful practices in the  context of medical treatment such  as psychological treatment of a  mental disease or physical  
rehabilitation, when those practices  are. Research for legitimate  
purposes in relation to such AI  systems should not be stifled by the  prohibition, if such research does  not amount to use of the AI system in  human-machine relations that  exposes natural persons to harm and  such research is carried out in  accordance with recognised  
ethicalthe applicable medical standards for scientific researchand  legislation. In addition, common  and legitimate commercial practices  that are in compliance with the  applicable law should not in  
themselves be regarded as  
constituting harmful manipulative  AI practices.
-- COL 8.1.6 --
machine relations that exposes  natural persons to harm and such  research isdeployer, meaning  factors that may not be reasonably  foreseen and mitigated by the  provider or the deployer of the AI  system. In any case, it is not  
necessary for the provider or the  deployer to have the intention to  cause significant harm, as long as  such harm results from the  
manipulative or exploitative AI enabled practices. The prohibitions  for such AI practices are  
complementary to the provisions  contained in Directive 2005/29/EC,  notably unfair commercial  
practices leading to economic or  financial harms to consumers are  prohibited under all circumstances,  irrespective of whether they are put  in place through AI systems or  otherwise.  
The prohibitions of manipulative  and exploitative practices in this  Regulation should not affect lawful  practices in the context of medical  treatment such as psychological  treatment of a mental disease or  physical rehabilitation, when those  practices are carried out in  
accordance with recognised 
ethicalthe applicable legislation and  medical standards, for example  explicit consent of the individuals  or their legal representatives . In  addition, common and legitimate 
-- COL 8.1.7 --

-- COL 8.1.8 --


TTTXX TABLE: 9 XXTTT
== ROW 9.0 ==
-- COL 9.0.0 --

-- COL 9.0.1 --

-- COL 9.0.2 --

-- COL 9.0.3 --
Commission Proposal 
-- COL 9.0.4 --
EP Mandate 
-- COL 9.0.5 --
Council Mandate 
-- COL 9.0.6 --
Draft Agreement
-- COL 9.0.7 --

-- COL 9.0.8 --

== ROW 9.1 ==
-- COL 9.1.0 --

-- COL 9.1.1 --

-- COL 9.1.2 --

-- COL 9.1.3 --

-- COL 9.1.4 --

-- COL 9.1.5 --

-- COL 9.1.6 --
commercial practices, for example  in the field of advertising, that are  in compliance with the applicable  
law should not in themselves be  regarded as constituting harmful  manipulative AI practices for  scientific research.
-- COL 9.1.7 --

-- COL 9.1.8 --

== ROW 9.2 ==
-- COL 9.2.0 --

-- COL 9.2.1 --

-- COL 9.2.2 --
Recital 16a
-- COL 9.2.3 --

-- COL 9.2.4 --

-- COL 9.2.5 --

-- COL 9.2.6 --

-- COL 9.2.7 --

-- COL 9.2.8 --

== ROW 9.3 ==
-- COL 9.3.0 --
G 
-- COL 9.3.1 --

-- COL 9.3.2 --
26a
-- COL 9.3.3 --

-- COL 9.3.4 --
(16a) AI systems that categorise  natural persons by assigning them  to specific categories, according to  known or inferred sensitive or  protected characteristics are  particularly intrusive, violate  human dignity and hold great risk  of discrimination. Such  
characteristics include gender,  gender identity, race, ethnic origin,  migration or citizenship status,  political orientation, sexual  
orientation, religion, disability or  any other grounds on which  
discrimination is prohibited under  Article 21 of the Charter of  
Fundamental Rights of the  
European Union, as well as under  Article 9 of Regulation  
(EU)2016/769. Such systems should  therefore be prohibited. 
-- COL 9.3.5 --

-- COL 9.3.6 --
(16a) Biometric categorisation  systems that are based on  
individuals’ biometric data, such as  an individual person’s face or  fingerprint, to deduce or infer an  individuals’ political opinions, trade  union membership, religious or  philosophical beliefs, race, sex life  or sexual orientation should be  prohibited. This prohibition does  not cover the lawful labelling,  filtering or categorisation of  biometric datasets acquired in line  with Union or national law  
according to biometric data, such as  the sorting of images according to  hair colour or eye colour, which  can for example be used in the area  of law enforcement.
-- COL 9.3.7 --
G
-- COL 9.3.8 --

== ROW 9.4 ==
-- COL 9.4.0 --

-- COL 9.4.1 --

-- COL 9.4.2 --
Recital 17
-- COL 9.4.3 --

-- COL 9.4.4 --

-- COL 9.4.5 --

-- COL 9.4.6 --

-- COL 9.4.7 --

-- COL 9.4.8 --

== ROW 9.5 ==
-- COL 9.5.0 --
G 
-- COL 9.5.1 --

-- COL 9.5.2 --
27 
-- COL 9.5.3 --
(17) AI systems providing social 
-- COL 9.5.4 --
(17) AI systems providing social 
-- COL 9.5.5 --
(17) AI systems providing social 
-- COL 9.5.6 --
(17) AI systems providing social 
-- COL 9.5.7 --
G
-- COL 9.5.8 --


TTTXX TABLE: 10 XXTTT
== ROW 10.0 ==
-- COL 10.0.0 --

-- COL 10.0.1 --

-- COL 10.0.2 --

-- COL 10.0.3 --
Commission Proposal 
-- COL 10.0.4 --
EP Mandate 
-- COL 10.0.5 --
Council Mandate 
-- COL 10.0.6 --
Draft Agreement
-- COL 10.0.7 --

-- COL 10.0.8 --

== ROW 10.1 ==
-- COL 10.1.0 --

-- COL 10.1.1 --

-- COL 10.1.2 --

-- COL 10.1.3 --
scoring of natural persons for  general purpose by public authorities  or on their behalf may lead to  discriminatory outcomes and the  exclusion of certain groups. They  may violate the right to dignity and  non-discrimination and the values of  equality and justice. Such AI  
systems evaluate or classify the  trustworthiness of natural persons  based on their social behaviour in  multiple contexts or known or  predicted personal or personality  characteristics. The social score  obtained from such AI systems may  lead to the detrimental or  
unfavourable treatment of natural  persons or whole groups thereof in  social contexts, which are unrelated  to the context in which the data was  originally generated or collected or  to a detrimental treatment that is  disproportionate or unjustified to the  gravity of their social behaviour.  Such AI systems should be therefore  prohibited.
-- COL 10.1.4 --
scoring of natural persons for  general purpose by public authorities  or on their behalf may lead to  discriminatory outcomes and the  exclusion of certain groups. They  may violate the right to dignity and  non-discrimination and the values of  equality and justice. Such AI  
systems evaluate or classify the  trustworthiness of natural persons or  groups based on multiple data  points and time occurrences related  to their social behaviour in multiple  contexts or known, inferred or  predicted personal or personality  characteristics. The social score  obtained from such AI systems may  lead to the detrimental or  
unfavourable treatment of natural  persons or whole groups thereof in  social contexts, which are unrelated  to the context in which the data was  originally generated or collected or  to a detrimental treatment that is  disproportionate or unjustified to the  gravity of their social behaviour.  Such AI systems should be therefore  prohibited.
-- COL 10.1.5 --
scoring of natural persons for  general purpose by public  
authorities or on their behalfby  private actors may lead to  
discriminatory outcomes and the  exclusion of certain groups. They  may violate the right to dignity and  non-discrimination and the values of  equality and justice. Such AI  
systems evaluate or classify the  trustworthiness of natural persons  based on their social behaviour in  multiple contexts or known or  predicted personal or personality  characteristics. The social score  obtained from such AI systems may  lead to the detrimental or  
unfavourable treatment of natural  persons or whole groups thereof in  social contexts, which are unrelated  to the context in which the data was  originally generated or collected or  to a detrimental treatment that is  disproportionate or unjustified to the  gravity of their social behaviour. AI  systems entailing such  
unacceptable scoring practicesSuch  AI systems should be therefore  prohibited. This prohibition should  not affect lawful evaluation  
practices of natural persons done  for one or more specific purpose in  compliance with the law.
-- COL 10.1.6 --
scoring of natural persons for  general purpose by public 
authorities or on their behalf or  private actors may lead to  
discriminatory outcomes and the  exclusion of certain groups. They  may violate the right to dignity and  non-discrimination and the values of  equality and justice. Such AI  
systems evaluate or classify the  trustworthiness of natural  
personsnatural persons or groups  thereof based on multiple data  points related to their social  
behaviour in multiple contexts or  known, inferred or predicted  
personal or personality  
characteristics over certain periods  of time. The social score obtained  from such AI systems may lead to  the detrimental or unfavourable  treatment of natural persons or  whole groups thereof in social  contexts, which are unrelated to the  context in which the data was  originally generated or collected or  to a detrimental treatment that is  disproportionate or unjustified to the  gravity of their social behaviour. AI  systems entailing such  
unacceptable scoring practices  leading to such detrimental or  unfavorable outcomes Such AI  systems should be therefore  
prohibited. This prohibition should  not affect lawful evaluation  
practices of natural persons done 
-- COL 10.1.7 --

-- COL 10.1.8 --


TTTXX TABLE: 11 XXTTT
== ROW 11.0 ==
-- COL 11.0.0 --

-- COL 11.0.1 --

-- COL 11.0.2 --

-- COL 11.0.3 --
Commission Proposal 
-- COL 11.0.4 --
EP Mandate 
-- COL 11.0.5 --
Council Mandate 
-- COL 11.0.6 --
Draft Agreement
-- COL 11.0.7 --

-- COL 11.0.8 --

== ROW 11.1 ==
-- COL 11.1.0 --

-- COL 11.1.1 --

-- COL 11.1.2 --

-- COL 11.1.3 --

-- COL 11.1.4 --

-- COL 11.1.5 --

-- COL 11.1.6 --
for a specific purpose in  
compliance with national and  Union law.
-- COL 11.1.7 --

-- COL 11.1.8 --

== ROW 11.2 ==
-- COL 11.2.0 --

-- COL 11.2.1 --

-- COL 11.2.2 --
Recital 18
-- COL 11.2.3 --

-- COL 11.2.4 --

-- COL 11.2.5 --

-- COL 11.2.6 --

-- COL 11.2.7 --

-- COL 11.2.8 --

== ROW 11.3 ==
-- COL 11.3.0 --
G 
-- COL 11.3.1 --

-- COL 11.3.2 --
28
-- COL 11.3.3 --
(18) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement is  considered particularly intrusive in  the rights and freedoms of the  concerned persons, to the extent that  it may affect the private life of a  large part of the population, evoke a  feeling of constant surveillance and  indirectly dissuade the exercise of  the freedom of assembly and other  fundamental rights. In addition, the  immediacy of the impact and the limited opportunities for further  checks or corrections in relation to  the use of such systems operating in  ‘real-time’ carry heightened risks for  the rights and freedoms of the  persons that are concerned by law  enforcement activities.
-- COL 11.3.4 --
(18) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement is  considered is particularly intrusive  into the rights and freedoms of the  concerned persons, to the extent that it mayand can ultimately affect the  private life of a large part of the  population, evoke a feeling of  constant surveillance, give parties  deploying biometric identification  in publicly accessible spaces a  position of uncontrollable power and indirectly dissuade the exercise  of the freedom of assembly and  other fundamental rights at the core  to the Rule of Law. Technical  inaccuracies of AI systems intended  for the remote biometric  
identification of natural persons  can lead to biased results and entail  discriminatory effects. This is  particularly relevant when it comes  to age, ethnicity, sex or disabilities.  In addition, the immediacy of the  impact and the limited opportunities  for further checks or corrections in  relation to the use of such systems 
-- COL 11.3.5 --
(18) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement is  considered particularly intrusive in  the rights and freedoms of the  concerned persons, to the extent that  it may affect the private life of a  large part of the population, evoke a  feeling of constant surveillance and  indirectly dissuade the exercise of  the freedom of assembly and other  fundamental rights. In addition, the  immediacy of the impact and the  limited opportunities for further  checks or corrections in relation to  the use of such systems operating in  ‘real-time’ carry heightened risks for  the rights and freedoms of the  persons that are concerned by law  enforcement activities.
-- COL 11.3.6 --
(18) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement is considered particularly intrusive in  to the rights and freedoms of the  concerned persons, to the extent that  it may affect the private life of a  large part of the population, evoke a  feeling of constant surveillance and  indirectly dissuade the exercise of  the freedom of assembly and other  fundamental rights . 
Technical inaccuracies of AI  systems intended for the remote  biometric identification of natural  persons can lead to biased results  and entail discriminatory effects.  This is particularly relevant when it  comes to age, ethnicity, race, sex or  disabilities.  
In addition, the immediacy of the  impact and the limited opportunities  for further checks or corrections in  relation to the use of such systems  operating in ‘real-time’ carry  
heightened risks for the rights and  freedoms of the persons that are  concerned by law enforcement 
-- COL 11.3.7 --
G
-- COL 11.3.8 --


TTTXX TABLE: 12 XXTTT
== ROW 12.0 ==
-- COL 12.0.0 --

-- COL 12.0.1 --

-- COL 12.0.2 --

-- COL 12.0.3 --
Commission Proposal 
-- COL 12.0.4 --
EP Mandate 
-- COL 12.0.5 --
Council Mandate 
-- COL 12.0.6 --
Draft Agreement
-- COL 12.0.7 --

-- COL 12.0.8 --

== ROW 12.1 ==
-- COL 12.1.0 --

-- COL 12.1.1 --

-- COL 12.1.2 --

-- COL 12.1.3 --

-- COL 12.1.4 --
operating in ‘real-time’ carry  
heightened risks for the rights and  freedoms of the persons that are  concerned by law enforcement  activities. The use of those systems  in publicly accessible places should  therefore be prohibited. Similarly,  AI systems used for the analysis of  recorded footage of publicly  
accessible spaces through ‘post’  remote biometric identification  systems should also be prohibited,  unless there is pre-judicial  
authorisation for use in the context  of law enforcement, when strictly  necessary for the targeted search  connected to a specific serious  criminal offense that already took  place, and only subject to a pre judicial authorisation. 
-- COL 12.1.5 --

-- COL 12.1.6 --
activities.
-- COL 12.1.7 --

-- COL 12.1.8 --

== ROW 12.2 ==
-- COL 12.2.0 --

-- COL 12.2.1 --

-- COL 12.2.2 --
Recital 19
-- COL 12.2.3 --

-- COL 12.2.4 --

-- COL 12.2.5 --

-- COL 12.2.6 --

-- COL 12.2.7 --

-- COL 12.2.8 --

== ROW 12.3 ==
-- COL 12.3.0 --
G 
-- COL 12.3.1 --

-- COL 12.3.2 --
29
-- COL 12.3.3 --
(19) The use of those systems for  the purpose of law enforcement  should therefore be prohibited,  except in three exhaustively listed  and narrowly defined situations,  where the use is strictly necessary to  achieve a substantial public interest,  the importance of which outweighs  the risks. Those situations involve  the search for potential victims of  crime, including missing children;  certain threats to the life or physical  safety of natural persons or of a 
-- COL 12.3.4 --
deleted
-- COL 12.3.5 --
(19) The use of those systems for  the purpose of law enforcement  should therefore be prohibited,  except in three exhaustively listed  and narrowly defined situations,  where the use is strictly necessary to  achieve a substantial public interest,  the importance of which outweighs  the risks. Those situations involve  the search for potential victims of  crime, including missing children;  certain threats to the life or physical  safety of natural persons or of a 
-- COL 12.3.6 --
(19) The use of those systems for  the purpose of law enforcement  should therefore be prohibited,  except in three exhaustively listed  and narrowly defined situations,  where the use is strictly necessary to  achieve a substantial public interest,  the importance of which outweighs  the risks. Those situations involve  the search for potential certain  victims of crime, including missing  childrenpeople; certain threats to the  life or physical safety of natural 
-- COL 12.3.7 --
G
-- COL 12.3.8 --


TTTXX TABLE: 13 XXTTT
== ROW 13.0 ==
-- COL 13.0.0 --

-- COL 13.0.1 --

-- COL 13.0.2 --

-- COL 13.0.3 --
Commission Proposal 
-- COL 13.0.4 --
EP Mandate 
-- COL 13.0.5 --
Council Mandate 
-- COL 13.0.6 --
Draft Agreement
-- COL 13.0.7 --

-- COL 13.0.8 --

== ROW 13.1 ==
-- COL 13.1.0 --

-- COL 13.1.1 --

-- COL 13.1.2 --

-- COL 13.1.3 --
terrorist attack; and the detection,  localisation, identification or  
prosecution of perpetrators or  suspects of the criminal offences  referred to in Council Framework  Decision 2002/584/JHA1if those  criminal offences are punishable in  the Member State concerned by a  custodial sentence or a detention  order for a maximum period of at  least three years and as they are  defined in the law of that Member  State. Such threshold for the  
custodial sentence or detention order  in accordance with national law  contributes to ensure that the offence  should be serious enough to  
potentially justify the use of ‘real time’ remote biometric identification  systems. Moreover, of the 32  
criminal offences listed in the  Council Framework Decision  2002/584/JHA, some are in practice  likely to be more relevant than  others, in that the recourse to ‘real time’ remote biometric identification  will foreseeably be necessary and  proportionate to highly varying  degrees for the practical pursuit of  the detection, localisation,  
identification or prosecution of a  perpetrator or suspect of the  
different criminal offences listed and  having regard to the likely  
differences in the seriousness,  probability and scale of the harm or  possible negative consequences.
-- COL 13.1.4 --

-- COL 13.1.5 --
terrorist attack; and the detection,  localisation, identification or  
prosecution of perpetrators or  suspects of the criminal offences  referred to in Council Framework  Decision 2002/584/JHA1if those  criminal offences are punishable in  the Member State concerned by a  custodial sentence or a detention  order for a maximum period of at  least three years and as they are  defined in the law of that Member  State. Such threshold for the  
custodial sentence or detention order  in accordance with national law  contributes to ensure that the offence  should be serious enough to  
potentially justify the use of ‘real time’ remote biometric identification  systems. Moreover, of the 32  
criminal offences listed in the  Council Framework Decision  2002/584/JHA, some are in practice  likely to be more relevant than  others, in that the recourse to ‘real time’ remote biometric identification  will foreseeably be necessary and  proportionate to highly varying  degrees for the practical pursuit of  the detection, localisation,  
identification or prosecution of a  perpetrator or suspect of the  
different criminal offences listed and  having regard to the likely  
differences in the seriousness,  probability and scale of the harm or  possible negative consequences. In 
-- COL 13.1.6 --
persons or of a terrorist attack; and  the detection, localisation,  
identification or prosecution or  identification of perpetrators or  suspects of the criminal offences  
referred to in Council Framework  Decision 2002/584/JHA1 Annex IIa  if those criminal offences are 
punishable in the Member State  concerned by a custodial sentence or  a detention order for a maximum  period of at least three four years  and as they are defined in the law of  that Member State. Such threshold  for the custodial sentence or  
detention order in accordance with  national law contributes to ensure  that the offence should be serious  enough to potentially justify the use  of ‘real-time’ remote biometric  identification systems. Moreover,  the list of criminal offences as  referred in Annex IIa is based on of  the 32 criminal offences listed in the  Council Framework Decision  2002/584/JHA1, taking into account  that , some are in practice likely to  be more relevant than others, in that  the recourse to ‘real-time’ remote  biometric identification will  
foreseeably be necessary and  
proportionate to highly varying  degrees for the practical pursuit of  the detection, localisation,  
identification or prosecution or  identification of a perpetrator or  suspect of the different criminal 
-- COL 13.1.7 --

-- COL 13.1.8 --


TTTXX TABLE: 14 XXTTT
== ROW 14.0 ==
-- COL 14.0.0 --

-- COL 14.0.1 --

-- COL 14.0.2 --

-- COL 14.0.3 --
Commission Proposal 
-- COL 14.0.4 --
EP Mandate 
-- COL 14.0.5 --
Council Mandate 
-- COL 14.0.6 --
Draft Agreement
-- COL 14.0.7 --

-- COL 14.0.8 --

== ROW 14.1 ==
-- COL 14.1.0 --

-- COL 14.1.1 --

-- COL 14.1.2 --

-- COL 14.1.3 --
_________ 
1. Council Framework Decision  
2002/584/JHA of 13 June 2002 on the  European arrest warrant and the surrender  procedures between Member States (OJ L  190, 18.7.2002, p. 1).
-- COL 14.1.4 --

-- COL 14.1.5 --
addition, this Regulation should  preserve the ability for law  
enforcement, border control,  immigration or asylum authorities  to carry out identity checks in the  presence of the person that is  concerned in accordance with the  conditions set out in Union and  national law for such checks. In  particular, law enforcement, border  control, immigration or asylum  authorities should be able to use  information systems, in accordance  with Union or national law, to  identify a person who, during an  identity check, either refuses to be  identified or is unable to state or  prove his or her identity, without  being required by this Regulation to  obtain prior authorisation. This  could be, for example, a person  involved in a crime, being  
unwilling, or unable due to an  accident or a medical condition, to  disclose their identity to law  
enforcement authorities.  
_________ 
1. [1] Council Framework Decision  2002/584/JHA of 13 June 2002 on the  European arrest warrant and the surrender  procedures between Member States (OJ L  190, 18.7.2002, p. 1).
-- COL 14.1.6 --
offences listed and having regard to  the likely differences in the  
seriousness, probability and scale of  the harm or possible negative  consequences. 
An imminent threat to life or  physical safety of natural persons  could also result from a serious  disruption of critical infrastructure,  as defined in Article 2, point (a) of  Directive 2008/114/EC, where the  disruption or destruction of such  critical infrastructure would result  in an imminent threat to life or  physical safety of a person,  
including through serious harm to  the provision of basic supplies to  the population or to the exercise of  the core function of the State.  
In addition, this Regulation should  preserve the ability for law  
enforcement, border control,  immigration or asylum authorities  to carry out identity checks in the  presence of the person that is  concerned in accordance with the  conditions set out in Union and  national law for such checks. In  particular, law enforcement, border  control, immigration or asylum  authorities should be able to use  information systems, in accordance  with Union or national law, to  identify a person who, during an  identity check, either refuses to be 
-- COL 14.1.7 --

-- COL 14.1.8 --


TTTXX TABLE: 15 XXTTT
== ROW 15.0 ==
-- COL 15.0.0 --

-- COL 15.0.1 --

-- COL 15.0.2 --

-- COL 15.0.3 --
Commission Proposal 
-- COL 15.0.4 --
EP Mandate 
-- COL 15.0.5 --
Council Mandate 
-- COL 15.0.6 --
Draft Agreement
-- COL 15.0.7 --

-- COL 15.0.8 --

== ROW 15.1 ==
-- COL 15.1.0 --

-- COL 15.1.1 --

-- COL 15.1.2 --

-- COL 15.1.3 --

-- COL 15.1.4 --

-- COL 15.1.5 --

-- COL 15.1.6 --
identified or is unable to state or  prove his or her identity, without  being required by this Regulation to  obtain prior authorisation. This  could be, for example, a person  involved in a crime, being  
unwilling, or unable due to an  accident or a medical condition, to  disclose their identity to law  
enforcement authorities.  
_________ 
1. Council Framework Decision  
2002/584/JHA of 13 June 2002 on the  European arrest warrant and the surrender  procedures between Member States (OJ L  190, 18.7.2002, p. 1).
-- COL 15.1.7 --

-- COL 15.1.8 --

== ROW 15.2 ==
-- COL 15.2.0 --

-- COL 15.2.1 --

-- COL 15.2.2 --
Recital 20
-- COL 15.2.3 --

-- COL 15.2.4 --

-- COL 15.2.5 --

-- COL 15.2.6 --

-- COL 15.2.7 --

-- COL 15.2.8 --

== ROW 15.3 ==
-- COL 15.3.0 --
G 
-- COL 15.3.1 --

-- COL 15.3.2 --
30
-- COL 15.3.3 --
(20) In order to ensure that those  systems are used in a responsible  and proportionate manner, it is also  important to establish that, in each of  those three exhaustively listed and  narrowly defined situations, certain  elements should be taken into  account, in particular as regards the  nature of the situation giving rise to  the request and the consequences of  the use for the rights and freedoms  of all persons concerned and the  safeguards and conditions provided  for with the use. In addition, the use  of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for the purpose of  law enforcement should be subject 
-- COL 15.3.4 --
deleted
-- COL 15.3.5 --
(20) In order to ensure that those  systems are used in a responsible  and proportionate manner, it is also  important to establish that, in each of  those three exhaustively listed and  narrowly defined situations, certain  elements should be taken into  account, in particular as regards the  nature of the situation giving rise to  the request and the consequences of  the use for the rights and freedoms  of all persons concerned and the  safeguards and conditions provided  for with the use. In addition, the use  of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for the purpose of  law enforcement should be subject 
-- COL 15.3.6 --
(20) In order to ensure that those  systems are used in a responsible  and proportionate manner, it is also  important to establish that, in each of  those three exhaustively listed and  narrowly defined situations, certain  elements should be taken into  account, in particular as regards the  nature of the situation giving rise to  the request and the consequences of  the use for the rights and freedoms  of all persons concerned and the  safeguards and conditions provided  for with the use. In addition, the use  of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for the purpose of  law enforcement should only be 
-- COL 15.3.7 --
G
-- COL 15.3.8 --


TTTXX TABLE: 16 XXTTT
== ROW 16.0 ==
-- COL 16.0.0 --

-- COL 16.0.1 --

-- COL 16.0.2 --

-- COL 16.0.3 --
Commission Proposal 
-- COL 16.0.4 --
EP Mandate 
-- COL 16.0.5 --
Council Mandate 
-- COL 16.0.6 --
Draft Agreement
-- COL 16.0.7 --

-- COL 16.0.8 --

== ROW 16.1 ==
-- COL 16.1.0 --

-- COL 16.1.1 --

-- COL 16.1.2 --

-- COL 16.1.3 --
to appropriate limits in time and  space, having regard in particular to  the evidence or indications regarding  the threats, the victims or  
perpetrator. The reference database  of persons should be appropriate for  each use case in each of the three  situations mentioned above.
-- COL 16.1.4 --

-- COL 16.1.5 --
to appropriate limits in time and  space, having regard in particular to  the evidence or indications regarding  the threats, the victims or  
perpetrator. The reference database  of persons should be appropriate for  each use case in each of the three situations mentioned above.
-- COL 16.1.6 --
deployed to confirm the specifically  target individual’s identity and  should be limited to what is strictly  necessary concerning the period of  time as well as geographic and  personal scope be subject to  
appropriate limits in time and space,  having regard in particular to the  evidence or indications regarding the  threats, the victims or perpetrator.  The use of the ‘real-time’ remote  biometric identification system in  publicly accessible spaces should  only be authorised if the law  enforcement authority has  
completed a fundamental rights  impact assessment and, unless  provided otherwise in this  
Regulation, has registered the  system in the database as set out in  this Regulation. The reference  database of persons should be  appropriate for each use case in each  of the three situations mentioned  above.
-- COL 16.1.7 --

-- COL 16.1.8 --

== ROW 16.2 ==
-- COL 16.2.0 --

-- COL 16.2.1 --

-- COL 16.2.2 --
Recital 21
-- COL 16.2.3 --

-- COL 16.2.4 --

-- COL 16.2.5 --

-- COL 16.2.6 --

-- COL 16.2.7 --

-- COL 16.2.8 --

== ROW 16.3 ==
-- COL 16.3.0 --
G 
-- COL 16.3.1 --

-- COL 16.3.2 --
31
-- COL 16.3.3 --
(21) Each use of a ‘real-time’  remote biometric identification  system in publicly accessible spaces  for the purpose of law enforcement  should be subject to an express and  specific authorisation by a judicial  authority or by an independent  administrative authority of a 
-- COL 16.3.4 --
deleted
-- COL 16.3.5 --
(21) Each use of a ‘real-time’  remote biometric identification  system in publicly accessible spaces  for the purpose of law enforcement  should be subject to an express and  specific authorisation by a judicial  authority or by an independent  administrative authority of a 
-- COL 16.3.6 --
(21) Each use of a ‘real-time’  remote biometric identification  system in publicly accessible spaces  for the purpose of law enforcement  should be subject to an express and  specific authorisation by a judicial  authority or by an independent  administrative authority whose 
-- COL 16.3.7 --
G
-- COL 16.3.8 --


TTTXX TABLE: 17 XXTTT
== ROW 17.0 ==
-- COL 17.0.0 --

-- COL 17.0.1 --

-- COL 17.0.2 --

-- COL 17.0.3 --
Commission Proposal 
-- COL 17.0.4 --
EP Mandate 
-- COL 17.0.5 --
Council Mandate 
-- COL 17.0.6 --
Draft Agreement
-- COL 17.0.7 --

-- COL 17.0.8 --

== ROW 17.1 ==
-- COL 17.1.0 --

-- COL 17.1.1 --

-- COL 17.1.2 --

-- COL 17.1.3 --
Member State. Such authorisation  should in principle be obtained prior  to the use, except in duly justified  situations of urgency, that is,  
situations where the need to use the  systems in question is such as to  make it effectively and objectively  impossible to obtain an authorisation  before commencing the use. In such  situations of urgency, the use should  be restricted to the absolute  
minimum necessary and be subject  to appropriate safeguards and  conditions, as determined in national  law and specified in the context of  each individual urgent use case by  the law enforcement authority itself.  In addition, the law enforcement  authority should in such situations  seek to obtain an authorisation as  soon as possible, whilst providing  the reasons for not having been able  to request it earlier.
-- COL 17.1.4 --

-- COL 17.1.5 --
Member State. Such authorisation  should in principle be obtained prior  to the use, except of the system with  a view to identify a person or  persons. Exceptions to this rule  should be allowed in duly justified  situations of urgency, that is,  
situations where the need to use the  systems in question is such as to  make it effectively and objectively  impossible to obtain an authorisation  before commencing the use. In such  situations of urgency, the use should  be restricted to the absolute  
minimum necessary and be subject  to appropriate safeguards and  conditions, as determined in national  law and specified in the context of  each individual urgent use case by  the law enforcement authority itself.  In addition, the law enforcement  authority should in such situations  seek to obtain an authorisation as  soon as possible, whilst providing  the reasons for not having been able  to request it earlier.
-- COL 17.1.6 --
decision is binding of a Member  State. Such authorisation should in  principle be obtained prior to the  use, except of the system with a view  to identify a person or persons.  Exceptions to this rule should be  allowed in duly justified situations  of urgency, that is, situations where  the need to use the systems in  question is such as to make it  effectively and objectively  
impossible to obtain an authorisation  before commencing the use. In such  situations of urgency, the use should  be restricted to the absolute  
minimum necessary and be subject  to appropriate safeguards and  conditions, as determined in national  law and specified in the context of  each individual urgent use case by  the law enforcement authority itself.  In addition, the law enforcement  authority should in such situations seek to obtain an request such authorisation as soon as possible,  whilst providing the reasons for not  having been able to request it earlier,  without undue delay and, at the  latest within 24 hours.  
If such authorisation is rejected, the  use of real-time biometric  
identification systems linked to that  authorisation should be stopped  with immediate effect and all the  data related to such use should be  discarded and deleted. Such data  includes input data directly 
-- COL 17.1.7 --

-- COL 17.1.8 --


TTTXX TABLE: 18 XXTTT
== ROW 18.0 ==
-- COL 18.0.0 --

-- COL 18.0.1 --

-- COL 18.0.2 --

-- COL 18.0.3 --
Commission Proposal 
-- COL 18.0.4 --
EP Mandate 
-- COL 18.0.5 --
Council Mandate 
-- COL 18.0.6 --
Draft Agreement
-- COL 18.0.7 --

-- COL 18.0.8 --

== ROW 18.1 ==
-- COL 18.1.0 --

-- COL 18.1.1 --

-- COL 18.1.2 --

-- COL 18.1.3 --

-- COL 18.1.4 --

-- COL 18.1.5 --

-- COL 18.1.6 --
acquired by an AI system in the  course of the use of such system as  well as the results and outputs of  the use linked to that authorisation.  It should not include input legally  acquired in accordance with  another national or Union law. In  any case, no decision producing an  adverse legal effect on a person  may be taken solely based on the  output of the remote biometric  identification system.
-- COL 18.1.7 --

-- COL 18.1.8 --

== ROW 18.2 ==
-- COL 18.2.0 --

-- COL 18.2.1 --

-- COL 18.2.2 --
Recital 21a
-- COL 18.2.3 --

-- COL 18.2.4 --

-- COL 18.2.5 --

-- COL 18.2.6 --

-- COL 18.2.7 --

-- COL 18.2.8 --

== ROW 18.3 ==
-- COL 18.3.0 --
G 
-- COL 18.3.1 --

-- COL 18.3.2 --
31a
-- COL 18.3.3 --

-- COL 18.3.4 --

-- COL 18.3.5 --

-- COL 18.3.6 --
(21a) In order to carry out their  tasks in accordance with the  requirements set out in this  
Regulation as well as in national  rules, the relevant market  
surveillance authority and the  national data protection authority  should be notified of each use of  the ‘real-time biometric  
identification system’. National  market surveillance authorities and  the national data protection  
authorities that have been notified  should submit to the Commission  an annual report on the use of  ‘real-time biometric identification  systems’. 
-- COL 18.3.7 --
G
-- COL 18.3.8 --

== ROW 18.4 ==
-- COL 18.4.0 --

-- COL 18.4.1 --

-- COL 18.4.2 --
Recital 22
-- COL 18.4.3 --

-- COL 18.4.4 --

-- COL 18.4.5 --

-- COL 18.4.6 --

-- COL 18.4.7 --

-- COL 18.4.8 --

== ROW 18.5 ==
-- COL 18.5.0 --
G 
-- COL 18.5.1 --

-- COL 18.5.2 --
32 
-- COL 18.5.3 --

-- COL 18.5.4 --

-- COL 18.5.5 --

-- COL 18.5.6 --

-- COL 18.5.7 --
G
-- COL 18.5.8 --


TTTXX TABLE: 19 XXTTT
== ROW 19.0 ==
-- COL 19.0.0 --

-- COL 19.0.1 --

-- COL 19.0.2 --

-- COL 19.0.3 --
Commission Proposal 
-- COL 19.0.4 --
EP Mandate 
-- COL 19.0.5 --
Council Mandate 
-- COL 19.0.6 --
Draft Agreement
-- COL 19.0.7 --

-- COL 19.0.8 --

== ROW 19.1 ==
-- COL 19.1.0 --

-- COL 19.1.1 --

-- COL 19.1.2 --

-- COL 19.1.3 --
(22) Furthermore, it is appropriate  to provide, within the exhaustive  framework set by this Regulation  that such use in the territory of a  Member State in accordance with  this Regulation should only be  possible where and in as far as the  Member State in question has  decided to expressly provide for the  possibility to authorise such use in  its detailed rules of national law.  Consequently, Member States  remain free under this Regulation  not to provide for such a possibility  at all or to only provide for such a  possibility in respect of some of the  objectives capable of justifying  authorised use identified in this  Regulation.
-- COL 19.1.4 --
deleted
-- COL 19.1.5 --
(22) Furthermore, it is appropriate  to provide, within the exhaustive  framework set by this Regulation  that such use in the territory of a  Member State in accordance with  this Regulation should only be  possible where and in as far as the  Member State in question has  decided to expressly provide for the  possibility to authorise such use in  its detailed rules of national law.  Consequently, Member States  remain free under this Regulation  not to provide for such a possibility  at all or to only provide for such a  possibility in respect of some of the  objectives capable of justifying  authorised use identified in this  Regulation.
-- COL 19.1.6 --
(22) Furthermore, it is appropriate  to provide, within the exhaustive  framework set by this Regulation  that such use in the territory of a  Member State in accordance with  this Regulation should only be  possible where and in as far as the  Member State in question has  decided to expressly provide for the  possibility to authorise such use in  its detailed rules of national law.  Consequently, Member States  remain free under this Regulation  not to provide for such a possibility  at all or to only provide for such a  possibility in respect of some of the  objectives capable of justifying  authorised use identified in this  Regulation. These national rules  should be notified to the  
Commission at the latest 30 days  following their adoption. 
-- COL 19.1.7 --

-- COL 19.1.8 --

== ROW 19.2 ==
-- COL 19.2.0 --

-- COL 19.2.1 --

-- COL 19.2.2 --
Recital 23
-- COL 19.2.3 --

-- COL 19.2.4 --

-- COL 19.2.5 --

-- COL 19.2.6 --

-- COL 19.2.7 --

-- COL 19.2.8 --

== ROW 19.3 ==
-- COL 19.3.0 --
G 
-- COL 19.3.1 --

-- COL 19.3.2 --
33
-- COL 19.3.3 --
(23) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement  
necessarily involves the processing  of biometric data. The rules of this  Regulation that prohibit, subject to  certain exceptions, such use, which  
are based on Article 16 TFEU,  should apply as lex specialis in 
-- COL 19.3.4 --
deleted
-- COL 19.3.5 --
(23) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement  
necessarily involves the processing  of biometric data. The rules of this  Regulation that prohibit, subject to  certain exceptions, such use, which  
are based on Article 16 TFEU,  should apply as lex specialislex 
-- COL 19.3.6 --
(23) The use of AI systems for  ‘real-time’ remote biometric  
identification of natural persons in  publicly accessible spaces for the  purpose of law enforcement  
necessarily involves the processing  of biometric data. The rules of this  Regulation that prohibit, subject to  certain exceptions, such use, which  
are based on Article 16 TFEU,  should apply as lex specialis in 
-- COL 19.3.7 --
G
-- COL 19.3.8 --


TTTXX TABLE: 20 XXTTT
== ROW 20.0 ==
-- COL 20.0.0 --

-- COL 20.0.1 --

-- COL 20.0.2 --

-- COL 20.0.3 --
Commission Proposal 
-- COL 20.0.4 --
EP Mandate 
-- COL 20.0.5 --
Council Mandate 
-- COL 20.0.6 --
Draft Agreement
-- COL 20.0.7 --

-- COL 20.0.8 --

== ROW 20.1 ==
-- COL 20.1.0 --

-- COL 20.1.1 --

-- COL 20.1.2 --

-- COL 20.1.3 --
respect of the rules on the processing  of biometric data contained in  Article 10 of Directive (EU)  
2016/680, thus regulating such use  and the processing of biometric data  involved in an exhaustive manner.  Therefore, such use and processing  should only be possible in as far as it  is compatible with the framework set  by this Regulation, without there  being scope, outside that framework,  for the competent authorities, where  they act for purpose of law  
enforcement, to use such systems  and process such data in connection  thereto on the grounds listed in  Article 10 of Directive (EU)  
2016/680. In this context, this  Regulation is not intended to provide  the legal basis for the processing of  personal data under Article 8 of  Directive 2016/680. However, the  use of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for purposes other  than law enforcement, including by  competent authorities, should not be  covered by the specific framework  regarding such use for the purpose of  law enforcement set by this  
Regulation. Such use for purposes  other than law enforcement should  therefore not be subject to the  requirement of an authorisation  under this Regulation and the  applicable detailed rules of national  law that may give effect to it.
-- COL 20.1.4 --

-- COL 20.1.5 --
specialis in respect of the rules on  the processing of biometric data  contained in Article 10 of Directive  (EU) 2016/680, thus regulating such  use and the processing of biometric  data involved in an exhaustive  manner. Therefore, such use and  processing should only be possible  in as far as it is compatible with the  framework set by this Regulation,  without there being scope, outside  that framework, for the competent  authorities, where they act for  purpose of law enforcement, to use  such systems and process such data  in connection thereto on the grounds  listed in Article 10 of Directive (EU)  2016/680. In this context, this  Regulation is not intended to provide  the legal basis for the processing of  personal data under Article 8 of  Directive 2016/680. However, the  use of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for purposes other  than law enforcement, including by  competent authorities, should not be  covered by the specific framework  regarding such use for the purpose of  law enforcement set by this  
Regulation. Such use for purposes  other than law enforcement should  therefore not be subject to the  requirement of an authorisation  under this Regulation and the  applicable detailed rules of national  law that may give effect to it.
-- COL 20.1.6 --
respect of the rules on the processing  of biometric data contained in  Article 10 of Directive (EU)  
2016/680, thus regulating such use  and the processing of biometric data  involved in an exhaustive manner.  Therefore, such use and processing  should only be possible in as far as it  is compatible with the framework set  by this Regulation, without there  being scope, outside that framework,  for the competent authorities, where  they act for purpose of law  
enforcement, to use such systems  and process such data in connection  thereto on the grounds listed in  Article 10 of Directive (EU)  
2016/680. In this context, this  Regulation is not intended to provide  the legal basis for the processing of  personal data under Article 8 of  Directive 2016/680. However, the  use of ‘real-time’ remote biometric  identification systems in publicly  accessible spaces for purposes other  than law enforcement, including by  competent authorities, should not be  covered by the specific framework  regarding such use for the purpose of  law enforcement set by this  
Regulation. Such use for purposes  other than law enforcement should  therefore not be subject to the  requirement of an authorisation  under this Regulation and the  applicable detailed rules of national  law that may give effect to it.
-- COL 20.1.7 --

-- COL 20.1.8 --


TTTXX TABLE: 21 XXTTT
== ROW 21.0 ==
-- COL 21.0.0 --

-- COL 21.0.1 --

-- COL 21.0.2 --

-- COL 21.0.3 --
Commission Proposal 
-- COL 21.0.4 --
EP Mandate 
-- COL 21.0.5 --
Council Mandate 
-- COL 21.0.6 --
Draft Agreement
-- COL 21.0.7 --

-- COL 21.0.8 --

== ROW 21.1 ==
-- COL 21.1.0 --

-- COL 21.1.1 --

-- COL 21.1.2 --

-- COL 21.1.3 --

-- COL 21.1.4 --

-- COL 21.1.5 --

-- COL 21.1.6 --

-- COL 21.1.7 --

-- COL 21.1.8 --

== ROW 21.2 ==
-- COL 21.2.0 --

-- COL 21.2.1 --

-- COL 21.2.2 --
Recital 24
-- COL 21.2.3 --

-- COL 21.2.4 --

-- COL 21.2.5 --

-- COL 21.2.6 --

-- COL 21.2.7 --

-- COL 21.2.8 --

== ROW 21.3 ==
-- COL 21.3.0 --
G 
-- COL 21.3.1 --

-- COL 21.3.2 --
34
-- COL 21.3.3 --
(24) Any processing of biometric  data and other personal data  
involved in the use of AI systems for  biometric identification, other than  in connection to the use of ‘real time’ remote biometric identification  systems in publicly accessible spaces  for the purpose of law enforcement  as regulated by this Regulation,  including where those systems are  used by competent authorities in  publicly accessible spaces for other  purposes than law enforcement,  should continue to comply with all  requirements resulting from Article  9(1) of Regulation (EU) 2016/679,  Article 10(1) of Regulation (EU)  2018/1725 and Article 10 of  
Directive (EU) 2016/680, as  
applicable.
-- COL 21.3.4 --
(24) Any processing of biometric  data and other personal data  
involved in the use of AI systems for  biometric identification, other than  in connection to the use of ‘real time’ remote biometric identification  systems in publicly accessible spaces  for the purpose of law enforcement  as regulated by this Regulation,  including where those systems are  used by competent authorities in  publicly accessible spaces for other  purposes than law enforcement, should continue to comply with all  requirements resulting from Article  9(1) of Regulation (EU) 2016/679,  Article 10(1) of Regulation (EU)  2018/1725 and Article 10 of  
Directive (EU) 2016/680, as  
applicable.
-- COL 21.3.5 --
(24) Any processing of biometric  data and other personal data  
involved in the use of AI systems for  biometric identification, other than  in connection to the use of ‘real time’ remote biometric identification  systems in publicly accessible spaces  for the purpose of law enforcement  as regulated by this Regulation,  including where those systems are  used by competent authorities in  publicly accessible spaces for other  should continue to comply with all  requirements resulting from Article  10 of Directive (EU) 2016/680. For  purposes other than law  
enforcement, should continue to  comply with all requirements  resulting from Article 9(1) of  Regulation (EU) 2016/679, and Article 10(1) of Regulation (EU)  2018/1725 and Article 10 of  
Directive (EU) 2016/680, as  
applicableprohibit the processing of  biometric data for the purpose of  uniquely identifying a natural  person, unless one of the situations  in the respective second paragraphs  of those two articles applies.
-- COL 21.3.6 --
(24) Any processing of biometric  data and other personal data  
involved in the use of AI systems for  biometric identification, other than  in connection to the use of ‘real time’ remote biometric identification  systems in publicly accessible spaces  for the purpose of law enforcement  as regulated by this Regulation,  including where those systems are  used by competent authorities in  publicly accessible spaces for other  should continue to comply with all  requirements resulting from Article  10 of Directive (EU) 2016/680.  For purposes other than law  
enforcement, should continue to  comply with all requirements  resulting from Article 9(1) of  Regulation (EU) 2016/679, and Article 10(1) of Regulation (EU)  2018/1725 andprohibit the  
processing of biometric data subject  to limited exceptions as provided in  those articles. In application of Article 10 of Directive (EU)  
2016/680, as applicable9(1) of  Regulation (EU) 2016/679, the use  of remote biometric identification  for purposes other than law  
enforcement has already been  subject to prohibition decisions by  national data protection authorities.
-- COL 21.3.7 --
G
-- COL 21.3.8 --


TTTXX TABLE: 22 XXTTT
== ROW 22.0 ==
-- COL 22.0.0 --

-- COL 22.0.1 --

-- COL 22.0.2 --

-- COL 22.0.3 --
Commission Proposal 
-- COL 22.0.4 --
EP Mandate 
-- COL 22.0.5 --
Council Mandate 
-- COL 22.0.6 --
Draft Agreement
-- COL 22.0.7 --

-- COL 22.0.8 --

== ROW 22.1 ==
-- COL 22.1.0 --

-- COL 22.1.1 --

-- COL 22.1.2 --

-- COL 22.1.3 --

-- COL 22.1.4 --

-- COL 22.1.5 --

-- COL 22.1.6 --

-- COL 22.1.7 --

-- COL 22.1.8 --

== ROW 22.2 ==
-- COL 22.2.0 --

-- COL 22.2.1 --

-- COL 22.2.2 --
Recital 25
-- COL 22.2.3 --

-- COL 22.2.4 --

-- COL 22.2.5 --

-- COL 22.2.6 --

-- COL 22.2.7 --

-- COL 22.2.8 --

== ROW 22.3 ==
-- COL 22.3.0 --
G 
-- COL 22.3.1 --

-- COL 22.3.2 --
35
-- COL 22.3.3 --
(25) In accordance with Article 6a  of Protocol No 21 on the position of  the United Kingdom and Ireland in  respect of the area of freedom,  security and justice, as annexed to  the TEU and to the TFEU, Ireland is  not bound by the rules laid down in  Article 5(1), point (d), (2) and (3) of  this Regulation adopted on the basis  of Article 16 of the TFEU which  relate to the processing of personal  data by the Member States when  carrying out activities falling within  the scope of Chapter 4 or Chapter 5  of Title V of Part Three of the  TFEU, where Ireland is not bound  by the rules governing the forms of  judicial cooperation in criminal  matters or police cooperation which  require compliance with the  
provisions laid down on the basis of  Article 16 of the TFEU.
-- COL 22.3.4 --
(25) In accordance with Article 6a  of Protocol No 21 on the position of  the United Kingdom and Ireland in  respect of the area of freedom,  security and justice, as annexed to  the TEU and to the TFEU, Ireland is  not bound by the rules laid down in  Article 5(1), point (d), (2) and (3) of  this Regulation adopted on the basis  of Article 16 of the TFEU which  relate to the processing of personal  data by the Member States when  carrying out activities falling within  the scope of Chapter 4 or Chapter 5  of Title V of Part Three of the  TFEU, where Ireland is not bound  by the rules governing the forms of  judicial cooperation in criminal  matters or police cooperation which  require compliance with the  
provisions laid down on the basis of  Article 16 of the TFEU.
-- COL 22.3.5 --
(25) In accordance with Article 6a  of Protocol No 21 on the position of  the United Kingdom and Ireland in  respect of the area of freedom,  security and justice, as annexed to  the TEU and to the TFEU, Ireland is  not bound by the rules laid down in  Article 5(1), point (d), (2), (3) and  (4) and (3) of this Regulation  adopted on the basis of Article 16 of  the TFEU which relate to the  
processing of personal data by the  Member States when carrying out  activities falling within the scope of  Chapter 4 or Chapter 5 of Title V of  Part Three of the TFEU, where  Ireland is not bound by the rules  governing the forms of judicial  cooperation in criminal matters or  police cooperation which require  compliance with the provisions laid  down on the basis of Article 16 of  the TFEU.
-- COL 22.3.6 --
(25) In accordance with Article 6a  of Protocol No 21 on the position of  the United Kingdom and Ireland in  respect of the area of freedom,  security and justice, as annexed to  the TEU and to the TFEU, Ireland is  not bound by the rules laid down in  Article 5(1), point (d), (2), (3), (3a),  (4) and (5), Article 5(1)(ba) to the  extent it applies to the use of  biometric categorisation systems for  activities in the field of police  cooperation and judicial  
cooperation in criminal matters,  Article 5(1)(da) to the extent it  applies to the use of AI systems  covered by that provision and  Article 29(6a) and (3) of this  Regulation adopted on the basis of  Article 16 of the TFEU which relate  to the processing of personal data by  the Member States when carrying  out activities falling within the scope  of Chapter 4 or Chapter 5 of Title V  of Part Three of the TFEU, where  Ireland is not bound by the rules  governing the forms of judicial  cooperation in criminal matters or  police cooperation which require  compliance with the provisions laid  down on the basis of Article 16 of  the TFEU.
-- COL 22.3.7 --
G
-- COL 22.3.8 --


TTTXX TABLE: 23 XXTTT
== ROW 23.0 ==
-- COL 23.0.0 --

-- COL 23.0.1 --

-- COL 23.0.2 --

-- COL 23.0.3 --
Commission Proposal 
-- COL 23.0.4 --
EP Mandate 
-- COL 23.0.5 --
Council Mandate 
-- COL 23.0.6 --
Draft Agreement
-- COL 23.0.7 --

-- COL 23.0.8 --

== ROW 23.1 ==
-- COL 23.1.0 --

-- COL 23.1.1 --

-- COL 23.1.2 --
Recital 26
-- COL 23.1.3 --

-- COL 23.1.4 --

-- COL 23.1.5 --

-- COL 23.1.6 --

-- COL 23.1.7 --

-- COL 23.1.8 --

== ROW 23.2 ==
-- COL 23.2.0 --
G 
-- COL 23.2.1 --

-- COL 23.2.2 --
36
-- COL 23.2.3 --
(26) In accordance with Articles 2  and 2a of Protocol No 22 on the  position of Denmark, annexed to the  TEU and TFEU, Denmark is not  bound by rules laid down in Article  5(1), point (d), (2) and (3) of this  Regulation adopted on the basis of  Article 16 of the TFEU, or subject to  their application, which relate to the  processing of personal data by the  Member States when carrying out  activities falling within the scope of  Chapter 4 or Chapter 5 of Title V of  Part Three of the TFEU.
-- COL 23.2.4 --
(26) In accordance with Articles 2  and 2a of Protocol No 22 on the  position of Denmark, annexed to the  TEU and TFEU, Denmark is not  bound by rules laid down in Article  5(1), point (d), (2) and (3) of this  Regulation adopted on the basis of  Article 16 of the TFEU, or subject to  their application, which relate to the  processing of personal data by the  Member States when carrying out  activities falling within the scope of  Chapter 4 or Chapter 5 of Title V of  Part Three of the TFEU.
-- COL 23.2.5 --
(26) In accordance with Articles 2  and 2a of Protocol No 22 on the  position of Denmark, annexed to the  TEU and TFEU, Denmark is not  bound by rules laid down in Article  5(1), point (d), (2), (3) and (4) and  (3) of this Regulation adopted on the  basis of Article 16 of the TFEU, or  subject to their application, which  relate to the processing of personal  data by the Member States when  carrying out activities falling within  the scope of Chapter 4 or Chapter 5  of Title V of Part Three of the  TFEU.
-- COL 23.2.6 --
(26) In accordance with Articles 2  and 2a of Protocol No 22 on the  position of Denmark, annexed to the  TEU and TFEU, Denmark is not  bound by rules laid down in Article  5(1), point (d), (2), (3), (3a), (4) and  (5), Article 5(1)(ba) to the extent it  applies to the use of biometric  categorisation systems for activities  in the field of police cooperation  and judicial cooperation in criminal  matters, Article 5(1)(da) to the  extent it applies to the use of AI  systems covered by that provision  and Article 29(6a) and (3) of this  Regulation adopted on the basis of  Article 16 of the TFEU, or subject to  their application, which relate to the  processing of personal data by the  Member States when carrying out  activities falling within the scope of  Chapter 4 or Chapter 5 of Title V of  Part Three of the TFEU.
-- COL 23.2.7 --
G
-- COL 23.2.8 --

== ROW 23.3 ==
-- COL 23.3.0 --

-- COL 23.3.1 --

-- COL 23.3.2 --
Recital 26a
-- COL 23.3.3 --

-- COL 23.3.4 --

-- COL 23.3.5 --

-- COL 23.3.6 --

-- COL 23.3.7 --

-- COL 23.3.8 --

== ROW 23.4 ==
-- COL 23.4.0 --
G 
-- COL 23.4.1 --

-- COL 23.4.2 --
36a
-- COL 23.4.3 --

-- COL 23.4.4 --
(26a) AI systems used by law  enforcement authorities or on their  behalf to make predictions, profiles  or risk assessments based on  profiling of natural persons or data  analysis based on personality traits  and characteristics, including the  person’s location, or past criminal 
-- COL 23.4.5 --

-- COL 23.4.6 --
(26a) In line with the presumption  of innocence, natural persons in the  EU should always be judged on  their actual behaviour. Natural  persons should never be judged on  AI-predicted behaviour based solely  on their profiling, personality traits  or characteristics, such as 
-- COL 23.4.7 --
G
-- COL 23.4.8 --


TTTXX TABLE: 24 XXTTT
== ROW 24.0 ==
-- COL 24.0.0 --

-- COL 24.0.1 --

-- COL 24.0.2 --

-- COL 24.0.3 --
Commission Proposal 
-- COL 24.0.4 --
EP Mandate 
-- COL 24.0.5 --
Council Mandate 
-- COL 24.0.6 --
Draft Agreement
-- COL 24.0.7 --

-- COL 24.0.8 --

== ROW 24.1 ==
-- COL 24.1.0 --

-- COL 24.1.1 --

-- COL 24.1.2 --

-- COL 24.1.3 --

-- COL 24.1.4 --
behaviour of natural persons or  groups of persons for the purpose  of predicting the occurrence or  reoccurrence of an actual or  potential criminal offence(s) or  other criminalised social behaviour  or administrative offences,  
including fraud-predicition systems,  hold a particular risk of  
discrimination against certain  persons or groups of persons, as  they violate human dignity as well  as the key legal principle of  
presumption of innocence. Such AI  systems should therefore be  
prohibited. 
-- COL 24.1.5 --

-- COL 24.1.6 --
nationality, place of birth, place of  residence, number of children, debt,  their type of car, without a  
reasonable suspicion of that person  being involved in a criminal activity  based on objective verifiable facts  and without human assessment  thereof. Therefore, risk assessments  of natural persons in order to assess  the risk of them offending or for  predicting the occurrence of an  actual or potential criminal offence  solely based on the profiling of a  natural person or on assessing their  personality traits and  
characteristics should be  
prohibited. In any case, this  
prohibition does not refer to nor  touch upon risk analytics that are  not based on the profiling of  individuals or on the personality  traits and characteristics of  
individuals, such as AI systems  using risk analytics to assess the  risk of financial fraud by  
undertakings based on suspicious  transactions or risk analytic tools to  predict the likelihood of localisation  of narcotics or illicit goods by  customs authorities, for example  based on known trafficking routes.
-- COL 24.1.7 --

-- COL 24.1.8 --

== ROW 24.2 ==
-- COL 24.2.0 --

-- COL 24.2.1 --

-- COL 24.2.2 --
Recital 26b
-- COL 24.2.3 --

-- COL 24.2.4 --

-- COL 24.2.5 --

-- COL 24.2.6 --

-- COL 24.2.7 --

-- COL 24.2.8 --

== ROW 24.3 ==
-- COL 24.3.0 --
G 
-- COL 24.3.1 --

-- COL 24.3.2 --
36b 
-- COL 24.3.3 --

-- COL 24.3.4 --
(26b) The indiscriminate and  untargeted scraping of biometric 
-- COL 24.3.5 --

-- COL 24.3.6 --
(26b) The placing on the market,  putting into service for this specific 
-- COL 24.3.7 --
G
-- COL 24.3.8 --


TTTXX TABLE: 25 XXTTT
== ROW 25.0 ==
-- COL 25.0.0 --

-- COL 25.0.1 --

-- COL 25.0.2 --

-- COL 25.0.3 --
Commission Proposal 
-- COL 25.0.4 --
EP Mandate 
-- COL 25.0.5 --
Council Mandate 
-- COL 25.0.6 --
Draft Agreement
-- COL 25.0.7 --

-- COL 25.0.8 --

== ROW 25.1 ==
-- COL 25.1.0 --

-- COL 25.1.1 --

-- COL 25.1.2 --

-- COL 25.1.3 --

-- COL 25.1.4 --
data from social media or CCTV  footage to create or expand facial  recognition databases add to the  feeling of mass surveillance and  
can lead to gross violations of  fundamental rights, including the  right to privacy. The use of AI  systems with this intended purpose  should therefore be prohibited. 
-- COL 25.1.5 --

-- COL 25.1.6 --
purpose, or use of AI systems that  create or expand facial recognition  databases through the untargeted  scraping of facial images from the  internet or CCTV footage should be  prohibited, as this practice adds to  the feeling of mass surveillance and  can lead to gross violations of  fundamental rights, including the  right to privacy.
-- COL 25.1.7 --

-- COL 25.1.8 --

== ROW 25.2 ==
-- COL 25.2.0 --

-- COL 25.2.1 --

-- COL 25.2.2 --
Recital 26c
-- COL 25.2.3 --

-- COL 25.2.4 --

-- COL 25.2.5 --

-- COL 25.2.6 --

-- COL 25.2.7 --

-- COL 25.2.8 --

== ROW 25.3 ==
-- COL 25.3.0 --
G 
-- COL 25.3.1 --

-- COL 25.3.2 --
36c
-- COL 25.3.3 --

-- COL 25.3.4 --
(26c) There are serious concerns  about the scientific basis of AI  systems aiming to detect emotions,  physical or physiological features  such as facial expressions,  
movements, pulse frequency or  voice. Emotions or expressions of  emotions and perceptions thereof  vary considerably across cultures  and situations, and even within a  single individual. Among the key  shortcomings of such technologies,  are the limited reliability (emotion  categories are neither reliably  expressed through, nor  
unequivocally associated with, a  common set of physical or  
physiological movements), the lack  of specificity (physical or  
physiological expressions do not  perfectly match emotion categories)  and the limited generalisability (the  effects of context and culture are 
-- COL 25.3.5 --

-- COL 25.3.6 --
(26c) There are serious concerns  about the scientific basis of AI  systems aiming to identify or infer  emotions, particularly as expression  of emotions vary considerably  across cultures and situations, and  even within a single individual.  Among the key shortcomings of  such systems are the limited  
reliability, the lack of specificity  and the limited generalizability.  Therefore, AI systems identifying or  inferring emotions or intentions of  natural persons on the basis of their  biometric data may lead to  
discriminatory outcomes and can be  intrusive to the rights and freedoms  of the concerned persons.  
Considering the imbalance of  power in the context of work or  education, combined with the  intrusive nature of these systems,  such systems could lead to 
-- COL 25.3.7 --
G
-- COL 25.3.8 --


TTTXX TABLE: 26 XXTTT
== ROW 26.0 ==
-- COL 26.0.0 --

-- COL 26.0.1 --

-- COL 26.0.2 --

-- COL 26.0.3 --
Commission Proposal 
-- COL 26.0.4 --
EP Mandate 
-- COL 26.0.5 --
Council Mandate 
-- COL 26.0.6 --
Draft Agreement
-- COL 26.0.7 --

-- COL 26.0.8 --

== ROW 26.1 ==
-- COL 26.1.0 --

-- COL 26.1.1 --

-- COL 26.1.2 --

-- COL 26.1.3 --

-- COL 26.1.4 --
not sufficiently considered).  
Reliability issues and consequently,  major risks for abuse, may  
especially arise when deploying the  system in real-life situations related  to law enforcement, border  
management, workplace and  education institutions. Therefore,  the placing on the market, putting  into service, or use of AI systems  intended to be used in these  
contexts to detect the emotional  state of individuals should be  prohibited. 
-- COL 26.1.5 --

-- COL 26.1.6 --
detrimental or unfavourable  treatment of certain natural persons  or whole groups thereof. Therefore,  the placing on the market, putting  into service, or use of AI systems  intended to be used to detect the  emotional state of individuals in  situations related to the workplace  and education should be prohibited.  This prohibition should not cover  AI systems placed on the market  strictly for medical or safety  
reasons, such as systems intended  for therapeutical use. 
-- COL 26.1.7 --

-- COL 26.1.8 --

== ROW 26.2 ==
-- COL 26.2.0 --

-- COL 26.2.1 --

-- COL 26.2.2 --
Recital 26d
-- COL 26.2.3 --

-- COL 26.2.4 --

-- COL 26.2.5 --

-- COL 26.2.6 --

-- COL 26.2.7 --

-- COL 26.2.8 --

== ROW 26.3 ==
-- COL 26.3.0 --
G 
-- COL 26.3.1 --

-- COL 26.3.2 --
36d
-- COL 26.3.3 --

-- COL 26.3.4 --
(26d) Practices that are prohibited  by Union legislation, including data  protection law, non-discrimination  law, consumer protection law, and  competition law, should not be  affected by this Regulation. 
-- COL 26.3.5 --

-- COL 26.3.6 --
(26d) Practices that are prohibited  by Union legislation, including data  protection law, non-discrimination  law, consumer protection law, and  competition law, should not be  affected by this Regulation.
-- COL 26.3.7 --
G
-- COL 26.3.8 --

== ROW 26.4 ==
-- COL 26.4.0 --

-- COL 26.4.1 --

-- COL 26.4.2 --
Recital 27
-- COL 26.4.3 --

-- COL 26.4.4 --

-- COL 26.4.5 --

-- COL 26.4.6 --

-- COL 26.4.7 --

-- COL 26.4.8 --

== ROW 26.5 ==
-- COL 26.5.0 --
G 
-- COL 26.5.1 --

-- COL 26.5.2 --
37
-- COL 26.5.3 --
(27) High-risk AI systems should  only be placed on the Union market  or put into service if they comply  with certain mandatory  
requirements. Those requirements  should ensure that high-risk AI  systems available in the Union or  whose output is otherwise used in  the Union do not pose unacceptable 
-- COL 26.5.4 --
(27) High-risk AI systems should  only be placed on the Union market or, put into service or used if they  
comply with certain mandatory  requirements. Those requirements  should ensure that high-risk AI  systems available in the Union or  whose output is otherwise used in  the Union do not pose unacceptable 
-- COL 26.5.5 --
(27) High-risk AI systems should  only be placed on the Union market  or put into service if they comply  with certain mandatory  
requirements. Those requirements  should ensure that high-risk AI  systems available in the Union or  whose output is otherwise used in  the Union do not pose unacceptable 
-- COL 26.5.6 --
(27) High-risk AI systems should  only be placed on the Union market or, put into service or used if they  
comply with certain mandatory  requirements. Those requirements  should ensure that high-risk AI  systems available in the Union or  whose output is otherwise used in  the Union do not pose unacceptable 
-- COL 26.5.7 --
G
-- COL 26.5.8 --


TTTXX TABLE: 27 XXTTT
== ROW 27.0 ==
-- COL 27.0.0 --

-- COL 27.0.1 --

-- COL 27.0.2 --

-- COL 27.0.3 --
Commission Proposal 
-- COL 27.0.4 --
EP Mandate 
-- COL 27.0.5 --
Council Mandate 
-- COL 27.0.6 --
Draft Agreement
-- COL 27.0.7 --

-- COL 27.0.8 --

== ROW 27.1 ==
-- COL 27.1.0 --

-- COL 27.1.1 --

-- COL 27.1.2 --

-- COL 27.1.3 --
risks to important Union public  interests as recognised and protected  by Union law. AI systems identified  as high-risk should be limited to  those that have a significant harmful  impact on the health, safety and  fundamental rights of persons in the  Union and such limitation minimises  any potential restriction to  
international trade, if any.
-- COL 27.1.4 --
risks to important Union public  interests as recognised and protected  by Union law, including  
fundamental rights, democracy, the  rule or law or the environment. In  order to ensure alignment with  sectoral legislation and avoid  duplications, requirements for  high-risk AI systems should take  into account sectoral legislation  laying down requirements for high risk AI systems included in the  scope of this Regulation, such as  Regulation (EU) 2017/745 on  Medical Devices and Regulation  (EU) 2017/746 on In Vitro  
Diagnostic Devices or Directive  2006/42/EC on Machinery. AI  systems identified as high-risk  should be limited to those that have  a significant harmful impact on the  health, safety and fundamental rights  of persons in the Union and such  limitation minimises any potential  restriction to international trade, if  any. Given the rapid pace of  
technological development, as well  as the potential changes in the use  of AI systems, the list of high-risk  
areas and use-cases in Annex III  should nonetheless be subject to  permanent review through the  exercise of regular assessment. 
-- COL 27.1.5 --
risks to important Union public  interests as recognised and protected  by Union law. AI systems identified  as high-risk should be limited to  those that have a significant harmful  impact on the health, safety and  fundamental rights of persons in the  Union and such limitation minimises  any potential restriction to  
international trade, if any.
-- COL 27.1.6 --
risks to important Union public  interests as recognised and protected  by Union law. Following the New  Legislative Framework approach,  as clarified in Commission notice  the ‘Blue Guide’ on the  
implementation of EU product rules  2022 (C/2022/3637) the general  rule is that several pieces of the EU  legislation, such as Regulation  (EU) 2017/745 on Medical Devices  and Regulation (EU) 2017/746 on  In Vitro Diagnostic Devices or  Directive 2006/42/EC on  
Machinery, may have to be taken  into consideration for one product,  since the making available or  putting into service can only take  place when the product complies  with all applicable Union  
harmonisation legislation. To  ensure consistency and avoid  unnecessary administrative burden  or costs, providers of a product that  contains one or more high-risk  artificial intelligence system, to  which the requirements of this  Regulation as well as requirements  of the Union harmonisation  
legislation listed in Annex II,  Section A apply, should have a  flexibility on operational decisions  on how to ensure compliance of a  product that contains one or more  artificial intelligence systems with  all applicable requirements of the  Union harmonised legislation in a 
-- COL 27.1.7 --

-- COL 27.1.8 --


TTTXX TABLE: 28 XXTTT
== ROW 28.0 ==
-- COL 28.0.0 --

-- COL 28.0.1 --

-- COL 28.0.2 --

-- COL 28.0.3 --
Commission Proposal 
-- COL 28.0.4 --
EP Mandate 
-- COL 28.0.5 --
Council Mandate 
-- COL 28.0.6 --
Draft Agreement
-- COL 28.0.7 --

-- COL 28.0.8 --

== ROW 28.1 ==
-- COL 28.1.0 --

-- COL 28.1.1 --

-- COL 28.1.2 --

-- COL 28.1.3 --

-- COL 28.1.4 --

-- COL 28.1.5 --

-- COL 28.1.6 --
best way. AI systems identified as  high-risk should be limited to those  that have a significant harmful  impact on the health, safety and  fundamental rights of persons in the  Union and such limitation minimises  any potential restriction to  
international trade, if any.
-- COL 28.1.7 --

-- COL 28.1.8 --

== ROW 28.2 ==
-- COL 28.2.0 --

-- COL 28.2.1 --

-- COL 28.2.2 --
Recital 28
-- COL 28.2.3 --

-- COL 28.2.4 --

-- COL 28.2.5 --

-- COL 28.2.6 --

-- COL 28.2.7 --

-- COL 28.2.8 --

== ROW 28.3 ==
-- COL 28.3.0 --
G 
-- COL 28.3.1 --

-- COL 28.3.2 --
38
-- COL 28.3.3 --
(28) AI systems could produce  adverse outcomes to health and  safety of persons, in particular when  such systems operate as components  of products. Consistently with the  objectives of Union harmonisation  legislation to facilitate the free  movement of products in the internal  market and to ensure that only safe  and otherwise compliant products  find their way into the market, it is  important that the safety risks that  may be generated by a product as a  whole due to its digital components,  including AI systems, are duly  prevented and mitigated. For  
instance, increasingly autonomous  robots, whether in the context of  manufacturing or personal assistance  and care should be able to safely  operate and performs their functions  in complex environments. Similarly,  in the health sector where the stakes  for life and health are particularly  high, increasingly sophisticated 
-- COL 28.3.4 --
(28) AI systems could producehave  an adverse outcomesimpact to health  and safety of persons, in particular  when such systems operate as safety components of products.  
Consistently with the objectives of  Union harmonisation legislation to  facilitate the free movement of  products in the internal market and  to ensure that only safe and  
otherwise compliant products find  their way into the market, it is  important that the safety risks that  may be generated by a product as a  whole due to its digital components,  including AI systems, are duly  prevented and mitigated. For  
instance, increasingly autonomous  robots, whether in the context of  manufacturing or personal assistance  and care should be able to safely  operate and performs their functions  in complex environments. Similarly,  in the health sector where the stakes  for life and health are particularly 
-- COL 28.3.5 --
(28) AI systems could produce  adverse outcomes to health and  safety of persons, in particular when  such systems operate as components  of products. Consistently with the  objectives of Union harmonisation  legislation to facilitate the free  movement of products in the internal  market and to ensure that only safe  and otherwise compliant products  find their way into the market, it is  important that the safety risks that  may be generated by a product as a  whole due to its digital components,  including AI systems, are duly  prevented and mitigated. For  
instance, increasingly autonomous  robots, whether in the context of  manufacturing or personal assistance  and care should be able to safely  operate and performs their functions  in complex environments. Similarly,  in the health sector where the stakes  for life and health are particularly  high, increasingly sophisticated 
-- COL 28.3.6 --
(28) AI systems could producehave  an adverse outcomesimpact to health  and safety of persons, in particular  when such systems operate as safety components of products.  
Consistently with the objectives of  Union harmonisation legislation to  facilitate the free movement of  products in the internal market and  to ensure that only safe and  
otherwise compliant products find  their way into the market, it is  important that the safety risks that  may be generated by a product as a  whole due to its digital components,  including AI systems, are duly  prevented and mitigated. For  
instance, increasingly autonomous  robots, whether in the context of  manufacturing or personal assistance  and care should be able to safely  operate and performs their functions  in complex environments. Similarly,  in the health sector where the stakes  for life and health are particularly 
-- COL 28.3.7 --
G
-- COL 28.3.8 --


TTTXX TABLE: 29 XXTTT
== ROW 29.0 ==
-- COL 29.0.0 --

-- COL 29.0.1 --

-- COL 29.0.2 --

-- COL 29.0.3 --
Commission Proposal 
-- COL 29.0.4 --
EP Mandate 
-- COL 29.0.5 --
Council Mandate 
-- COL 29.0.6 --
Draft Agreement
-- COL 29.0.7 --

-- COL 29.0.8 --

== ROW 29.1 ==
-- COL 29.1.0 --

-- COL 29.1.1 --

-- COL 29.1.2 --

-- COL 29.1.3 --
diagnostics systems and systems  supporting human decisions should  be reliable and accurate. The extent  of the adverse impact caused by the  AI system on the fundamental rights  protected by the Charter is of  particular relevance when  
classifying an AI system as high risk. Those rights include the right to  human dignity, respect for private  and family life, protection of  
personal data, freedom of expression  and information, freedom of  
assembly and of association, and  non-discrimination, consumer  protection, workers’ rights, rights of  persons with disabilities, right to an  effective remedy and to a fair trial,  right of defence and the presumption  of innocence, right to good  
administration. In addition to those  rights, it is important to highlight  that children have specific rights as  enshrined in Article 24 of the EU  Charter and in the United Nations  Convention on the Rights of the  Child (further elaborated in the  UNCRC General Comment No. 25  as regards the digital environment),  both of which require consideration  of the children’s vulnerabilities and  provision of such protection and care  as necessary for their well-being.  The fundamental right to a high level  of environmental protection  
enshrined in the Charter and  
implemented in Union policies 
-- COL 29.1.4 --
high, increasingly sophisticated  diagnostics systems and systems  supporting human decisions should  be reliable and accurate. The extent  of the adverse impact caused by the  AI system on the fundamental rights  protected by the Charter is of  particular relevance when  
classifying an AI system as high-risk.  Those rights include the right to  human dignity, respect for private  and family life, protection of  
personal data, freedom of expression  and information, freedom of  
assembly and of association, and  non-discrimination, consumer  protection, workers’ rights, rights of  persons with disabilities, right to an  effective remedy and to a fair trial,  right of defence and the presumption  of innocence, right to good  
administration. In addition to those  rights, it is important to highlight  that children have specific rights as  enshrined in Article 24 of the EU  Charter and in the United Nations  Convention on the Rights of the  Child (further elaborated in the  UNCRC General Comment No. 25  as regards the digital environment),  both of which require consideration  of the children’s vulnerabilities and  provision of such protection and  care as necessary for their well being. The fundamental right to a  high level of environmental  
protection enshrined in the Charter 
-- COL 29.1.5 --
diagnostics systems and systems  supporting human decisions should  be reliable and accurate. The extent  of the adverse impact caused by the  AI system on the fundamental rights  protected by the Charter is of  particular relevance when  
classifying an AI system as high risk. Those rights include the right to  human dignity, respect for private  and family life, protection of  
personal data, freedom of expression  and information, freedom of  
assembly and of association, and  non-discrimination, consumer  protection, workers’ rights, rights of  persons with disabilities, right to an  effective remedy and to a fair trial,  right of defence and the presumption  of innocence, right to good  
administration. In addition to those  rights, it is important to highlight  that children have specific rights as  enshrined in Article 24 of the EU  Charter and in the United Nations  Convention on the Rights of the  Child (further elaborated in the  UNCRC General Comment No. 25  as regards the digital environment),  both of which require consideration  of the children’s vulnerabilities and  provision of such protection and care  as necessary for their well-being.  The fundamental right to a high level  of environmental protection  
enshrined in the Charter and  
implemented in Union policies 
-- COL 29.1.6 --
high, increasingly sophisticated  diagnostics systems and systems  supporting human decisions should  be reliable and accurate. The extent  of the adverse impact caused by the  AI system on the fundamental rights  protected by the Charter is of  particular relevance when  
classifying an AI system as high-risk.  Those rights include the right to  human dignity, respect for private  and family life, protection of  
personal data, freedom of expression  and information, freedom of  
assembly and of association, and  non-discrimination, consumer  protection, workers’ rights, rights of  persons with disabilities, right to an  effective remedy and to a fair trial,  right of defence and the presumption  of innocence, right to good  
administration. In addition to those  rights, it is important to highlight  that children have specific rights as  enshrined in Article 24 of the EU  Charter and in the United Nations  Convention on the Rights of the  Child (further elaborated in the  UNCRC General Comment No. 25  as regards the digital environment),  both of which require consideration  of the children’s vulnerabilities and  provision of such protection and  care as necessary for their well being. The fundamental right to a  high level of environmental  
protection enshrined in the Charter 
-- COL 29.1.7 --

-- COL 29.1.8 --


TTTXX TABLE: 30 XXTTT
== ROW 30.0 ==
-- COL 30.0.0 --

-- COL 30.0.1 --

-- COL 30.0.2 --

-- COL 30.0.3 --
Commission Proposal 
-- COL 30.0.4 --
EP Mandate 
-- COL 30.0.5 --
Council Mandate 
-- COL 30.0.6 --
Draft Agreement
-- COL 30.0.7 --

-- COL 30.0.8 --

== ROW 30.1 ==
-- COL 30.1.0 --

-- COL 30.1.1 --

-- COL 30.1.2 --

-- COL 30.1.3 --
should also be considered when  assessing the severity of the harm  that an AI system can cause,  
including in relation to the health  and safety of persons.
-- COL 30.1.4 --
and implemented in Union policies  should also be considered when  assessing the severity of the harm  that an AI system can cause,  
including in relation to the health  and safety of persons.
-- COL 30.1.5 --
should also be considered when  assessing the severity of the harm  that an AI system can cause,  
including in relation to the health  and safety of persons.
-- COL 30.1.6 --
and implemented in Union policies  should also be considered when  assessing the severity of the harm  that an AI system can cause,  
including in relation to the health  and safety of persons.
-- COL 30.1.7 --

-- COL 30.1.8 --

== ROW 30.2 ==
-- COL 30.2.0 --

-- COL 30.2.1 --

-- COL 30.2.2 --
Recital 28a
-- COL 30.2.3 --

-- COL 30.2.4 --

-- COL 30.2.5 --

-- COL 30.2.6 --

-- COL 30.2.7 --

-- COL 30.2.8 --

== ROW 30.3 ==
-- COL 30.3.0 --
G 
-- COL 30.3.1 --

-- COL 30.3.2 --
38a
-- COL 30.3.3 --

-- COL 30.3.4 --
(28a) The extent of the adverse  impact caused by the AI system on  the fundamental rights protected by  the Charter is of particular  
relevance when classifying an AI  system as high-risk. Those rights  include the right to human dignity,  respect for private and family life,  protection of personal data,  
freedom of expression and  
information, freedom of assembly  and of association, and non 
discrimination, right to education  consumer protection, workers’  rights, rights of persons with  disabilities, gender equality,  
intellectual property rights, right to  an effective remedy and to a fair  trial, right of defence and the  presumption of innocence, right to  good administration. In addition to  those rights, it is important to  highlight that children have specific  rights as enshrined in Article 24 of  the EU Charter and in the United  Nations Convention on the Rights  of the Child (further elaborated in 
-- COL 30.3.5 --

-- COL 30.3.6 --
(28a) The extent of the adverse  impact caused by the AI system on  the fundamental rights protected by  the Charter is of particular  
relevance when classifying an AI  system as high-risk. Those rights  include the right to human dignity,  respect for private and family life,  protection of personal data,  
freedom of expression and  
information, freedom of assembly  and of association, and non 
discrimination, right to education  consumer protection, workers’  rights, rights of persons with  disabilities, gender equality,  
intellectual property rights, right to  an effective remedy and to a fair  trial, right of defence and the  presumption of innocence, right to  good administration. In addition to  those rights, it is important to  highlight that children have specific  rights as enshrined in Article 24 of  the EU Charter and in the United  Nations Convention on the Rights  of the Child (further elaborated in 
-- COL 30.3.7 --
G
-- COL 30.3.8 --


TTTXX TABLE: 31 XXTTT
== ROW 31.0 ==
-- COL 31.0.0 --

-- COL 31.0.1 --

-- COL 31.0.2 --

-- COL 31.0.3 --
Commission Proposal 
-- COL 31.0.4 --
EP Mandate 
-- COL 31.0.5 --
Council Mandate 
-- COL 31.0.6 --
Draft Agreement
-- COL 31.0.7 --

-- COL 31.0.8 --

== ROW 31.1 ==
-- COL 31.1.0 --

-- COL 31.1.1 --

-- COL 31.1.2 --

-- COL 31.1.3 --

-- COL 31.1.4 --
the UNCRC General Comment No.  25 as regards the digital  
environment), both of which  require consideration of the  
children’s vulnerabilities and  provision of such protection and  care as necessary for their well being. The fundamental right to a  high level of environmental  
protection enshrined in the Charter  and implemented in Union policies  should also be considered when  assessing the severity of the harm  that an AI system can cause,  including in relation to the health  and safety of persons or to the  environment. 
-- COL 31.1.5 --

-- COL 31.1.6 --
the UNCRC General Comment No.  25 as regards the digital  
environment), both of which  require consideration of the  
children’s vulnerabilities and  provision of such protection and  care as necessary for their well being. The fundamental right to a  high level of environmental  
protection enshrined in the Charter  and implemented in Union policies  should also be considered when  assessing the severity of the harm  that an AI system can cause,  including in relation to the health  and safety of persons.
-- COL 31.1.7 --

-- COL 31.1.8 --

== ROW 31.2 ==
-- COL 31.2.0 --

-- COL 31.2.1 --

-- COL 31.2.2 --
Recital 29
-- COL 31.2.3 --

-- COL 31.2.4 --

-- COL 31.2.5 --

-- COL 31.2.6 --

-- COL 31.2.7 --

-- COL 31.2.8 --

== ROW 31.3 ==
-- COL 31.3.0 --
G 
-- COL 31.3.1 --

-- COL 31.3.2 --
39
-- COL 31.3.3 --
(29) As regards high-risk AI  
systems that are safety components  of products or systems, or which are  themselves products or systems  falling within the scope of  
Regulation (EC) No 300/2008 of the  European Parliament and of the  Council1, Regulation (EU) No  167/2013 of the European  
Parliament and of the Council2,  Regulation (EU) No 168/2013 of the  European Parliament and of the  Council3, Directive 2014/90/EU of  the European Parliament and of the  Council4, Directive (EU) 2016/797  of the European Parliament and of 
-- COL 31.3.4 --
(29) As regards high-risk AI  
systems that are safety components  of products or systems, or which are  themselves products or systems  falling within the scope of  
Regulation (EC) No 300/2008 of the  European Parliament and of the  Council1, Regulation (EU) No  167/2013 of the European  
Parliament and of the Council2,  Regulation (EU) No 168/2013 of the  European Parliament and of the  Council3, Directive 2014/90/EU of  the European Parliament and of the  Council4, Directive (EU) 2016/797  of the European Parliament and of 
-- COL 31.3.5 --
(29) As regards high-risk AI  
systems that are safety components  of products or systems, or which are  themselves products or systems  falling within the scope of  
Regulation (EC) No 300/2008 of the  European Parliament and of the  Council1, Regulation (EU) No  167/2013 of the European  
Parliament and of the Council2,  Regulation (EU) No 168/2013 of the  European Parliament and of the  Council3, Directive 2014/90/EU of  the European Parliament and of the  Council4, Directive (EU) 2016/797  of the European Parliament and of 
-- COL 31.3.6 --
(29) As regards high-risk AI  
systems that are safety components  of products or systems, or which are  themselves products or systems  falling within the scope of  
Regulation (EC) No 300/2008 of the  European Parliament and of the  Council1, Regulation (EU) No  167/2013 of the European  
Parliament and of the Council2,  Regulation (EU) No 168/2013 of the  European Parliament and of the  Council3, Directive 2014/90/EU of  the European Parliament and of the  Council4, Directive (EU) 2016/797  of the European Parliament and of 
-- COL 31.3.7 --
G
-- COL 31.3.8 --


TTTXX TABLE: 32 XXTTT
== ROW 32.0 ==
-- COL 32.0.0 --

-- COL 32.0.1 --

-- COL 32.0.2 --

-- COL 32.0.3 --
Commission Proposal 
-- COL 32.0.4 --
EP Mandate 
-- COL 32.0.5 --
Council Mandate 
-- COL 32.0.6 --
Draft Agreement
-- COL 32.0.7 --

-- COL 32.0.8 --

== ROW 32.1 ==
-- COL 32.1.0 --

-- COL 32.1.1 --

-- COL 32.1.2 --

-- COL 32.1.3 --
the Council5, Regulation (EU)  2018/858 of the European  
Parliament and of the Council6,  Regulation (EU) 2018/1139 of the  European Parliament and of the  Council7, and Regulation (EU)  2019/2144 of the European  
Parliament and of the Council8, it is  appropriate to amend those acts to  ensure that the Commission takes  into account, on the basis of the  technical and regulatory specificities  of each sector, and without  
interfering with existing governance,  conformity assessment and  
enforcement mechanisms and  authorities established therein, the  mandatory requirements for high risk AI systems laid down in this  Regulation when adopting any  relevant future delegated or  
implementing acts on the basis of  those acts. 
_________ 
1. Regulation (EC) No 300/2008 of the  European Parliament and of the Council of  11 March 2008 on common rules in the field  of civil aviation security and repealing  Regulation (EC) No 2320/2002 (OJ L 97,  9.4.2008, p. 72). 
2. Regulation (EU) No 167/2013 of the  European Parliament and of the Council of 5  February 2013 on the approval and market  surveillance of agricultural and forestry  vehicles (OJ L 60, 2.3.2013, p. 1). 
3. Regulation (EU) No 168/2013 of the  European Parliament and of the Council of  15 January 2013 on the approval and market  surveillance of two- or three-wheel vehicles  and quadricycles (OJ L 60, 2.3.2013, p. 52).
-- COL 32.1.4 --
the Council5, Regulation (EU)  2018/858 of the European  
Parliament and of the Council6,  Regulation (EU) 2018/1139 of the  European Parliament and of the  Council7, and Regulation (EU)  2019/2144 of the European  
Parliament and of the Council8, it is  appropriate to amend those acts to  ensure that the Commission takes  into account, on the basis of the  technical and regulatory specificities  of each sector, and without  
interfering with existing governance,  conformity assessment, market  surveillance and enforcement  mechanisms and authorities  
established therein, the mandatory  requirements for high-risk AI  systems laid down in this Regulation  when adopting any relevant future  delegated or implementing acts on  the basis of those acts. 
_________ 
1. Regulation (EC) No 300/2008 of the  European Parliament and of the Council of  11 March 2008 on common rules in the field  of civil aviation security and repealing  Regulation (EC) No 2320/2002 (OJ L 97,  9.4.2008, p. 72). 
2. Regulation (EU) No 167/2013 of the  European Parliament and of the Council of 5  February 2013 on the approval and market  surveillance of agricultural and forestry  vehicles (OJ L 60, 2.3.2013, p. 1). 
3. Regulation (EU) No 168/2013 of the  European Parliament and of the Council of  15 January 2013 on the approval and market  surveillance of two- or three-wheel vehicles  and quadricycles (OJ L 60, 2.3.2013, p. 52).
-- COL 32.1.5 --
the Council5, Regulation (EU) 2018/858 of the European  
Parliament and of the Council6,  Regulation (EU) 2018/1139 of the  European Parliament and of the  Council7, and Regulation (EU)  2019/2144 of the European  
Parliament and of the Council8, it is  appropriate to amend those acts to  ensure that the Commission takes  into account, on the basis of the  technical and regulatory specificities  of each sector, and without  
interfering with existing governance,  conformity assessment and  
enforcement mechanisms and  authorities established therein, the mandatory requirements for high risk AI systems laid down in this  Regulation when adopting any  relevant future delegated or  
implementing acts on the basis of  those acts. 
_________ 
1. [1] Regulation (EC) No 300/2008  of the European Parliament and of the  Council of 11 March 2008 on common rules  in the field of civil aviation security and  repealing Regulation (EC) No 2320/2002  (OJ L 97, 9.4.2008, p. 72). 
2. [2] Regulation (EU) No 167/2013  of the European Parliament and of the  Council of 5 February 2013 on the approval  and market surveillance of agricultural and  forestry vehicles (OJ L 60, 2.3.2013, p. 1). 3. [3] Regulation (EU) No 168/2013  of the European Parliament and of the  Council of 15 January 2013 on the approval  and market surveillance of two- or three wheel vehicles and quadricycles (OJ L 60, 
-- COL 32.1.6 --
the Council5, Regulation (EU)  2018/858 of the European  
Parliament and of the Council6,  Regulation (EU) 2018/1139 of the  European Parliament and of the  Council7, and Regulation (EU)  2019/2144 of the European  
Parliament and of the Council8, it is  appropriate to amend those acts to  ensure that the Commission takes  into account, on the basis of the  technical and regulatory specificities  of each sector, and without  
interfering with existing governance,  conformity assessment and  
enforcement mechanisms and  authorities established therein, the  mandatory requirements for high risk AI systems laid down in this  Regulation when adopting any  relevant future delegated or  
implementing acts on the basis of  those acts. 
_________ 
1. Regulation (EC) No 300/2008 of the  European Parliament and of the Council of  11 March 2008 on common rules in the field  of civil aviation security and repealing  Regulation (EC) No 2320/2002 (OJ L 97,  9.4.2008, p. 72). 
2. Regulation (EU) No 167/2013 of the  European Parliament and of the Council of 5  February 2013 on the approval and market  surveillance of agricultural and forestry  vehicles (OJ L 60, 2.3.2013, p. 1). 
3. Regulation (EU) No 168/2013 of the  European Parliament and of the Council of  15 January 2013 on the approval and market  surveillance of two- or three-wheel vehicles  and quadricycles (OJ L 60, 2.3.2013, p. 52).
-- COL 32.1.7 --

-- COL 32.1.8 --


TTTXX TABLE: 33 XXTTT
== ROW 33.0 ==
-- COL 33.0.0 --

-- COL 33.0.1 --

-- COL 33.0.2 --

-- COL 33.0.3 --
Commission Proposal 
-- COL 33.0.4 --
EP Mandate 
-- COL 33.0.5 --
Council Mandate 
-- COL 33.0.6 --
Draft Agreement
-- COL 33.0.7 --

-- COL 33.0.8 --

== ROW 33.1 ==
-- COL 33.1.0 --

-- COL 33.1.1 --

-- COL 33.1.2 --

-- COL 33.1.3 --
4. Directive 2014/90/EU of the European  Parliament and of the Council of 23 July  2014 on marine equipment and repealing  
Council Directive 96/98/EC (OJ L 257,  28.8.2014, p. 146). 
5. Directive (EU) 2016/797 of the European  Parliament and of the Council of 11 May  2016 on the interoperability of the rail  system within the European Union (OJ L  138, 26.5.2016, p. 44). 
6. Regulation (EU) 2018/858 of the  
European Parliament and of the Council of  30 May 2018 on the approval and market  surveillance of motor vehicles and their  trailers, and of systems, components and  separate technical units intended for such  vehicles, amending Regulations (EC) No  715/2007 and (EC) No 595/2009 and  repealing Directive 2007/46/EC (OJ L 151,  14.6.2018, p. 1). 
7. Regulation (EU) 2018/1139 of the  European Parliament and of the Council of 4  July 2018 on common rules in the field of  civil aviation and establishing a European  Union Aviation Safety Agency, and  amending Regulations (EC) No 2111/2005,  (EC) No 1008/2008, (EU) No 996/2010,  (EU) No 376/2014 and Directives  
2014/30/EU and 2014/53/EU of the  
European Parliament and of the Council, and  repealing Regulations (EC) No 552/2004 and  (EC) No 216/2008 of the European  
Parliament and of the Council and Council  Regulation (EEC) No 3922/91 (OJ L 212,  22.8.2018, p. 1). 
8. Regulation (EU) 2019/2144 of the  European Parliament and of the Council of  27 November 2019 on type-approval  requirements for motor vehicles and their  trailers, and systems, components and  separate technical units intended for such  vehicles, as regards their general safety and  the protection of vehicle occupants and  vulnerable road users, amending Regulation  (EU) 2018/858 of the European Parliament 
-- COL 33.1.4 --
4. Directive 2014/90/EU of the European  Parliament and of the Council of 23 July  2014 on marine equipment and repealing  
Council Directive 96/98/EC (OJ L 257,  28.8.2014, p. 146). 
5. Directive (EU) 2016/797 of the European  Parliament and of the Council of 11 May  2016 on the interoperability of the rail  system within the European Union (OJ L  138, 26.5.2016, p. 44). 
6. Regulation (EU) 2018/858 of the  
European Parliament and of the Council of  30 May 2018 on the approval and market  surveillance of motor vehicles and their  trailers, and of systems, components and  separate technical units intended for such  vehicles, amending Regulations (EC) No  715/2007 and (EC) No 595/2009 and  repealing Directive 2007/46/EC (OJ L 151,  14.6.2018, p. 1). 
7. Regulation (EU) 2018/1139 of the  European Parliament and of the Council of 4  July 2018 on common rules in the field of  civil aviation and establishing a European  Union Aviation Safety Agency, and  amending Regulations (EC) No 2111/2005,  (EC) No 1008/2008, (EU) No 996/2010,  (EU) No 376/2014 and Directives  
2014/30/EU and 2014/53/EU of the  
European Parliament and of the Council, and  repealing Regulations (EC) No 552/2004 and  (EC) No 216/2008 of the European  
Parliament and of the Council and Council  Regulation (EEC) No 3922/91 (OJ L 212,  22.8.2018, p. 1). 
8. Regulation (EU) 2019/2144 of the  European Parliament and of the Council of  27 November 2019 on type-approval  requirements for motor vehicles and their  trailers, and systems, components and  separate technical units intended for such  vehicles, as regards their general safety and  the protection of vehicle occupants and  vulnerable road users, amending Regulation  (EU) 2018/858 of the European Parliament 
-- COL 33.1.5 --
2.3.2013, p. 52). 
4. [4] Directive 2014/90/EU of the  European Parliament and of the Council of  23 July 2014 on marine equipment and  repealing Council Directive 96/98/EC (OJ L  257, 28.8.2014, p. 146). 
5. [5] Directive (EU) 2016/797 of the  European Parliament and of the Council of  11 May 2016 on the interoperability of the  rail system within the European Union (OJ L  138, 26.5.2016, p. 44). 
6. [6] Regulation (EU) 2018/858 of  the European Parliament and of the Council  of 30 May 2018 on the approval and market  
surveillance of motor vehicles and their  trailers, and of systems, components and  separate technical units intended for such  vehicles, amending Regulations (EC) No  715/2007 and (EC) No 595/2009 and  repealing Directive 2007/46/EC (OJ L 151,  14.6.2018, p. 1). 
7. [7] Regulation (EU) 2018/1139 of  the European Parliament and of the Council  of 4 July 2018 on common rules in the field  of civil aviation and establishing a European  
Union Aviation Safety Agency, and  amending Regulations (EC) No 2111/2005,  (EC) No 1008/2008, (EU) No 996/2010,  (EU) No 376/2014 and Directives  
2014/30/EU and 2014/53/EU of the  
European Parliament and of the Council, and  repealing Regulations (EC) No 552/2004 and  (EC) No 216/2008 of the European  
Parliament and of the Council and Council  Regulation (EEC) No 3922/91 (OJ L 212,  22.8.2018, p. 1). 
8. [8] Regulation (EU) 2019/2144 of  the European Parliament and of the Council  of 27 November 2019 on type-approval  requirements for motor vehicles and their  trailers, and systems, components and  separate technical units intended for such  vehicles, as regards their general safety and  the protection of vehicle occupants and  vulnerable road users, amending Regulation 
-- COL 33.1.6 --
4. Directive 2014/90/EU of the European  Parliament and of the Council of 23 July  2014 on marine equipment and repealing  
Council Directive 96/98/EC (OJ L 257,  28.8.2014, p. 146). 
5. Directive (EU) 2016/797 of the European  Parliament and of the Council of 11 May  2016 on the interoperability of the rail  system within the European Union (OJ L  138, 26.5.2016, p. 44). 
6. Regulation (EU) 2018/858 of the  
European Parliament and of the Council of  30 May 2018 on the approval and market  surveillance of motor vehicles and their  trailers, and of systems, components and  separate technical units intended for such  vehicles, amending Regulations (EC) No  715/2007 and (EC) No 595/2009 and  repealing Directive 2007/46/EC (OJ L 151,  14.6.2018, p. 1). 
7. Regulation (EU) 2018/1139 of the  European Parliament and of the Council of 4  July 2018 on common rules in the field of  civil aviation and establishing a European  Union Aviation Safety Agency, and  amending Regulations (EC) No 2111/2005,  (EC) No 1008/2008, (EU) No 996/2010,  (EU) No 376/2014 and Directives  
2014/30/EU and 2014/53/EU of the  
European Parliament and of the Council, and  repealing Regulations (EC) No 552/2004 and  (EC) No 216/2008 of the European  
Parliament and of the Council and Council  Regulation (EEC) No 3922/91 (OJ L 212,  22.8.2018, p. 1). 
8. Regulation (EU) 2019/2144 of the  European Parliament and of the Council of  27 November 2019 on type-approval  requirements for motor vehicles and their  trailers, and systems, components and  separate technical units intended for such  vehicles, as regards their general safety and  the protection of vehicle occupants and  vulnerable road users, amending Regulation  (EU) 2018/858 of the European Parliament 
-- COL 33.1.7 --

-- COL 33.1.8 --


TTTXX TABLE: 34 XXTTT
== ROW 34.0 ==
-- COL 34.0.0 --

-- COL 34.0.1 --

-- COL 34.0.2 --

-- COL 34.0.3 --
Commission Proposal 
-- COL 34.0.4 --
EP Mandate 
-- COL 34.0.5 --
Council Mandate 
-- COL 34.0.6 --
Draft Agreement
-- COL 34.0.7 --

-- COL 34.0.8 --

== ROW 34.1 ==
-- COL 34.1.0 --

-- COL 34.1.1 --

-- COL 34.1.2 --

-- COL 34.1.3 --
and of the Council and repealing Regulations  (EC) No 78/2009, (EC) No 79/2009 and  (EC) No 661/2009 of the European  
Parliament and of the Council and  
Commission Regulations (EC) No 631/2009,  (EU) No 406/2010, (EU) No 672/2010, (EU)  No 1003/2010, (EU) No 1005/2010, (EU)  No 1008/2010, (EU) No 1009/2010, (EU)  No 19/2011, (EU) No 109/2011, (EU) No  458/2011, (EU) No 65/2012, (EU) No  130/2012, (EU) No 347/2012, (EU) No  351/2012, (EU) No 1230/2012 and (EU)  2015/166 (OJ L 325, 16.12.2019, p. 1).
-- COL 34.1.4 --
and of the Council and repealing Regulations  (EC) No 78/2009, (EC) No 79/2009 and  (EC) No 661/2009 of the European  
Parliament and of the Council and  
Commission Regulations (EC) No 631/2009,  (EU) No 406/2010, (EU) No 672/2010, (EU)  No 1003/2010, (EU) No 1005/2010, (EU)  No 1008/2010, (EU) No 1009/2010, (EU)  No 19/2011, (EU) No 109/2011, (EU) No  458/2011, (EU) No 65/2012, (EU) No  130/2012, (EU) No 347/2012, (EU) No  351/2012, (EU) No 1230/2012 and (EU)  2015/166 (OJ L 325, 16.12.2019, p. 1).
-- COL 34.1.5 --
(EU) 2018/858 of the European Parliament  and of the Council and repealing Regulations  (EC) No 78/2009, (EC) No 79/2009 and  (EC) No 661/2009 of the European  
Parliament and of the Council and  
Commission Regulations (EC) No 631/2009,  (EU) No 406/2010, (EU) No 672/2010, (EU)  No 1003/2010, (EU) No 1005/2010, (EU)  No 1008/2010, (EU) No 1009/2010, (EU)  No 19/2011, (EU) No 109/2011, (EU) No  458/2011, (EU) No 65/2012, (EU) No  130/2012, (EU) No 347/2012, (EU) No  351/2012, (EU) No 1230/2012 and (EU)  2015/166 (OJ L 325, 16.12.2019, p. 1).
-- COL 34.1.6 --
and of the Council and repealing Regulations  (EC) No 78/2009, (EC) No 79/2009 and  (EC) No 661/2009 of the European  
Parliament and of the Council and  
Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU)  No 1003/2010, (EU) No 1005/2010, (EU)  No 1008/2010, (EU) No 1009/2010, (EU)  No 19/2011, (EU) No 109/2011, (EU) No  458/2011, (EU) No 65/2012, (EU) No  130/2012, (EU) No 347/2012, (EU) No  351/2012, (EU) No 1230/2012 and (EU)  2015/166 (OJ L 325, 16.12.2019, p. 1). 
Text Origin: Commission  
Proposal
-- COL 34.1.7 --

-- COL 34.1.8 --

== ROW 34.2 ==
-- COL 34.2.0 --

-- COL 34.2.1 --

-- COL 34.2.2 --
Recital 30
-- COL 34.2.3 --

-- COL 34.2.4 --

-- COL 34.2.5 --

-- COL 34.2.6 --

-- COL 34.2.7 --

-- COL 34.2.8 --

== ROW 34.3 ==
-- COL 34.3.0 --
G 
-- COL 34.3.1 --

-- COL 34.3.2 --
40
-- COL 34.3.3 --
(30) As regards AI systems that are  safety components of products, or  which are themselves products,  falling within the scope of certain  Union harmonisation legislation, it is  appropriate to classify them as high risk under this Regulation if the  product in question undergoes the  conformity assessment procedure  with a third-party conformity  assessment body pursuant to that  relevant Union harmonisation  legislation. In particular, such  products are machinery, toys, lifts,  equipment and protective systems  intended for use in potentially  explosive atmospheres, radio  
equipment, pressure equipment,  recreational craft equipment,  
cableway installations, appliances 
-- COL 34.3.4 --
(30) As regards AI systems that are  safety components of products, or  which are themselves products,  falling within the scope of certain  Union harmonisation legislationlaw  listed in Annex II, it is appropriate  to classify them as high-risk under  this Regulation if the product in  question undergoes the conformity  assessment procedure in order to  ensure compliance with essential  safety requirements with a third party conformity assessment body  pursuant to that relevant Union  harmonisation legislationlaw. In  particular, such products are  
machinery, toys, lifts, equipment and  protective systems intended for use  in potentially explosive atmospheres,  radio equipment, pressure 
-- COL 34.3.5 --
(30) As regards AI systems that are  safety components of products, or  which are themselves products,  falling within the scope of certain  Union harmonisation legislation, it is  appropriate to classify them as high risk under this Regulation if the  product in question undergoes the  conformity assessment procedure  with a third-party conformity  assessment body pursuant to that  relevant Union harmonisation  legislation. In particular, such  products are machinery, toys, lifts,  equipment and protective systems  intended for use in potentially  explosive atmospheres, radio  
equipment, pressure equipment,  recreational craft equipment,  
cableway installations, appliances 
-- COL 34.3.6 --
(30) As regards AI systems that are  safety components of products, or  which are themselves products,  falling within the scope of certain  Union harmonisation legislation listed in Annex II, it is appropriate  to classify them as high-risk under  this Regulation if the product in  question undergoes the conformity  assessment procedure with a third party conformity assessment body  pursuant to that relevant Union  harmonisation legislation. In  
particular, such products are  
machinery, toys, lifts, equipment and  protective systems intended for use  in potentially explosive atmospheres,  radio equipment, pressure  
equipment, recreational craft  
equipment, cableway installations, 
-- COL 34.3.7 --
G
-- COL 34.3.8 --


TTTXX TABLE: 35 XXTTT
== ROW 35.0 ==
-- COL 35.0.0 --

-- COL 35.0.1 --

-- COL 35.0.2 --

-- COL 35.0.3 --
Commission Proposal 
-- COL 35.0.4 --
EP Mandate 
-- COL 35.0.5 --
Council Mandate 
-- COL 35.0.6 --
Draft Agreement
-- COL 35.0.7 --

-- COL 35.0.8 --

== ROW 35.1 ==
-- COL 35.1.0 --

-- COL 35.1.1 --

-- COL 35.1.2 --

-- COL 35.1.3 --
burning gaseous fuels, medical  devices, and in vitro diagnostic  medical devices.
-- COL 35.1.4 --
equipment, recreational craft  
equipment, cableway installations,  appliances burning gaseous fuels,  medical devices, and in vitro  
diagnostic medical devices.
-- COL 35.1.5 --
burning gaseous fuels, medical  devices, and in vitro diagnostic  medical devices.
-- COL 35.1.6 --
appliances burning gaseous fuels,  medical devices, and in vitro  
diagnostic medical devices.
-- COL 35.1.7 --

-- COL 35.1.8 --

== ROW 35.2 ==
-- COL 35.2.0 --

-- COL 35.2.1 --

-- COL 35.2.2 --
Recital 31
-- COL 35.2.3 --

-- COL 35.2.4 --

-- COL 35.2.5 --

-- COL 35.2.6 --

-- COL 35.2.7 --

-- COL 35.2.8 --

== ROW 35.3 ==
-- COL 35.3.0 --
G 
-- COL 35.3.1 --

-- COL 35.3.2 --
41
-- COL 35.3.3 --
(31) The classification of an AI  system as high-risk pursuant to this  Regulation should not necessarily  mean that the product whose safety  component is the AI system, or the  AI system itself as a product, is  considered ‘high-risk’ under the  criteria established in the relevant  Union harmonisation legislation that  applies to the product. This is  notably the case for Regulation (EU)  2017/745 of the European  
Parliament and of the Council1and  Regulation (EU) 2017/746 of the  European Parliament and of the  Council2, where a third-party  
conformity assessment is provided  for medium-risk and high-risk  products. 
_________ 
1. Regulation (EU) 2017/745 of the  
European Parliament and of the Council of 5  April 2017 on medical devices, amending  Directive 2001/83/EC, Regulation (EC) No  178/2002 and Regulation (EC) No  
1223/2009 and repealing Council Directives  90/385/EEC and 93/42/EEC (OJ L 117,  5.5.2017, p. 1). 
2. Regulation (EU) 2017/746 of the  
European Parliament and of the Council of 5 
-- COL 35.3.4 --
(31) The classification of an AI  system as high-risk pursuant to this  Regulation should not necessarily  mean that the product whose safety  component is the AI system, or the  AI system itself as a product, is  considered ‘high-risk’ under the  criteria established in the relevant  Union harmonisation legislationlaw that applies to the product. This is  notably the case for Regulation (EU)  2017/745 of the European  
Parliament and of the Council1and  Regulation (EU) 2017/746 of the  European Parliament and of the  Council2, where a third-party  
conformity assessment is provided  for medium-risk and high-risk  products. 
_________ 
1. Regulation (EU) 2017/745 of the  
European Parliament and of the Council of 5  April 2017 on medical devices, amending  Directive 2001/83/EC, Regulation (EC) No  178/2002 and Regulation (EC) No  
1223/2009 and repealing Council Directives  90/385/EEC and 93/42/EEC (OJ L 117,  5.5.2017, p. 1). 
2. Regulation (EU) 2017/746 of the  
European Parliament and of the Council of 5 
-- COL 35.3.5 --
(31) The classification of an AI  system as high-risk pursuant to this  Regulation should not necessarily  mean that the product whose safety  component is the AI system, or the  AI system itself as a product, is  considered ‘high-risk’ under the  criteria established in the relevant  Union harmonisation legislation that  applies to the product. This is  notably the case for Regulation (EU)  2017/745 of the European  
Parliament and of the Council1and  Regulation (EU) 2017/746 of the  European Parliament and of the  Council2, where a third-party  
conformity assessment is provided  for medium-risk and high-risk  products. 
_________ 
1. [1] Regulation (EU) 2017/745 of  the European Parliament and of the Council  of 5 April 2017 on medical devices,  amending Directive 2001/83/EC, Regulation  (EC) No 178/2002 and Regulation (EC) No  1223/2009 and repealing Council Directives  90/385/EEC and 93/42/EEC (OJ L 117,  5.5.2017, p. 1). 
2. [2] Regulation (EU) 2017/746 of  the European Parliament and of the Council 
-- COL 35.3.6 --
(31) The classification of an AI  system as high-risk pursuant to this  Regulation should not necessarily  mean that the product whose safety  component is the AI system, or the  AI system itself as a product, is  considered ‘high-risk’ under the  criteria established in the relevant  Union harmonisation legislation that  applies to the product. This is  notably the case for Regulation (EU)  2017/745 of the European  
Parliament and of the Council1and  Regulation (EU) 2017/746 of the  European Parliament and of the  Council2, where a third-party  
conformity assessment is provided  for medium-risk and high-risk  products. 
_________ 
1. Regulation (EU) 2017/745 of the  
European Parliament and of the Council of 5  April 2017 on medical devices, amending  Directive 2001/83/EC, Regulation (EC) No  178/2002 and Regulation (EC) No  
1223/2009 and repealing Council Directives  90/385/EEC and 93/42/EEC (OJ L 117,  5.5.2017, p. 1). 
2. Regulation (EU) 2017/746 of the  
European Parliament and of the Council of 5 
-- COL 35.3.7 --
G
-- COL 35.3.8 --


TTTXX TABLE: 36 XXTTT
== ROW 36.0 ==
-- COL 36.0.0 --

-- COL 36.0.1 --

-- COL 36.0.2 --

-- COL 36.0.3 --
Commission Proposal 
-- COL 36.0.4 --
EP Mandate 
-- COL 36.0.5 --
Council Mandate 
-- COL 36.0.6 --
Draft Agreement
-- COL 36.0.7 --

-- COL 36.0.8 --

== ROW 36.1 ==
-- COL 36.1.0 --

-- COL 36.1.1 --

-- COL 36.1.2 --

-- COL 36.1.3 --
April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC  and Commission Decision 2010/227/EU (OJ  L 117, 5.5.2017, p. 176).
-- COL 36.1.4 --
April 2017 on in vitro diagnostic medical  devices and repealing Directive 98/79/EC  and Commission Decision 2010/227/EU (OJ  L 117, 5.5.2017, p. 176).
-- COL 36.1.5 --
of 5 April 2017 on in vitro diagnostic  medical devices and repealing Directive  98/79/EC and Commission Decision  2010/227/EU (OJ L 117, 5.5.2017, p. 176).
-- COL 36.1.6 --
April 2017 on in vitro diagnostic medical  devices and repealing Directive 98/79/EC  and Commission Decision 2010/227/EU (OJ  L 117, 5.5.2017, p. 176). 
Text Origin: Commission  
Proposal
-- COL 36.1.7 --

-- COL 36.1.8 --

== ROW 36.2 ==
-- COL 36.2.0 --

-- COL 36.2.1 --

-- COL 36.2.2 --
Recital 32
-- COL 36.2.3 --

-- COL 36.2.4 --

-- COL 36.2.5 --

-- COL 36.2.6 --

-- COL 36.2.7 --

-- COL 36.2.8 --

== ROW 36.3 ==
-- COL 36.3.0 --
G 
-- COL 36.3.1 --

-- COL 36.3.2 --
42
-- COL 36.3.3 --
(32) As regards stand-alone AI  systems, meaning high-risk AI  systems other than those that are  safety components of products, or  which are themselves products, it is  appropriate to classify them as high risk if, in the light of their intended  purpose, they pose a high risk of  harm to the health and safety or the  fundamental rights of persons,  taking into account both the severity  of the possible harm and its  
probability of occurrence and they are used in a number of specifically  pre-defined areas specified in the  Regulation. The identification of  those systems is based on the same  methodology and criteria envisaged  also for any future amendments of  the list of high-risk AI systems.
-- COL 36.3.4 --
(32) As regards stand-alone AI  systems, meaning high-risk AI  systems other than those that are  safety components of products, or  which are themselves products and  that are listed in one of the areas  and use cases in Annex III, it is  appropriate to classify them as high risk if, in the light of their intended  purpose, they pose a highsignificant risk of harm to the health and safety  or the fundamental rights of persons and, where the AI system is used as  a safety component of a critical  infrastructure, to the environment .  Such significant risk of harm  should be identified by assessing on  the one hand the effect of such risk  with respect to its level of severity,  intensity, , taking into account both  the severity of the possible harm and  its probability of occurrence and  they are used in a number of  
specifically pre-defined areas  specified in the Regulationduration  combined altogether and on the  other hand whether the risk can 
-- COL 36.3.5 --
(32) As regards stand-alone AI  systems, meaning high-risk AI  systems other than those that are  safety components of products, or  which are themselves products, it is  appropriate to classify them as high risk if, in the light of their intended  purpose, they pose a high risk of  harm to the health and safety or the  fundamental rights of persons,  taking into account both the severity  of the possible harm and its  
probability of occurrence, and they  are used in a number of specifically  pre-defined areas specified in the  Regulation. The identification of  those systems is based on the same  methodology and criteria envisaged  also for any future amendments of  the list of high-risk AI systems. It is  also important to clarify that within  the high-risk scenarios referred to  in Annex III there may be systems  that do not lead to a significant risk  to the legal interests protected  under those scenarios, taking into  account the output produced by the 
-- COL 36.3.6 --
(32) As regards stand-alone AI  systems, meaning high-risk AI  systems other than those that are  safety components of products, or  which are themselves products, it is  appropriate to classify them as high risk if, in the light of their intended  purpose, they pose a high risk of  harm to the health and safety or the  fundamental rights of persons,  taking into account both the severity  of the possible harm and its  
probability of occurrence and they  are used in a number of specifically  pre-defined areas specified in the  Regulation. The identification of  those systems is based on the same  methodology and criteria envisaged  also for any future amendments of  the list of high-risk AI systems that  the Commission should be  
empowered to adopt, via delegated  acts, to take into account the rapid  pace of technological development,  as well as the potential changes in  the use of AI systems.
-- COL 36.3.7 --
G
-- COL 36.3.8 --


TTTXX TABLE: 37 XXTTT
== ROW 37.0 ==
-- COL 37.0.0 --

-- COL 37.0.1 --

-- COL 37.0.2 --

-- COL 37.0.3 --
Commission Proposal 
-- COL 37.0.4 --
EP Mandate 
-- COL 37.0.5 --
Council Mandate 
-- COL 37.0.6 --
Draft Agreement
-- COL 37.0.7 --

-- COL 37.0.8 --

== ROW 37.1 ==
-- COL 37.1.0 --

-- COL 37.1.1 --

-- COL 37.1.2 --

-- COL 37.1.3 --

-- COL 37.1.4 --
affect an individual, a plurality of  persons or a particular group of  persons. Such combination could  for instance result in a high severity  but low probability to affect a  natural person, or a high  
probability to affect a group of  persons with a low intensity over a  long period of time, depending on  the context. The identification of  those systems is based on the same  methodology and criteria envisaged  also for any future amendments of  the list of high-risk AI systems.
-- COL 37.1.5 --
AI system. Therefore only when  such output has a high degree of  importance (i.e. is not purely  accessory) in respect of the relevant  action or decision so as to generate  a significant risk to the legal  interests protected, the AI system  generating such output should be  considered as high-risk. For  instance, when the information  provided by an AI systems to the  human consists of the profiling of  natural persons within the meaning  of of Article 4(4) Regulation (EU)  2016/679 and Article 3(4) of  
Directive (EU) 2016/680 and Article  3(5) of Regulation (EU) 2018/1725,  such information should not  typically be considered of accessory  nature in the context of high risk  AI systems as referred to in Annex  III. However, if the output of the AI  system has only negligible or minor  relevance for human action or  decision, it may be considered  purely accessory, including for  example, AI systems used for  translation for informative  
purposes or for the management of  documents.
-- COL 37.1.6 --

-- COL 37.1.7 --

-- COL 37.1.8 --

== ROW 37.2 ==
-- COL 37.2.0 --

-- COL 37.2.1 --

-- COL 37.2.2 --
Recital 32a
-- COL 37.2.3 --

-- COL 37.2.4 --

-- COL 37.2.5 --

-- COL 37.2.6 --

-- COL 37.2.7 --

-- COL 37.2.8 --

== ROW 37.3 ==
-- COL 37.3.0 --
G 
-- COL 37.3.1 --

-- COL 37.3.2 --
42a 
-- COL 37.3.3 --

-- COL 37.3.4 --
(32a) Providers whose AI systems  fall under one of the areas and use  cases listed in Annex III that 
-- COL 37.3.5 --

-- COL 37.3.6 --
(32a) It is also important to clarify  that there may be specific cases in  which AI systems referred to pre 
-- COL 37.3.7 --
G
-- COL 37.3.8 --


TTTXX TABLE: 38 XXTTT
== ROW 38.0 ==
-- COL 38.0.0 --

-- COL 38.0.1 --

-- COL 38.0.2 --

-- COL 38.0.3 --
Commission Proposal 
-- COL 38.0.4 --
EP Mandate 
-- COL 38.0.5 --
Council Mandate 
-- COL 38.0.6 --
Draft Agreement
-- COL 38.0.7 --

-- COL 38.0.8 --

== ROW 38.1 ==
-- COL 38.1.0 --

-- COL 38.1.1 --

-- COL 38.1.2 --

-- COL 38.1.3 --

-- COL 38.1.4 --
consider their system does not pose  a significant risk of harm to the  health, safety, fundamental rights  or the environment should inform  the national supervisory authorities  by submitting a reasoned  
notification. This could take the  form of a one-page summary of the  relevant information on the AI  system in question, including its  intended purpose and why it would  not pose a significant risk of harm  to the health, safety, fundamental  rights or the environment. The  Commission should specify criteria  to enable companies to assess  whether their system would pose  such risks, as well as develop an  easy to use and standardised  template for the notification.  Providers should submit the  
notification as early as possible and  in any case prior to the placing of  the AI system on the market or its  putting into service, ideally at the  development stage, and they should  be free to place it on the market at  any given time after the 
notification. However, if the  
authority estimates the AI system in  question was misclassified, it  should object to the notification  within a period of three months.  The objection should be  
substantiated and duly explain why  the AI system has been  
misclassified. The provider should 
-- COL 38.1.5 --

-- COL 38.1.6 --
defined areas specified in this  Regulation do not lead to a  
significant risk of harm to the legal  interests protected under those  areas, because they do not  
materially influence the decision making or do not harm those  interests substantially. For the  purpose of this Regulation an AI  system not materially influencing  the outcome of decision-making  should be understood as an AI  system that does not impact the  substance, and thereby the  
outcome, of decision-making,  whether human or automated. This  could be the case if one or more of  the following conditions are  
fulfilled. The first criterion should  be that the AI system is intended to  perform a narrow procedural task,  such as an AI system that  
transforms unstructured data into  structured data, an AI system that  classifies incoming documents into  categories or an AI system that is  used to detect duplicates among a  large number of applications. These  tasks are of such narrow and  limited nature that they pose only limited risks which are not  
increased through the use in a  context listed in Annex III. The  second criterion should be that the  task performed by the AI system is  intended to improve the result of a  previously completed human 
-- COL 38.1.7 --

-- COL 38.1.8 --


TTTXX TABLE: 39 XXTTT
== ROW 39.0 ==
-- COL 39.0.0 --

-- COL 39.0.1 --

-- COL 39.0.2 --

-- COL 39.0.3 --
Commission Proposal 
-- COL 39.0.4 --
EP Mandate 
-- COL 39.0.5 --
Council Mandate 
-- COL 39.0.6 --
Draft Agreement
-- COL 39.0.7 --

-- COL 39.0.8 --

== ROW 39.1 ==
-- COL 39.1.0 --

-- COL 39.1.1 --

-- COL 39.1.2 --

-- COL 39.1.3 --

-- COL 39.1.4 --
retain the right to appeal by  
providing further arguments. If  after the three months there has  been no objection to the  
notification, national supervisory  authorities could still intervene if  the AI system presents a risk at  national level, as for any other AI  system on the market. National  supervisory authorities should  submit annual reports to the AI  Office detailing the notifications  received and the decisions taken. 
-- COL 39.1.5 --

-- COL 39.1.6 --
activity that may be relevant for the  purpose of the use case listed in  Annex III. Considering these  characteristics, the AI system only  provides an additional layer to a  human activity with consequently  lowered risk. For example, this  criterion would apply to AI systems  that are intended to improve the  language used in previously drafted  documents, for instance in relation  to professional tone, academic style  of language or by aligning text to a  certain brand messaging. The third  criterion should be that the AI  system is intended to detect  
decision-making patterns or  
deviations from prior decision making patterns. The risk would be  lowered because the use of the AI  system follows a previously  
completed human assessment  which it is not meant to replace or  influence, without proper human review. Such AI systems include for  instance those that, given a certain  grading pattern of a teacher, can be  used to check ex post whether the  teacher may have deviated from the  grading pattern so as to flag  
potential inconsistencies or  
anomalies. The fourth criterion  should be that the AI system is  intended to perform a task that is  only preparatory to an assessment  relevant for the purpose of the use  case listed in Annex III, thus 
-- COL 39.1.7 --

-- COL 39.1.8 --


TTTXX TABLE: 40 XXTTT
== ROW 40.0 ==
-- COL 40.0.0 --

-- COL 40.0.1 --

-- COL 40.0.2 --

-- COL 40.0.3 --
Commission Proposal 
-- COL 40.0.4 --
EP Mandate 
-- COL 40.0.5 --
Council Mandate 
-- COL 40.0.6 --
Draft Agreement
-- COL 40.0.7 --

-- COL 40.0.8 --

== ROW 40.1 ==
-- COL 40.1.0 --

-- COL 40.1.1 --

-- COL 40.1.2 --

-- COL 40.1.3 --

-- COL 40.1.4 --

-- COL 40.1.5 --

-- COL 40.1.6 --
making the possible impact of the  output of the system very low in  terms of representing a risk for the  assessment to follow. For example,  this criterion covers smart solutions  for file handling, which include  various functions from indexing,  searching, text and speech  
processing or linking data to other  data sources, or AI systems used for  translation of initial documents. In  any case, AI systems referred to in  Annex III should be considered to  pose signficant risks of harm to the  health, safety or fundamental rights  of natural persons if the AI system  implies profiling within the  
meaning of Article 4(4) of  
Regulation (EU) 2016/679 and  Article 3(4) of Directive (EU)  2016/680 and Article 3(5) of  
Regulation 2018/1725. To ensure  traceability and transparency, a  provider who considers that an AI  system referred to in Annex III is  not high-risk on the basis of the  aforementioned criteria should  draw up documentation of the  assessment before that system is  placed on the market or put into  service and should provide this  documentation to national  
competent authorities upon request.  Such provider should be obliged to  register the system in the EU  database established under this  Regulation. With a view to provide 
-- COL 40.1.7 --

-- COL 40.1.8 --


TTTXX TABLE: 41 XXTTT
== ROW 41.0 ==
-- COL 41.0.0 --

-- COL 41.0.1 --

-- COL 41.0.2 --

-- COL 41.0.3 --
Commission Proposal 
-- COL 41.0.4 --
EP Mandate 
-- COL 41.0.5 --
Council Mandate 
-- COL 41.0.6 --
Draft Agreement
-- COL 41.0.7 --

-- COL 41.0.8 --

== ROW 41.1 ==
-- COL 41.1.0 --

-- COL 41.1.1 --

-- COL 41.1.2 --

-- COL 41.1.3 --

-- COL 41.1.4 --

-- COL 41.1.5 --

-- COL 41.1.6 --
further guidance for the practical  implementation of the criteria  under which AI systems referred to  in Annex III are exceptionally not  high-risk, the Commission should,  after consulting the AI Board,  provide guidelines specifying this  practical implementation completed  by a comprehensive list of practical  examples of high risk and non-high  risk use cases of AI systems.
-- COL 41.1.7 --

-- COL 41.1.8 --

== ROW 41.2 ==
-- COL 41.2.0 --

-- COL 41.2.1 --

-- COL 41.2.2 --
Recital 33
-- COL 41.2.3 --

-- COL 41.2.4 --

-- COL 41.2.5 --

-- COL 41.2.6 --

-- COL 41.2.7 --

-- COL 41.2.8 --

== ROW 41.3 ==
-- COL 41.3.0 --
G 
-- COL 41.3.1 --

-- COL 41.3.2 --
43
-- COL 41.3.3 --
(33) Technical inaccuracies of AI  systems intended for the remote  biometric identification of natural  persons can lead to biased results  and entail discriminatory effects.  This is particularly relevant when it  comes to age, ethnicity, sex or  disabilities. Therefore, ‘real-time’  and ‘post’ remote biometric  
identification systems should be  classified as high-risk. In view of the  risks that they pose, both types of  remote biometric identification  systems should be subject to specific  requirements on logging capabilities  and human oversight.
-- COL 41.3.4 --
deleted
-- COL 41.3.5 --
(33) Technical inaccuracies of AI  systems intended for the remote  biometric identification of natural  persons can lead to biased results  and entail discriminatory effects.  This is particularly relevant when it  comes to age, ethnicity, race, sex or  disabilities. Therefore, ‘real-time’  and ‘post’ remote biometric  
identification systems should be  classified as high-risk. In view of the  risks that they pose, both types of  remote biometric identification  systems should be subject to specific  requirements on logging capabilities  and human oversight.
-- COL 41.3.6 --
(33) Technical inaccuracies of AI  systems intended for the remote  biometric identification of natural  persons can lead to biased results  and entail discriminatory effects.  This is particularly relevant when it  comes to age, ethnicity, sex or  disabilities. Therefore, ‘real-time’  and ‘post’ remote biometric  
identification systems should be  classified as high-risk. In view of the  risks that they pose, both types of  remote biometric identification  systems should be subject to specific  requirements on logging capabilities  and human oversight.
-- COL 41.3.7 --
G
-- COL 41.3.8 --

== ROW 41.4 ==
-- COL 41.4.0 --

-- COL 41.4.1 --

-- COL 41.4.2 --
Recital 33a
-- COL 41.4.3 --

-- COL 41.4.4 --

-- COL 41.4.5 --

-- COL 41.4.6 --

-- COL 41.4.7 --

-- COL 41.4.8 --

== ROW 41.5 ==
-- COL 41.5.0 --
G 
-- COL 41.5.1 --

-- COL 41.5.2 --
43a 
-- COL 41.5.3 --

-- COL 41.5.4 --
(33a) As biometric data constitute a  special category of sensitive 
-- COL 41.5.5 --

-- COL 41.5.6 --
(33a) As biometric data constitutes  a special category of sensitive 
-- COL 41.5.7 --
G
-- COL 41.5.8 --


TTTXX TABLE: 42 XXTTT
== ROW 42.0 ==
-- COL 42.0.0 --

-- COL 42.0.1 --

-- COL 42.0.2 --

-- COL 42.0.3 --
Commission Proposal 
-- COL 42.0.4 --
EP Mandate 
-- COL 42.0.5 --
Council Mandate 
-- COL 42.0.6 --
Draft Agreement
-- COL 42.0.7 --

-- COL 42.0.8 --

== ROW 42.1 ==
-- COL 42.1.0 --

-- COL 42.1.1 --

-- COL 42.1.2 --

-- COL 42.1.3 --

-- COL 42.1.4 --
personal data in accordance with  Regulation 2016/679, it is  
appropriate to classify as high-risk  several critical use-cases of  
biometric and biometrics-based  systems. AI systems intended to be  used for biometric identification of  natural persons and AI systems  intended to be used to make  
inferences about personal  
characteristics of natural persons  on the basis of biometric or  
biometrics-based data, including  emotion recognition systems, with  the exception of those which are  prohibited under this Regulation  should therefore be classified as  high-risk. This should not include  AI systems intended to be used for  biometric verification, which  includes authentication, whose sole  purpose is to confirm that a specific  natural person is the person he or  she claims to be and to confirm the  identity of a natural person for the  sole purpose of having access to a  service, a device or premises (one to-one verification). Biometric and  biometrics-based systems which are  provided for under Union law to  enable cybersecurity and personal  data protection measures should  not be considered as posing a  significant risk of harm to the  health, safety and fundamental  rights.
-- COL 42.1.5 --

-- COL 42.1.6 --
personal data, it is appropriate to  classify as high-risk several critical  use-cases of biometric systems,  insofar as their use is permitted  under relevant Union and national  law. 
Technical inaccuracies of AI  systems intended for the remote  biometric identification of natural  persons can lead to biased results  and entail discriminatory effects.  This is particularly relevant when it  comes to age, ethnicity, race, sex or  disabilities. Therefore, remote  biometric identification systems  should be classified as high-risk in  view of the risks that they pose. This  excludes AI systems intended to be  used for biometric verification,  which includes authentication,  whose sole purpose is to confirm  that a specific natural person is the  person he or she claims to be and to  confirm the identity of a natural  person for the sole purpose of  having access to a service,  
unlocking a device or having secure  access to premises. 
In addition, AI systems intended to  be used for biometric categorisation  according to sensitive attributes or  characteristics protected under  Article 9(1) of Regulation (EU)  2016/679 based on biometric data,  in so far as these are not prohibited  under this Regulation, and emotion 
-- COL 42.1.7 --

-- COL 42.1.8 --


TTTXX TABLE: 43 XXTTT
== ROW 43.0 ==
-- COL 43.0.0 --

-- COL 43.0.1 --

-- COL 43.0.2 --

-- COL 43.0.3 --
Commission Proposal 
-- COL 43.0.4 --
EP Mandate 
-- COL 43.0.5 --
Council Mandate 
-- COL 43.0.6 --
Draft Agreement
-- COL 43.0.7 --

-- COL 43.0.8 --

== ROW 43.1 ==
-- COL 43.1.0 --

-- COL 43.1.1 --

-- COL 43.1.2 --

-- COL 43.1.3 --

-- COL 43.1.4 --

-- COL 43.1.5 --

-- COL 43.1.6 --
recognition systems that are not  prohibited under this Regulation,  should be classified as high-risk.  Biometric systems which are  intended to be used solely for the  purpose of enabling cybersecurity  and personal data protection  measures should not be considered  as high risk systems.
-- COL 43.1.7 --

-- COL 43.1.8 --

== ROW 43.2 ==
-- COL 43.2.0 --

-- COL 43.2.1 --

-- COL 43.2.2 --
Recital 34
-- COL 43.2.3 --

-- COL 43.2.4 --

-- COL 43.2.5 --

-- COL 43.2.6 --

-- COL 43.2.7 --

-- COL 43.2.8 --

== ROW 43.3 ==
-- COL 43.3.0 --
G 
-- COL 43.3.1 --

-- COL 43.3.2 --
44
-- COL 43.3.3 --
(34) As regards the management  and operation of critical  
infrastructure, it is appropriate to  classify as high-risk the AI systems  intended to be used as safety  
components in the management and  operation of road traffic and the  supply of water, gas, heating and  electricity, since their failure or  malfunctioning may put at risk the  life and health of persons at large  scale and lead to appreciable  
disruptions in the ordinary conduct  of social and economic activities.
-- COL 43.3.4 --
(34) As regards the management  and operation of critical  
infrastructure, it is appropriate to  classify as high-risk the AI systems  intended to be used as safety  
components in the management and  operation of road traffic and the  supply of water, gas, heating 
electricity and critical digital  infrastructure and electricity, since  their failure or malfunctioning may  infringe the security and integrity  of such critical infrastructure or put  at risk the life and health of persons  at large scale and lead to appreciable  disruptions in the ordinary conduct  of social and economic activities. Safety components of critical  infrastructure, including critical  digital infrastructure, are systems  used to directly protect the physical  integrity of critical infrastructure or  health and safety of persons and  property. Failure or malfunctioning 
-- COL 43.3.5 --
(34) As regards the management  and operation of critical  
infrastructure, it is appropriate to  classify as high-risk the AI systems  intended to be used as safety  
components in the management and  operation of critical digital  
infrastructure as listed in Annex I  point 8 of the Directive on the  resilience of critical entities, road  traffic and the supply of water, gas,  heating and electricity, since their  failure or malfunctioning may put at  risk the life and health of persons at  large scale and lead to appreciable  disruptions in the ordinary conduct  of social and economic activities. Safety components of critical  infrastructure, including critical  digital infrastrucure, are systems  used to directly protect the physical  integrity of critical infrastructure or  health and safety of persons and  property but which are not 
-- COL 43.3.6 --
(34) As regards the management  and operation of critical  
infrastructure, it is appropriate to  classify as high-risk the AI systems  intended to be used as safety  
components in the management and  operation of critical digital  
infrastructure as listed in Annex I  point 8 of the Directive on the  resilience of critical entities, road  traffic and the supply of water, gas,  heating and electricity, since their  failure or malfunctioning may put at  risk the life and health of persons at  large scale and lead to appreciable  disruptions in the ordinary conduct  of social and economic activities. Safety components of critical  infrastructure, including critical  digital infrastrucure, are systems  used to directly protect the physical  integrity of critical infrastructure or  health and safety of persons and  property but which are not 
-- COL 43.3.7 --
G
-- COL 43.3.8 --


TTTXX TABLE: 44 XXTTT
== ROW 44.0 ==
-- COL 44.0.0 --

-- COL 44.0.1 --

-- COL 44.0.2 --

-- COL 44.0.3 --
Commission Proposal 
-- COL 44.0.4 --
EP Mandate 
-- COL 44.0.5 --
Council Mandate 
-- COL 44.0.6 --
Draft Agreement
-- COL 44.0.7 --

-- COL 44.0.8 --

== ROW 44.1 ==
-- COL 44.1.0 --

-- COL 44.1.1 --

-- COL 44.1.2 --

-- COL 44.1.3 --

-- COL 44.1.4 --
of such components might directly  lead to risks to the physical integrity  of critical infrastructure and thus to  risks to the health and safety of  persons and property. Components  intended to be used solely for  cybersecurity purposes should not  qualify as safety components.  Examples of such safety  
components may include systems  for monitoring water pressure or  fire alarm controlling systems in  cloud computing centres. 
-- COL 44.1.5 --
necessary in order for the system to  function. Failure or malfunctioning  of such components might directly  lead to risks to the physical integrity  of critical infrastructure and thus to  risks to health and safety of persons  and property. Components intended  to be used solely for cybersecurity  purposes should not qualify as  safety components. Examples of  safety components of such critical  infrastructure may include systems  for monitoring water pressure or  fire alarm controlling systems in  cloud computing centres.
-- COL 44.1.6 --
necessary in order for the system to  function. Failure or malfunctioning  of such components might directly  lead to risks to the physical integrity  of critical infrastructure and thus to  risks to health and safety of persons  and property. Components intended  to be used solely for cybersecurity  purposes should not qualify as  safety components. Examples of  safety components of such critical  infrastructure may include systems  for monitoring water pressure or  fire alarm controlling systems in  cloud computing centres.
-- COL 44.1.7 --

-- COL 44.1.8 --

== ROW 44.2 ==
-- COL 44.2.0 --

-- COL 44.2.1 --

-- COL 44.2.2 --
Recital 35
-- COL 44.2.3 --

-- COL 44.2.4 --

-- COL 44.2.5 --

-- COL 44.2.6 --

-- COL 44.2.7 --

-- COL 44.2.8 --

== ROW 44.3 ==
-- COL 44.3.0 --
G 
-- COL 44.3.1 --

-- COL 44.3.2 --
45
-- COL 44.3.3 --
(35) AI systems used in education  or vocational training, notably for  determining access or assigning  persons to educational and  
vocational training institutions or to  evaluate persons on tests as part of  or as a precondition for their  
education should be considered  high-risk, since they may determine  the educational and professional  course of a person’s life and  
therefore affect their ability to secure  their livelihood. When improperly  designed and used, such systems  may violate the right to education  and training as well as the right not  to be discriminated against and  perpetuate historical patterns of 
-- COL 44.3.4 --
(35) Deployment of AI systems in  education is important in order to  help modernise entire education  systems, to increase educational  quality, both offline and online and  to accelerate digital education, thus  also making it available to a  
broader audience. AI systems used  in education or vocational training,  notably for determining access or  materially influence decisions on  admission or assigning persons to  educational and vocational training  institutions or to evaluate persons on  tests as part of or as a precondition  for their education or to assess the  appropriate level of education for  an individual and materially 
-- COL 44.3.5 --
(35) AI systems used in education  or vocational training, notably for  determining access, admission or  assigning persons to educational and  vocational training institutions or to  evaluate persons on tests as part of  or as a precondition for their  educationprogrammes at all levels  or to evaluate learning outcomes of  persons should be considered high risk, since they may determine the  educational and professional course  of a person’s life and therefore affect  their ability to secure their  
livelihood. When improperly  
designed and used, such systems  may violate the right to education  and training as well as the right not 
-- COL 44.3.6 --
(35) Deployment of AI systems in  education is important to promote  high-quality digital education and  training and to allow all learners  
and teachers to acquire and share  the necessary digital skills and  competences, including media  literacy, and critical thinking, to  take an active part in the economy,  society, and in democratic  
processes. However, AI systems  used in education or vocational  training, notably for determining  access or admission, for assigning  persons to educational and  
vocational training institutions or  programmes at all levels, for  evaluating learning outcomes of to 
-- COL 44.3.7 --
G
-- COL 44.3.8 --


TTTXX TABLE: 45 XXTTT
== ROW 45.0 ==
-- COL 45.0.0 --

-- COL 45.0.1 --

-- COL 45.0.2 --

-- COL 45.0.3 --
Commission Proposal 
-- COL 45.0.4 --
EP Mandate 
-- COL 45.0.5 --
Council Mandate 
-- COL 45.0.6 --
Draft Agreement
-- COL 45.0.7 --

-- COL 45.0.8 --

== ROW 45.1 ==
-- COL 45.1.0 --

-- COL 45.1.1 --

-- COL 45.1.2 --

-- COL 45.1.3 --
discrimination. 
-- COL 45.1.4 --
influence the level of education and  training that individuals will receive  or be able to access or to monitor  and detect prohibited behaviour of  students during tests should be  considered high-riskclassified as  high-risk AI systems, since they  may determine the educational and  professional course of a person’s life  and therefore affect their ability to  secure their livelihood. When  improperly designed and used, such  systems can be particularly  
intrusive and may violate the right  to education and training as well as  the right not to be discriminated  against and perpetuate historical  patterns of discrimination, for  example against women, certain  age groups, persons with  
disabilities, or persons of certain  racial or ethnic origins or sexual  orientation.
-- COL 45.1.5 --
to be discriminated against and  perpetuate historical patterns of  discrimination.
-- COL 45.1.6 --
evaluate persons, for assessing the  appropriate level of education for  an individual and materially  influencing the level of education  and training that individuals will  receive or be able to access or for  monitoring and detecting prohibited  behaviour of students during tests on tests as part of or as a  
precondition for their education should be considered high 
riskclassified as high-risk AI  systems, since they may determine  the educational and professional  course of a person’s life and  
therefore affect their ability to secure  their livelihood. When improperly  designed and used, such systems can  be particularly intrusive and may  violate the right to education and  training as well as the right not to be  discriminated against and perpetuate  historical patterns of discrimination,  for example against women, certain  age groups, persons with  
disabilities, or persons of certain  racial or ethnic origins or sexual  orientation.
-- COL 45.1.7 --

-- COL 45.1.8 --

== ROW 45.2 ==
-- COL 45.2.0 --

-- COL 45.2.1 --

-- COL 45.2.2 --
Recital 36
-- COL 45.2.3 --

-- COL 45.2.4 --

-- COL 45.2.5 --

-- COL 45.2.6 --

-- COL 45.2.7 --

-- COL 45.2.8 --

== ROW 45.3 ==
-- COL 45.3.0 --
G 
-- COL 45.3.1 --

-- COL 45.3.2 --
46
-- COL 45.3.3 --
(36) AI systems used in  
employment, workers management  and access to self-employment,  notably for the recruitment and  selection of persons, for making 
-- COL 45.3.4 --
(36) AI systems used in  
employment, workers management  and access to self-employment,  notably for the recruitment and  selection of persons, for making 
-- COL 45.3.5 --
(36) AI systems used in  
employment, workers management  and access to self-employment,  notably for the recruitment and  selection of persons, for making 
-- COL 45.3.6 --
(36) AI systems used in  
employment, workers management  and access to self-employment,  notably for the recruitment and  selection of persons, for making 
-- COL 45.3.7 --
G
-- COL 45.3.8 --


TTTXX TABLE: 46 XXTTT
== ROW 46.0 ==
-- COL 46.0.0 --

-- COL 46.0.1 --

-- COL 46.0.2 --

-- COL 46.0.3 --
Commission Proposal 
-- COL 46.0.4 --
EP Mandate 
-- COL 46.0.5 --
Council Mandate 
-- COL 46.0.6 --
Draft Agreement
-- COL 46.0.7 --

-- COL 46.0.8 --

== ROW 46.1 ==
-- COL 46.1.0 --

-- COL 46.1.1 --

-- COL 46.1.2 --

-- COL 46.1.3 --
decisions on promotion and  
termination and for task allocation,  monitoring or evaluation of persons  in work-related contractual  
relationships, should also be  
classified as high-risk, since those  systems may appreciably impact  future career prospects and  
livelihoods of these persons.  
Relevant work-related contractual  relationships should involve  
employees and persons providing  services through platforms as  referred to in the Commission Work  Programme 2021. Such persons  should in principle not be considered  users within the meaning of this  Regulation. Throughout the  
recruitment process and in the  evaluation, promotion, or retention  of persons in work-related  
contractual relationships, such  systems may perpetuate historical  patterns of discrimination, for  example against women, certain age  groups, persons with disabilities, or  persons of certain racial or ethnic  origins or sexual orientation. AI  systems used to monitor the  
performance and behaviour of these  persons may also impact their rights  to data protection and privacy.
-- COL 46.1.4 --
decisions or materially influence  decisions on initiation, on  
promotion and termination and for  personalised task allocation based  on individual behaviour, personal  traits or biometric data, monitoring  
or evaluation of persons in work related contractual relationships,  should also be classified as high risk, since those systems may  
appreciably impact future career  prospects and, livelihoods of these  persons and workers’ rights.  
Relevant work-related contractual  relationships should meaningfully involve employees and persons  providing services through platforms  as referred to in the Commission  Work Programme 2021. Such  persons should in principle not be  considered users within the meaning  of this Regulation. Throughout the  recruitment process and in the  evaluation, promotion, or retention  of persons in work-related  
contractual relationships, such  systems may perpetuate historical  patterns of discrimination, for  example against women, certain age  groups, persons with disabilities, or  persons of certain racial or ethnic  origins or sexual orientation. AI  systems used to monitor the  
performance and behaviour of these  persons may also impactundermine  the essence of their fundamental  rights to data protection and privacy.
-- COL 46.1.5 --
decisions on promotion and  
termination and for task allocation based on individual behavior or  personal traits or characteristics,  monitoring or evaluation of persons  in work-related contractual  
relationships, should also be  
classified as high-risk, since those  systems may appreciably impact  future career prospects and  
livelihoods of these persons.  
Relevant work-related contractual  relationships should involve  
employees and persons providing  services through platforms as  referred to in the Commission Work  Programme 2021. Such persons  should in principle not be considered  users within the meaning of this  Regulation. Throughout the  
recruitment process and in the  evaluation, promotion, or retention  of persons in work-related  
contractual relationships, such  systems may perpetuate historical  patterns of discrimination, for  example against women, certain age  groups, persons with disabilities, or  persons of certain racial or ethnic  origins or sexual orientation. AI  systems used to monitor the  
performance and behaviour of these  persons may also impact their rights  to data protection and privacy.
-- COL 46.1.6 --
decisions on affecting terms of the  work related relationship  
promotion and termination and for  task allocation, of work-related  contractual relationships for  allocating tasks based on individual  behaviour, personal traits or  characteristics and for monitoring  or evaluation of persons in work related contractual relationships,  should also be classified as high risk, since those systems may  appreciably impact future career  prospects and, livelihoods of these  persons and workers’ rights.  
Relevant work-related contractual  relationships should meaningfully  involve employees and persons  providing services through platforms  as referred to in the Commission  Work Programme 2021. Such  persons should in principle not be  considered users within the meaning  of this Regulation. Throughout the  recruitment process and in the  evaluation, promotion, or retention  of persons in work-related  
contractual relationships, such  systems may perpetuate historical  patterns of discrimination, for  example against women, certain age  groups, persons with disabilities, or  persons of certain racial or ethnic  origins or sexual orientation. AI  systems used to monitor the  
performance and behaviour of these  persons may also impactundermine
-- COL 46.1.7 --

-- COL 46.1.8 --


TTTXX TABLE: 47 XXTTT
== ROW 47.0 ==
-- COL 47.0.0 --

-- COL 47.0.1 --

-- COL 47.0.2 --

-- COL 47.0.3 --
Commission Proposal 
-- COL 47.0.4 --
EP Mandate 
-- COL 47.0.5 --
Council Mandate 
-- COL 47.0.6 --
Draft Agreement
-- COL 47.0.7 --

-- COL 47.0.8 --

== ROW 47.1 ==
-- COL 47.1.0 --

-- COL 47.1.1 --

-- COL 47.1.2 --

-- COL 47.1.3 --

-- COL 47.1.4 --
This Regulation applies without  prejudice to Union and Member  State competences to provide for  more specific rules for the use of  
AI-systems in the employment  context. 
-- COL 47.1.5 --

-- COL 47.1.6 --
their fundamental rights to data  protection and privacy.
-- COL 47.1.7 --

-- COL 47.1.8 --

== ROW 47.2 ==
-- COL 47.2.0 --

-- COL 47.2.1 --

-- COL 47.2.2 --
Recital 37
-- COL 47.2.3 --

-- COL 47.2.4 --

-- COL 47.2.5 --

-- COL 47.2.6 --

-- COL 47.2.7 --

-- COL 47.2.8 --

== ROW 47.3 ==
-- COL 47.3.0 --
G 
-- COL 47.3.1 --

-- COL 47.3.2 --
47
-- COL 47.3.3 --
(37) Another area in which the use  of AI systems deserves special  consideration is the access to and  enjoyment of certain essential  private and public services and  benefits necessary for people to fully  participate in society or to improve  one’s standard of living. In  
particular, AI systems used to  evaluate the credit score or  
creditworthiness of natural persons  should be classified as high-risk AI  systems, since they determine those  persons’ access to financial  
resources or essential services such  as housing, electricity, and  
telecommunication services. AI  systems used for this purpose may  lead to discrimination of persons or  groups and perpetuate historical  patterns of discrimination, for  example based on racial or ethnic  origins, disabilities, age, sexual  orientation, or create new forms of  discriminatory impacts. Considering  the very limited scale of the impact  and the available alternatives on the 
-- COL 47.3.4 --
(37) Another area in which the use  of AI systems deserves special  consideration is the access to and  enjoyment of certain essential  private and public services,  
including healthcare services, and  essential services, including but not  limited to housing, electricity,  heating/cooling and internet, and  benefits necessary for people to fully  participate in society or to improve  one’s standard of living. In  
particular, AI systems used to  evaluate the credit score or  
creditworthiness of natural persons  should be classified as high-risk AI  systems, since they determine those  persons’ access to financial  
resources or essential services such  as housing, electricity, and  
telecommunication services. AI  systems used for this purpose may  lead to discrimination of persons or  groups and perpetuate historical  patterns of discrimination, for  example based on racial or ethnic  origins, gender, disabilities, age, 
-- COL 47.3.5 --
(37) Another area in which the use  of AI systems deserves special  consideration is the access to and  enjoyment of certain essential  private and public services and  benefits necessary for people to fully  participate in society or to improve  one’s standard of living. In  
particular, AI systems used to  evaluate the credit score or  
creditworthiness of natural persons  should be classified as high-risk AI  systems, since they determine those  persons’ access to financial  
resources or essential services such  as housing, electricity, and  
telecommunication services. AI  systems used for this purpose may  lead to discrimination of persons or  groups and perpetuate historical  patterns of discrimination, for  example based on racial or ethnic  origins, disabilities, age, sexual  orientation, or create new forms of  discriminatory impacts. Considering  the very limited scale of the impact  and the available alternatives on the 
-- COL 47.3.6 --
(37) Another area in which the use  of AI systems deserves special  consideration is the access to and  enjoyment of certain essential  private and public services and  benefits necessary for people to fully  participate in society or to improve  one’s standard of living. 
In particular, AI systems used to  evaluate the credit score or  
creditworthiness of natural persons  should be classified as high-risk AI  systems, since they determine those  persons’ access to financial  
resources or essentialapplying for  or receiving essential public  
assistance benefits and services  from public authorities namely  healthcare services, social security  benefits, social such as housing,  electricity, and telecommunication services. AI systems used for this  purpose may lead to discrimination  of persons or groups and perpetuate  historical patterns of discrimination,  for example based on racial or ethnic origins, disabilities, age, 
-- COL 47.3.7 --
G
-- COL 47.3.8 --


TTTXX TABLE: 48 XXTTT
== ROW 48.0 ==
-- COL 48.0.0 --

-- COL 48.0.1 --

-- COL 48.0.2 --

-- COL 48.0.3 --
Commission Proposal 
-- COL 48.0.4 --
EP Mandate 
-- COL 48.0.5 --
Council Mandate 
-- COL 48.0.6 --
Draft Agreement
-- COL 48.0.7 --

-- COL 48.0.8 --

== ROW 48.1 ==
-- COL 48.1.0 --

-- COL 48.1.1 --

-- COL 48.1.2 --

-- COL 48.1.3 --
market, it is appropriate to exempt  AI systems for the purpose of  creditworthiness assessment and  credit scoring when put into service  by small-scale providers for their  own use. Natural persons applying  for or receiving public assistance  benefits and services from public  authorities are typically dependent  on those benefits and services and in  a vulnerable position in relation to  the responsible authorities. If AI  systems are used for determining  whether such benefits and services  should be denied, reduced, revoked  or reclaimed by authorities, they  may have a significant impact on  persons’ livelihood and may infringe  their fundamental rights, such as the  right to social protection, non discrimination, human dignity or an  effective remedy. Those systems  should therefore be classified as  high-risk. Nonetheless, this  
Regulation should not hamper the  development and use of innovative  approaches in the public  
administration, which would stand to  benefit from a wider use of  
compliant and safe AI systems,  provided that those systems do not  entail a high risk to legal and natural  persons. Finally, AI systems used to  dispatch or establish priority in the  dispatching of emergency first  response services should also be  classified as high-risk since they 
-- COL 48.1.4 --
sexual orientation, or create new  forms of discriminatory impacts.  Considering the very limited scale of  the impact and the available  
alternatives on the market, it is  appropriate to exempt AI  
systemsHowever, AI systems  
provided for by Union law for the  purpose of creditworthiness  
assessment and credit scoring when  put into service by small-scale  providers for their own usedetecting  fraud in the offering of financial  services should not be considered as  high-risk under this Regulation.  Natural persons applying for or  receiving public assistance benefits  and services from public authorities,  including healthcare services and  essential services, including but not  limited to housing, electricity,  heating/cooling and internet, are  typically dependent on those benefits  and services and in a vulnerable  position in relation to the responsible  authorities. If AI systems are used  for determining whether such  benefits and services should be  denied, reduced, revoked or  
reclaimed by authorities, they may  have a significant impact on  
persons’ livelihood and may infringe  their fundamental rights, such as the  right to social protection, non discrimination, human dignity or an  effective remedy. Similarly, AI  systems intended to be used to make 
-- COL 48.1.5 --
market, it is appropriate to exempt  AI systems for the purpose of  creditworthiness assessment and  credit scoring when put into service  by small-scale providersmicro or  small entreprises, as defined in the  Annex of Commission  
Recommendation 2003/361/EC for  their own use. Natural persons  applying for or receiving essential public assistance benefits and  services from public authorities are  typically dependent on those benefits  and services and in a vulnerable  position in relation to the responsible  authorities. If AI systems are used  for determining whether such  benefits and services should be  denied, reduced, revoked or  
reclaimed by authorities,  
theyincluding whether beneficiaries  are legitimately entitled to such  benefits or services, those systems may have a significant impact on  persons’ livelihood and may infringe  their fundamental rights, such as the  right to social protection, non discrimination, human dignity or an  effective remedy. Those systems  should therefore be classified as  high-risk. Nonetheless, this  
Regulation should not hamper the  development and use of innovative  approaches in the public  
administration, which would stand to  benefit from a wider use of  
compliant and safe AI systems, 
-- COL 48.1.6 --
sexual orientation, or create new  forms of discriminatory impacts.  Considering the very limited scale of  the impact and the available  
alternatives on the market, it is  appropriate to exempt AI systems for  the purpose of creditworthiness  assessment and credit scoring when  put into service by small-scale  providers for their own use. Natural  persons applying for or receiving  public providing protection in cases  such as maternity, illness, industrial  accidents, dependency or old age  and loss of employment and social  and housing assistance benefits and  services from public authorities, are  typically dependent on those benefits  and services and in a vulnerable  position in relation to the responsible  authorities. If AI systems are used  for determining whether such  benefits and services should be  granted, denied, reduced, revoked or  reclaimed by authorities,  
theyincluding whether beneficiaries  are legitimately entitled to such  benefits or services, those systems may have a significant impact on  persons’ livelihood and may infringe  their fundamental rights, such as the  right to social protection, non discrimination, human dignity or an  effective remedy. Those systems and should therefore be classified as  high-risk. Nonetheless, this  
Regulation should not hamper the 
-- COL 48.1.7 --

-- COL 48.1.8 --


TTTXX TABLE: 49 XXTTT
== ROW 49.0 ==
-- COL 49.0.0 --

-- COL 49.0.1 --

-- COL 49.0.2 --

-- COL 49.0.3 --
Commission Proposal 
-- COL 49.0.4 --
EP Mandate 
-- COL 49.0.5 --
Council Mandate 
-- COL 49.0.6 --
Draft Agreement
-- COL 49.0.7 --

-- COL 49.0.8 --

== ROW 49.1 ==
-- COL 49.1.0 --

-- COL 49.1.1 --

-- COL 49.1.2 --

-- COL 49.1.3 --
make decisions in very critical  situations for the life and health of  persons and their property.
-- COL 49.1.4 --
decisions or materially influence  decisions on the eligibility of  natural persons for health and life  insurance may also have a  
significant impact on persons’  livelihood and may infringe their  fundamental rights such as by  limiting access to healthcare or by  perpetuating discrimination based  on personal characteristics. Those  systems should therefore be  
classified as high-risk. Nonetheless,  this Regulation should not hamper  the development and use of  
innovative approaches in the public  administration, which would stand to  benefit from a wider use of  
compliant and safe AI systems,  provided that those systems do not  entail a high risk to legal and natural  persons. Finally, AI systems used to  evaluate and classify emergency  calls by natural persons or to  dispatch or establish priority in the  dispatching of emergency first  response services should also be  classified as high-risk since they  make decisions in very critical  situations for the life and health of  persons and their property.
-- COL 49.1.5 --
provided that those systems do not  entail a high risk to legal and natural  persons. Finally, AI systems used to  dispatch or establish priority in the  dispatching of emergency first  response services should also be  classified as high-risk since they  make decisions in very critical  situations for the life and health of  persons and their property. AI  systems are also increasingly used  for risk assessment in relation to  natural persons and pricing in the  case of life and health insurance  which, if not duly designed,  
developed and used, can lead to  serious consequences for people’s  life and health, including financial  exclusion and discrimination. To  ensure a consistent approach within  the financial services sector, the  above mentioned exception for  micro or small enterprises for their  own use should apply, insofar as  they themselves provide and put  into service an AI system for the  purpose of selling their own  
insurance products.
-- COL 49.1.6 --
development and use of innovative  approaches in the public  
administration, which would stand to  benefit from a wider use of  
compliant and safe AI systems,  provided that those systems do not  entail a high risk to legal and natural  persons. 
In addition, AI systems used to  evaluate the credit score or  
creditworthiness of natural persons  should be classified as high-risk AI  systems, since they determine those  persons’ access to financial  
resources or essential services such  as housing, electricity, and  
telecommunication services. AI  systems used for this purpose may  lead to discrimination of persons or  groups and perpetuate historical  patterns of discrimination, for  example based on racial or ethnic  origins, gender, disabilities, age,  sexual orientation, or create new  forms of discriminatory impacts.  However, AI systems provided for  by Union law for the purpose of  detecting fraud in the offering of  financial services and for  
prudential purposes to calculate  credit institutions’ and insurances  undertakings’ capital requirements  should not be considered as high risk under this Regulation.  
Moreover, AI systems intended to  be used for risk assessment and  pricing in relation to natural 
-- COL 49.1.7 --

-- COL 49.1.8 --


TTTXX TABLE: 50 XXTTT
== ROW 50.0 ==
-- COL 50.0.0 --

-- COL 50.0.1 --

-- COL 50.0.2 --

-- COL 50.0.3 --
Commission Proposal 
-- COL 50.0.4 --
EP Mandate 
-- COL 50.0.5 --
Council Mandate 
-- COL 50.0.6 --
Draft Agreement
-- COL 50.0.7 --

-- COL 50.0.8 --

== ROW 50.1 ==
-- COL 50.1.0 --

-- COL 50.1.1 --

-- COL 50.1.2 --

-- COL 50.1.3 --

-- COL 50.1.4 --

-- COL 50.1.5 --

-- COL 50.1.6 --
persons for health and life  
insurance can also have a  
significant impact on persons’  livelihood and if not duly designed,  developed and used, can infringe  their fundamental rights and can  lead to serious consequences for  people’s life and health, including  financial exclusion and  
discrimination.  
Finally, AI systems used to evaluate  and classify emergency calls by  natural persons or to dispatch or  establish priority in the dispatching  of emergency first response services,  including by police, firefighters and  medical aid, as well as of  
emergency healthcare patient triage  systems, should also be classified as  high-risk since they make decisions  in very critical situations for the life  and health of persons and their  property.
-- COL 50.1.7 --

-- COL 50.1.8 --

== ROW 50.2 ==
-- COL 50.2.0 --

-- COL 50.2.1 --

-- COL 50.2.2 --
Recital 37a
-- COL 50.2.3 --

-- COL 50.2.4 --

-- COL 50.2.5 --

-- COL 50.2.6 --

-- COL 50.2.7 --

-- COL 50.2.8 --

== ROW 50.3 ==
-- COL 50.3.0 --
G 
-- COL 50.3.1 --

-- COL 50.3.2 --
47a
-- COL 50.3.3 --

-- COL 50.3.4 --
(37a) Given the role and  
responsibility of police and judicial  authorities, and the impact of  decisions they take for the purposes  of the prevention, investigation,  detection or prosecution of criminal  offences or the execution of  
criminal penalties, some specific  use-cases of AI applications in law  enforcement has to be classified as 
-- COL 50.3.5 --

-- COL 50.3.6 --

-- COL 50.3.7 --
G
-- COL 50.3.8 --
