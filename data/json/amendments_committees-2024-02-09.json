[
    {
        "uuid":"99cefc72-efad-44c6-aaeb-397d0b35dec1",
        "amendment_number":1,
        "author":"TRAN",
        "title":"Recital 1",
        "text":"(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI- based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless (1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety, fundamental rights and the environment, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by explicitly authorised by this Regulation. this Regulation."
    },
    {
        "uuid":"44fff5b3-2cdb-404a-9661-c562b3117177",
        "amendment_number":2,
        "author":"TRAN",
        "title":"Recital 5",
        "text":"(5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33 , and it ensures the protection of ethical principles, as specifically requested by the European Parliament34 . (5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety, the environment, and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33 , and it ensures the protection of ethical principles, as specifically requested by the European Parliament34. __________________ __________________ 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL). 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL). PE730.085v02-00 6\/70"
    },
    {
        "uuid":"afed2871-752d-4be8-8e6b-e8eb42e51ab2",
        "amendment_number":3,
        "author":"TRAN",
        "title":"Recital 5 a (new)",
        "text":"(5a) Union legislation on artificial intelligence should contribute to the dual green and digital transition. The artificial intelligence can contribute positively to the green transition but also has significant environmental impacts due to the critical raw material required to design and build its infrastructure and microprocessors and the energy used for its development, training, tuning and use. Development and use of AI should therefore be compatible with sustainable environmental resources at all stages of the lifecycle of AI systems. Also, unnecessary data acquisition and processing should be avoided. Moreover, Union legislation on artificial intelligence should be accompanied by actions aimed at addressing the main barriers hindering the digital transformation of the economy. Such measures should focus on education, upskilling and reskilling of workers, fostering investment in research and innovation, and boosting security in the digital sphere in line with initiatives aimed at achieving the targets of the Digital Decade. Digital transformation should occur in a harmonized manner across regions, paying particular attention to less digitally developed areas of the Union."
    },
    {
        "uuid":"06c039c8-09a3-44dd-b28c-c49f93861952",
        "amendment_number":4,
        "author":"TRAN",
        "title":"Recital 5 b (new)",
        "text":"(5b) Harmonised Union legislation on artificial intelligence can contribute to create legal certainty and coherence across the Union. However, due to risks associated with passenger and goods transport, the sector has been carefully monitored and regulated to avoid incidents and loss of life. The Union legal framework for transport presents sectoral legislation for the aviation, road, rail and maritime transport. With the progressive integration of AI systems in the sector, new challenges could emerge in risk management. This Regulation should only apply to high risk applications in the transport sector in so far as that they are not already covered by sectoral legislation and where they could have a harmful impact on the environment or health, safety and fundamental rights of persons. Double regulation should therefore be avoided."
    },
    {
        "uuid":"476159fd-3ca5-4b6a-bf48-0842d2a54032",
        "amendment_number":5,
        "author":"TRAN",
        "title":"Recital 5 c (new)",
        "text":"(5c) The Union aviation sector, for example, through the work of the European Aviation Safety Agency (EASA) and its stakeholders, is gradually developing its own guidance material and rules on the application and security management of AI systems in aviation. In the EASA’s roadmap for AI, AI systems with application to aviation are categorised in three distinct levels, from assistance to human, to human-machine cooperation, to full machine automation. A sector-specific oversight on AI systems laying out rules for the highest-level of safety for aviation while preserving the global competitiveness of Union businesses is needed. PE730.085v02-00 8\/70"
    },
    {
        "uuid":"dcdcb926-347f-45a9-80b2-a4c23a45b813",
        "amendment_number":6,
        "author":"TRAN",
        "title":"Recital 6",
        "text":"(6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand- alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list. (6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, and possibly also the hardware.1a In particular, for the purpose of this Regulation, AI systems should be intended as having the ability, on the basis of machine- and\/or human-based data and inputs, to infer the way to achieve a given set of human-defined objectives through learning, reasoning or modelling and generate specific outputs in the form of content for generative AI systems, as well as predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand- alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list. __________________ 1a https:\/\/digital- strategy.ec.europa.eu\/en\/library\/definition -artificial-intelligence-main-capabilities- and-scientific-disciplines"
    },
    {
        "uuid":"7debc7bd-b2b3-45fa-b56e-9b4a8f40e9ce",
        "amendment_number":7,
        "author":"TRAN",
        "title":"Recital 8 a (new)",
        "text":"(8a) The use of biometrics and high technologies in transport and tourism may vastly benefit user experience and overall safety and security. This Regulation should accompany these developments by setting the highest level of protection, in particular when use of biometrics data is involved, in line with the data protection framework of the Union, while fostering research and investment for the development and deployment of AI systems that can positively contribute to society."
    },
    {
        "uuid":"6c662bc9-b65d-41fb-8bfd-99c226a9781c",
        "amendment_number":8,
        "author":"TRAN",
        "title":"Recital 12 a (new)",
        "text":"(12a) This Regulation should support research and innovation for the application of AI systems in the transport and tourism sectors while ensuring a high level of protection of public interests, such as health, safety, fundamental rights, the environment and democracy. For this reason, this Regulation should exclude from its scope applications of AI systems developed, applied and assessed in a controlled testing environment, for the sole purpose of evaluating their use and functionality. As regards product oriented research activity by providers, the provisions of this Regulation should apply insofar as such research leads to or entails placing an AI system on the market or putting it into service. All forms of research and development should be conducted in compliance with the highest PE730.085v02-00 10\/70 ethical standards for scientific research."
    },
    {
        "uuid":"ff66d6e4-56ee-4789-9c4c-e18d1b9a35c3",
        "amendment_number":9,
        "author":"TRAN",
        "title":"Recital 13",
        "text":"(13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments. (13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights and the environment, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments. This is of particular importance in the transport sector in order to ensure the highest level of interoperability among transport vehicles, infrastructure and intelligent systems and to guarantee safety and security. The Union and its standards organisations should participate actively in the development of global standards for the different transport modes with a view to align them as much as possible with any applicable European standards and to ensure that they are in compliance with Union law. Regular reviews of this Regulation should take into account updated standards for the transport sector."
    },
    {
        "uuid":"237434f2-7b91-4501-92ba-069ac77166cf",
        "amendment_number":10,
        "author":"TRAN",
        "title":"Recital 17 a (new)",
        "text":"(17a) The use of AI in work can be beneficial to both the management and operations of an enterprise, supporting workers in their tasks and improving safety at the workplace. Still, AI systems applied to the management of workers, in particular by digital labour platforms, including in the field of transport, can entail a number of risks such as unjust\/unnecessary social scoring, rooted in biased data sets or intrusive surveillance practice which can lead to violation of workers’ and fundamental rights. This Regulation should therefore aim at protecting the rights of transport workers managed with the assistance of AI systems, including those working via digital labour platforms and promote transparency, fairness and accountability in algorithmic management, to ensure that workers have a broad understanding of how algorithms work, which personal data is issued and how their behaviour affects decisions taken by the automated system."
    },
    {
        "uuid":"502ba5ed-5781-4b27-83d1-257754ee0037",
        "amendment_number":11,
        "author":"TRAN",
        "title":"Recital 17 b (new)",
        "text":"(17b) In addition, users and individuals should have the right to object to a decision taken solely by an AI system, or relying to a significant degree on the output of an AI system, which produces legal effects concerning them, or similarly significantly affects them."
    },
    {
        "uuid":"5b26fbf4-23d2-4598-85ec-fa6bc93f6233",
        "amendment_number":12,
        "author":"TRAN",
        "title":"Recital 27",
        "text":"(27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any. (27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union or the environment and such limitation minimises any potential restriction to international trade, if any."
    },
    {
        "uuid":"cf732303-d087-4740-8f83-1555098ca04e",
        "amendment_number":13,
        "author":"TRAN",
        "title":"Recital 29",
        "text":"(29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300\/2008 of the European Parliament and of the Council39 , Regulation (EU) No 167\/2013 of the European Parliament and of the Council40 , Regulation (EU) No 168\/2013 of the European Parliament and of the Council41 , Directive 2014\/90\/EU of the European Parliament and of the Council42 , Directive (EU) 2016\/797 of the European Parliament and of the Council43 , Regulation (EU) 2018\/858 of the European Parliament and of the Council44 , Regulation (EU) 2018\/1139 of the European Parliament and of the Council45 , and Regulation (EU) 2019\/2144 of the European Parliament and of the Council46 , it is appropriate to amend (29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300\/2008 of the European Parliament and of the Council39 , Regulation (EU) No 167\/2013 of the European Parliament and of the Council40 , Regulation (EU) No 168\/2013 of the European Parliament and of the Council41 , Directive 2014\/90\/EU of the European Parliament and of the Council42 , Directive (EU) 2016\/797 of the European Parliament and of the Council43 , Regulation (EU) 2018\/858 of the European Parliament and of the Council44 , Regulation (EU) 2018\/1139 of the European Parliament and of the Council45 , and Regulation (EU) 2019\/2144 of the European Parliament and of the Council46 , it is appropriate, if those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts. required, to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without overlapping with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts. Transport sectoral legislation should prevail over this Regulation and it should be ensured that no conflicting overlap exists between this Regulation and other current and upcoming legal acts (i.e. Data Act, ITS Review) to avoid duplication of obligations on providers and manufacturers, which would cause legal uncertainty for business and slow down the uptake of new technologies in the market. This Regulation should also provide for an efficient review mechanism in order to take into account future technological developments and to ensure fair, proportionate and targeted implementation. In order to avoid substantial legal uncertainty, and to ensure that this Regulation applies to all sectors concerned by it without undue delays, those acts should be amended to integrate the provisions of this Regulation no later than 24 months after its entry into force. __________________ __________________ 39 Regulation (EC) No 300\/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of civil aviation security and repealing Regulation (EC) No 2320\/2002 (OJ L 97, 9.4.2008, p. 72). 39 Regulation (EC) No 300\/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of civil aviation security and repealing Regulation (EC) No 2320\/2002 (OJ L 97, 9.4.2008, p. 72). 40 Regulation (EU) No 167\/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1). 40 Regulation (EU) No 167\/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and market surveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1). PE730.085v02-00 14\/70 41 Regulation (EU) No 168\/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52). 41 Regulation (EU) No 168\/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52). 42 Directive 2014\/90\/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96\/98\/EC (OJ L 257, 28.8.2014, p. 146). 42 Directive 2014\/90\/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96\/98\/EC (OJ L 257, 28.8.2014, p. 146). 43 Directive (EU) 2016\/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44). 43 Directive (EU) 2016\/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. 44). 44 Regulation (EU) 2018\/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended for such vehicles, amending Regulations (EC) No 715\/2007 and (EC) No 595\/2009 and repealing Directive 2007\/46\/EC (OJ L 151, 14.6.2018, p. 1). 44 Regulation (EU) 2018\/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers, and of systems, components and separate technical units intended for such vehicles, amending Regulations (EC) No 715\/2007 and (EC) No 595\/2009 and repealing Directive 2007\/46\/EC (OJ L 151, 14.6.2018, p. 1). 45 Regulation (EU) 2018\/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111\/2005, (EC) No 1008\/2008, (EU) No 996\/2010, (EU) No 376\/2014 and Directives 2014\/30\/EU and 2014\/53\/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552\/2004 and (EC) No 216\/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1). 45 Regulation (EU) 2018\/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111\/2005, (EC) No 1008\/2008, (EU) No 996\/2010, (EU) No 376\/2014 and Directives 2014\/30\/EU and 2014\/53\/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552\/2004 and (EC) No 216\/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1). 46 Regulation (EU) 2019\/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such 46 Regulation (EU) 2019\/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018\/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78\/2009, (EC) No 79\/2009 and (EC) No 661\/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631\/2009, (EU) No 406\/2010, (EU) No 672\/2010, (EU) No 1003\/2010, (EU) No 1005\/2010, (EU) No 1008\/2010, (EU) No 1009\/2010, (EU) No 19\/2011, (EU) No 109\/2011, (EU) No 458\/2011, (EU) No 65\/2012, (EU) No 130\/2012, (EU) No 347\/2012, (EU) No 351\/2012, (EU) No 1230\/2012 and (EU) 2015\/166 (OJ L 325, 16.12.2019, p. 1). vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018\/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78\/2009, (EC) No 79\/2009 and (EC) No 661\/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631\/2009, (EU) No 406\/2010, (EU) No 672\/2010, (EU) No 1003\/2010, (EU) No 1005\/2010, (EU) No 1008\/2010, (EU) No 1009\/2010, (EU) No 19\/2011, (EU) No 109\/2011, (EU) No 458\/2011, (EU) No 65\/2012, (EU) No 130\/2012, (EU) No 347\/2012, (EU) No 351\/2012, (EU) No 1230\/2012 and (EU) 2015\/166 (OJ L 325, 16.12.2019, p. 1)."
    },
    {
        "uuid":"1e0af734-7c60-4776-96a3-c10b8d89ae6c",
        "amendment_number":14,
        "author":"TRAN",
        "title":"Recital 32",
        "text":"(32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems. (32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons or the environment, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems."
    },
    {
        "uuid":"065e6979-2c9a-436d-9807-321b641c04a9",
        "amendment_number":15,
        "author":"TRAN",
        "title":"Recital 34",
        "text":"(34) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities. (34) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities. Some examples of critical infrastructure management systems for road covered by Annex III should include traffic management control systems, intelligent transport systems and ICT infrastructure connected transport."
    },
    {
        "uuid":"6dde90d5-7ea7-4b35-a8fd-9f78dd291bfa",
        "amendment_number":16,
        "author":"TRAN",
        "title":"Recital 37",
        "text":"(37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and (37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of discriminatory impacts. Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use. Natural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non- discrimination, human dignity or an effective remedy. Those systems should therefore be classified as high-risk. Nonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons. Finally, AI systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high- risk since they make decisions in very critical situations for the life and health of persons and their property. perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, gender, disabilities, age, sexual orientation, or create new forms of discriminatory impacts. Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use. Natural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non- discrimination, human dignity or an effective remedy. Those systems should therefore be classified as high-risk. Nonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons. Finally, AI systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high- risk since they make decisions in very critical situations for the life and health of persons and their property."
    },
    {
        "uuid":"a6be0513-7cd1-4923-b906-d96903cccaf0",
        "amendment_number":17,
        "author":"TRAN",
        "title":"Recital 43",
        "text":"(43) Requirements should apply to high- risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety and fundamental rights, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade. (43) Requirements should apply to high- risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety, fundamental rights and the environment, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade."
    },
    {
        "uuid":"3e3f16cd-e989-4cec-bde9-afce7c9b174c",
        "amendment_number":18,
        "author":"TRAN",
        "title":"Recital 44",
        "text":"(44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the (44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative, up to date and, to the best extent possible free of errors and as complete as possible in view of the intended purpose of the system and to ensure the highest level of security. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high- risk AI systems. data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers should ensure that databases contain adequate data on groups which are more vulnerable to discriminatory effects posed by AI, such as people with disabilities, and be able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection, update, and correction in relation to high-risk AI systems."
    },
    {
        "uuid":"ec35b235-8e21-4104-bcb6-94c0624c5d37",
        "amendment_number":19,
        "author":"TRAN",
        "title":"Recital 47",
        "text":"(47) To address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems. Users should be able to interpret the system output and use it appropriately. High-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate. (47) To address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems, in particular when applied to digital labour platforms managing the activities of transport workers. Users should be able to interpret the system output and use it appropriately. Transparency, fairness, accountability and explanability of AI systems can also be a beneficial factor for their uptake by consumers in the market. High-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate. PE730.085v02-00 20\/70"
    },
    {
        "uuid":"1d366628-f57a-47d6-b737-32cf09772a78",
        "amendment_number":20,
        "author":"TRAN",
        "title":"Recital 47 a (new)",
        "text":"(47a) Based on previous experience, it is particularly important to ensure clear requirements and guidelines for interoperability between AI systems both within and amongst different economic sectors, contributing to foster innovation and providing favourable conditions for small and medium enterprises (SMEs)."
    },
    {
        "uuid":"eb90b99f-41a4-4a7c-9f17-f71ce6678a78",
        "amendment_number":21,
        "author":"TRAN",
        "title":"Recital 48",
        "text":"(48) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning. For this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in- built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. (48) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning, unless there is clear evidence that it doesn't add value and could even be detrimental to the protection of health, safety and fundamental rights. For this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. With regards to the transport sector, the AI system applications should respect the sector-specific legislation in place. When physical security is at stake, Union standards, and where applicable international standards, should determine in which case the possibility for a human operator to take back control should take prevalence over AI system’s decision."
    },
    {
        "uuid":"a05851f9-3520-4d6d-ba09-5d7bfbf95eb0",
        "amendment_number":22,
        "author":"TRAN",
        "title":"Recital 51",
        "text":"(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure. (51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, as well as the notified bodies, competent national authorities and market surveillance authorities accessing the data of providers of high risk AI systems, also taking into account as appropriate the underlying ICT infrastructure."
    },
    {
        "uuid":"008ee1d1-4099-4685-a246-c4e245dd58b2",
        "amendment_number":23,
        "author":"TRAN",
        "title":"Recital 54",
        "text":"(54) The provider should establish a (54) The provider should establish a PE730.085v02-00 22\/70 sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority in question. sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation in the language of the Member State concerned and establish a robust post-market monitoring system. All elements, from design to future development, should be made transparent for the user. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority in question."
    },
    {
        "uuid":"75c4f9cf-61f2-4612-8256-9758ff5420ba",
        "amendment_number":24,
        "author":"TRAN",
        "title":"Recital 59",
        "text":"(59) It is appropriate to envisage that the user of the AI system should be the natural or legal person, public authority, agency or other body under whose authority the AI system is operated except where the use is made in the course of a personal non- professional activity. (59) It is appropriate to envisage that the user of the AI system should be the natural or legal person, public authority, agency or other body under whose authority the AI system is operated."
    },
    {
        "uuid":"0e771765-551a-4845-a387-851a0f27cf15",
        "amendment_number":25,
        "author":"TRAN",
        "title":"Recital 71",
        "text":"(71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring (71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes and make such regulatory sandboxes widely available throughout the Union, in order to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. It is especially important to ensure that SMEs and start- ups can easily access these sandboxes, are actively involved and participate in the development and testing of innovative AI systems, in order to be able to contribute with their knowhow and experience. Their participation should be supported and facilitated."
    },
    {
        "uuid":"a3acfde6-a8c1-4774-b011-e919a59cf5f8",
        "amendment_number":26,
        "author":"TRAN",
        "title":"Recital 72",
        "text":"(72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups. To ensure uniform implementation across the Union and (72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises and start- ups, as well as to contribute to achieving the targets on AI as set in the Policy PE730.085v02-00 24\/70 economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016\/679, and Article 6 of Regulation (EU) 2018\/1725, and without prejudice to Article 4(2) of Directive (EU) 2016\/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016\/679 and Article 57 of Directive 2016\/680. Programme “Path to the Digital Decade\". To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016\/679, and Article 6 of Regulation (EU) 2018\/1725, and without prejudice to Article 4(2) of Directive (EU) 2016\/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016\/679 and Article 57 of Directive 2016\/680."
    },
    {
        "uuid":"893be504-6769-47e1-b759-d09024fe2d9c",
        "amendment_number":27,
        "author":"TRAN",
        "title":"Recital 73",
        "text":"(73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs (73) In order to promote and protect innovation, it is important that the interests of SMEs and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of SMEs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users. should be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users."
    },
    {
        "uuid":"4923c062-1b9c-4314-806b-293398e8ba2a",
        "amendment_number":28,
        "author":"TRAN",
        "title":"Recital 76",
        "text":"(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. (76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. In order to ensure a common and consistent approach to the development of AI and ensure good cooperation and exchange of views, the Board should regularly consult other EU institutions, as well as all sector-specific relevant stakeholders."
    },
    {
        "uuid":"907b4297-91da-46f8-9e0e-86d1e6b52bf2",
        "amendment_number":29,
        "author":"TRAN",
        "title":"Recital 77 a (new)",
        "text":"(77a) To encourage knowledge sharing from best practices, the Commission should organise regular consultative meetings for knowhow exchange between different Member States' national authorities responsible for notification policy."
    },
    {
        "uuid":"bfbe2298-3952-4200-850f-f46a01cb1836",
        "amendment_number":30,
        "author":"TRAN",
        "title":"Article 1 – paragraph 1 – point e",
        "text":"(e) rules on market monitoring and surveillance. (e) rules on market monitoring, market surveillance and governance."
    },
    {
        "uuid":"17551ed9-98b2-49e5-b46f-b4121088c166",
        "amendment_number":31,
        "author":"TRAN",
        "title":"Article 1 – paragraph 1 – point e a (new)",
        "text":"(ea) provision to foster and support research and development for innovation."
    },
    {
        "uuid":"91fe242f-10ae-493a-8ca0-91509b8e5f75",
        "amendment_number":32,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – introductory part",
        "text":"2. For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of the following acts, only Article 84 of this 2. For AI systems classified as high- risk AI in accordance with Article 6 related to products covered by Union harmonisation legislation listed in Annex II, section B, only Article 84 of this Regulation shall apply: Regulation shall apply:"
    },
    {
        "uuid":"a2a9e97a-8e64-4c69-af7d-b72b7b652521",
        "amendment_number":33,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point a",
        "text":"(a) Regulation (EC) 300\/2008; deleted"
    },
    {
        "uuid":"8d0c3d85-dd8d-4358-9569-f8f899ac7323",
        "amendment_number":34,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point b",
        "text":"(b) Regulation (EU) No 167\/2013; deleted"
    },
    {
        "uuid":"0981319d-89ea-4012-897e-aaa58804f862",
        "amendment_number":35,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point c",
        "text":"(c) Regulation (EU) No 168\/2013; deleted"
    },
    {
        "uuid":"030921c1-91e7-4598-b93d-f3388c407b27",
        "amendment_number":36,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point d",
        "text":"(d) Directive 2014\/90\/EU; deleted"
    },
    {
        "uuid":"c47b8e70-6017-4ff6-9b6e-7f6defc563c3",
        "amendment_number":37,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point e",
        "text":"(e) Directive (EU) 2016\/797; deleted"
    },
    {
        "uuid":"30a76142-d829-472e-9719-34ac019e3b89",
        "amendment_number":38,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point f",
        "text":"(f) Regulation (EU) 2018\/858; deleted"
    },
    {
        "uuid":"6fb0b6c0-606d-4efa-a48d-afe012de5e12",
        "amendment_number":39,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point g",
        "text":"(g) Regulation (EU) 2018\/1139; deleted"
    },
    {
        "uuid":"aa33c2a8-1bff-42cf-ab08-58c70fc6d857",
        "amendment_number":40,
        "author":"TRAN",
        "title":"Article 2 – paragraph 2 – point h",
        "text":"(h) Regulation (EU) 2019\/2144. deleted"
    },
    {
        "uuid":"f70f3635-baed-42e1-ae2f-36d8c99e1643",
        "amendment_number":41,
        "author":"TRAN",
        "title":"Article 2 – paragraph 5 a (new)",
        "text":"5a. This Regulation shall not apply to AI systems, including their output, developed and put into service for the sole purpose of research and development."
    },
    {
        "uuid":"4f8d620d-6e99-4320-b706-ad9224bd022b",
        "amendment_number":42,
        "author":"TRAN",
        "title":"Article 2 – paragraph 5 b (new)",
        "text":"5b. This Regulation shall not apply to any research and development activity regarding AI systems in so far as such activity does not lead to or require placing an AI system on the market or putting it into service and is in full respect of approved scientific ethical standards."
    },
    {
        "uuid":"bc706e1d-4f72-4f9c-bb48-f496a6840f61",
        "amendment_number":43,
        "author":"TRAN",
        "title":"Article 2 – paragraph 5 c (new)",
        "text":"5c. This Regulation is without prejudice to the rules laid down by other Union legal acts regulating the protection of personal data, in particular Regulation (EU) 2016\/679, Regulation (EU) 2018\/1725, Directive 2002\/57\/EC and Directive (EU) 2016\/680."
    },
    {
        "uuid":"fa3e33ed-01e8-4abb-b0ce-aeae0ba33669",
        "amendment_number":44,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 1",
        "text":"(1) ‘artificial intelligence system’ (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with; (1) ‘artificial intelligence system’ (AI system) means a system that: PE730.085v02-00 30\/70 i) receives machine and\/or human- based data and inputs, ii) infers how to achieve a given set of human-defined objectives using learning, reasoning or modelling implemented with the techniques and approaches listed in Annex I, and iii) generates outputs in the form of content (generative AI systems), predictions, recommendations or decisions, which influence the environments it interacts with;"
    },
    {
        "uuid":"b8ab9bfd-542b-4939-97c8-94b4e7b7c50e",
        "amendment_number":45,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 4",
        "text":"(4) ‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non- professional activity; (4) ‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority;"
    },
    {
        "uuid":"18271a26-3dc3-41d8-8a8a-2a5c75690f32",
        "amendment_number":46,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 5 a (new)",
        "text":"(5 a) ‘product manufacturer’ means a manufacturer within the meaning of any of the Union legislation listed in Annex II;"
    },
    {
        "uuid":"8f845981-1770-480b-a217-f3ef17e73822",
        "amendment_number":47,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 13",
        "text":"(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems; (13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its purpose as indicated in instruction for use or technical specification, but which may result from reasonably foreseeable human behaviour or interaction with other systems;"
    },
    {
        "uuid":"20657eb9-938f-4789-a88e-f2ff2629a2e4",
        "amendment_number":48,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 14",
        "text":"(14) ‘safety component of a product or system’ means a component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property; (14) ‘safety component of a product or system’ means a component of a product or of a system the failure or malfunctioning of which endangers the health and safety of persons or property;"
    },
    {
        "uuid":"d01db9dd-f1f9-4933-9bbc-0f8336e61da8",
        "amendment_number":49,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 35",
        "text":"(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data; (35) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, disability, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data;"
    },
    {
        "uuid":"329e6711-040f-472f-82a5-50098cc2aeb6",
        "amendment_number":50,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 44 – introductory part",
        "text":"(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led or might lead to any of the following: (44) ‘serious incident’ means any incident or malfunctioning of an AI system that directly or indirectly leads, might have led or might lead to any of the following:"
    },
    {
        "uuid":"61708568-818a-4b07-bffd-c0c233438127",
        "amendment_number":51,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 44 a (new)",
        "text":"(44a) 'personal data' means data as defined in point (1) of Article 4 of Regulation (EU)2016\/679;"
    },
    {
        "uuid":"d4a8e54e-1fc9-42fb-9b30-5e5ad2b36be6",
        "amendment_number":52,
        "author":"TRAN",
        "title":"Article 3 – paragraph 1 – point 44 b (new)",
        "text":"(44b) ‘non-personal data’ means data other than personal data as defined in point (1) of Article 4 of Regulation (EU) 2016\/679;"
    },
    {
        "uuid":"71633e13-da8c-4b32-a8a3-95916056bd5a",
        "amendment_number":53,
        "author":"TRAN",
        "title":"Article 4 – paragraph 1",
        "text":"The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I, in order to update that list to market and technological developments on the basis of The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I within the scope of the definition of an AI system as provided for in Article 3(1), in order to characteristics that are similar to the techniques and approaches listed therein. update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein"
    },
    {
        "uuid":"0bf1d1c8-4e2c-4593-a8e8-db312a385096",
        "amendment_number":54,
        "author":"TRAN",
        "title":"Article 6 – paragraph 1 – introductory part",
        "text":"1. Irrespective of whether an AI system is placed on the market or put into service independently from the products referred to in points (a) and (b), that AI system shall be considered high-risk where both of the following conditions are fulfilled: 1. An AI system that is itself a product covered by the Union harmonisation legislation listed in Annex II shall be considered as high risk if it is required to undergo a third-party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to the above mentioned legislation."
    },
    {
        "uuid":"cb88bab3-117a-4747-94f1-5ce1be1ce68e",
        "amendment_number":55,
        "author":"TRAN",
        "title":"Article 6 – paragraph 1 – point a",
        "text":"(a) the AI system is intended to be used as a safety component of a product, or is itself a product, covered by the Union harmonisation legislation listed in Annex II; 2. An AI system intended to be used as a safety component of a product covered by the legislation referred to in paragraph 1 shall be considered as high risk if it is required to undergo a third-party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to abovementioned legislation. This provision shall apply irrespective of whether the AI system is placed on the market or put into service independently from the product."
    },
    {
        "uuid":"c29b0c00-54f4-4b12-9d07-1d64161f0524",
        "amendment_number":56,
        "author":"TRAN",
        "title":"Article 6 – paragraph 2",
        "text":"2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk. 3. AI systems referred to in Annex III shall be considered high-risk."
    },
    {
        "uuid":"3de165c5-ba60-4888-8f54-bc6531593290",
        "amendment_number":57,
        "author":"TRAN",
        "title":"Article 6 – paragraph 2 a (new)",
        "text":"2a. The classification as high-risk as a consequence of Article 6(1) 6(2) and 6(3) shall be disregarded for AI systems whose intended purpose demonstrates that the generated output is a recommendation requiring a human intervention to convert this recommendation into a decision and for AI systems, which do not lead to autonomous decisions or actions of the overall system."
    },
    {
        "uuid":"ede9c52d-dca6-4d64-8ae9-c6e269f4f39b",
        "amendment_number":58,
        "author":"TRAN",
        "title":"Article 7 – paragraph 1 – point b",
        "text":"(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III. (b) the AI systems pose a risk of harm to the health or safety, or a risk of adverse impact on fundamental rights or the environment, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III."
    },
    {
        "uuid":"712067db-9d07-416c-a8ef-897c554e703c",
        "amendment_number":59,
        "author":"TRAN",
        "title":"Article 7 – paragraph 2 – introductory part",
        "text":"2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria: 2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights or on the environment that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria:"
    },
    {
        "uuid":"56bb19f0-f517-4776-9197-a2f9a1d5a803",
        "amendment_number":60,
        "author":"TRAN",
        "title":"Article 7 – paragraph 2 – point c",
        "text":"(c) the extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities; (c) the extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or on the environment or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities;"
    },
    {
        "uuid":"08975ca2-4dc6-49ce-94bb-69624a1b439f",
        "amendment_number":61,
        "author":"TRAN",
        "title":"Article 7 – paragraph 2 – point d",
        "text":"(d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a (d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a PE730.085v02-00 36\/70 plurality of persons; plurality of persons or the environment;"
    },
    {
        "uuid":"5817aea0-8615-4e1a-89ac-1283facb4fa6",
        "amendment_number":62,
        "author":"TRAN",
        "title":"Article 7 – paragraph 2 – point g",
        "text":"(g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible; (g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an adverse impact on the health or safety of persons, or on the environment shall not be considered as easily reversible;"
    },
    {
        "uuid":"27b45902-26d4-4cfc-9f03-fd019772a673",
        "amendment_number":63,
        "author":"TRAN",
        "title":"Article 8 – paragraph 1",
        "text":"1. High-risk AI systems shall comply with the requirements established in this Chapter. 1. High-risk AI systems shall comply with the requirements established in this Chapter, taking into account sectoral legislation where applicable, harmonised standards and common specifications."
    },
    {
        "uuid":"b0278605-d5ac-4e27-86bc-7e7d08b027be",
        "amendment_number":64,
        "author":"TRAN",
        "title":"Article 9 – paragraph 2 – point a",
        "text":"(a) identification and analysis of the known and foreseeable risks associated with each high-risk AI system; (a) identification and analysis of the known and foreseeable risks associated with each high-risk AI system that might cause harm or damage to the environment or to the health, safety and fundamental rights of persons in view of the intended purpose of or misuse of the high-risk AI system."
    },
    {
        "uuid":"8923286c-82f7-42e1-83cb-cacd0e5b3b2c",
        "amendment_number":65,
        "author":"TRAN",
        "title":"Article 9 – paragraph 2 – point c",
        "text":"(c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61; (c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system;"
    },
    {
        "uuid":"6a0fffcb-4281-4023-9aa2-58b7c67e4e32",
        "amendment_number":66,
        "author":"TRAN",
        "title":"Article 9 – paragraph 4 – subparagraph 1",
        "text":"The risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse. Those residual risks shall be communicated to the user. The risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, subject to terms, conditions as made available by the provider, and contractual and license restrictions. Those residual risks shall be communicated to the user."
    },
    {
        "uuid":"da557948-6b52-48b4-8343-19d1d883bfc9",
        "amendment_number":67,
        "author":"TRAN",
        "title":"Article 10 – paragraph 1",
        "text":"1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5. 1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5, where applicable. PE730.085v02-00 38\/70"
    },
    {
        "uuid":"bc4f371a-3735-4256-8c01-1614e8c5785c",
        "amendment_number":68,
        "author":"TRAN",
        "title":"Article 10 – paragraph 2 – point g",
        "text":"(g) the identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed. (g) the identification of any other data gaps or shortcomings that materially increase the risks of harm to the health, environment and safety or the fundamental rights of persons, and how those gaps and shortcomings can be addressed."
    },
    {
        "uuid":"64560e0e-46d5-484f-844a-f253b6cbb581",
        "amendment_number":69,
        "author":"TRAN",
        "title":"Article 10 – paragraph 3",
        "text":"3. Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof. 3. Training, validation and testing data sets shall be relevant, representative, free of errors and to the best extent possible and as complete as possible. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof."
    },
    {
        "uuid":"e5b836b9-9ad5-40fc-ad68-5c9f14191601",
        "amendment_number":70,
        "author":"TRAN",
        "title":"Article 10 – paragraph 4",
        "text":"4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are 4. Training, validation and testing data sets shall be sufficiently diverse to accurately capture, to the extent required by the intended purpose, the characteristics particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used. or elements that are particular to the specific geographical, behavioural or functional setting within which the high- risk AI system is intended to be used."
    },
    {
        "uuid":"247e3529-1d73-432d-8e59-705e14d7f1af",
        "amendment_number":71,
        "author":"TRAN",
        "title":"Article 11 – paragraph 1 – subparagraph 2",
        "text":"The technical documentation shall be drawn up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. The technical documentation shall be drawn up in such a way to demonstrate that the high-risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV or, in the case of SMEs and start-ups, any equivalent documentation meeting the same objectives, subject to approval of the competent authority. Documentation shall be kept up to date throughout its entire lifecycle."
    },
    {
        "uuid":"1bf527c9-7087-4e1a-89dc-9dddb7d0294a",
        "amendment_number":72,
        "author":"TRAN",
        "title":"Article 12 – paragraph 2",
        "text":"2. The logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to the intended purpose of the system. 2. The logging capabilities shall ensure a level of traceability of the AI system’s functioning while the AI system is used within its lifecycle that is appropriate to the intended purpose of the system."
    },
    {
        "uuid":"22bfd5af-9bc8-4539-9b0b-08b54963b3cc",
        "amendment_number":73,
        "author":"TRAN",
        "title":"Article 13 – paragraph 2",
        "text":"2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users. 2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or made otherwise available, that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users to assist them in operating and maintaining the AI system, taking into consideration the system’s intended purpose and the expected audience for the instructions."
    },
    {
        "uuid":"518cd0ed-a9ed-4d65-825c-016acbf1adb0",
        "amendment_number":74,
        "author":"TRAN",
        "title":"Article 13 – paragraph 3 – point b – point ii",
        "text":"(ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and reasonably foreseeable circumstances that could materially impact that expected level of accuracy, robustness and cybersecurity;"
    },
    {
        "uuid":"67980445-05bd-4d44-8337-f408e70fff45",
        "amendment_number":75,
        "author":"TRAN",
        "title":"Article 13 – paragraph 3 – point b – point iii",
        "text":"(iii) any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or (iii) any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights; fundamental rights or the environment;"
    },
    {
        "uuid":"cec029b3-7daf-457d-848c-eb0b242372c6",
        "amendment_number":76,
        "author":"TRAN",
        "title":"Article 13 – paragraph 3 – point e",
        "text":"(e) the expected lifetime of the high- risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates. (e) the expected lifetime of the high- risk AI system, the description of the procedure of withdrawing it from use and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates."
    },
    {
        "uuid":"a0ffcf81-052a-49c4-beef-2957053a610f",
        "amendment_number":77,
        "author":"TRAN",
        "title":"Article 14 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. 1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use, unless there is clear evidence that human intervention compromises the safety of the high risk AI system concerned."
    },
    {
        "uuid":"0aa26cde-9b8f-4112-a363-ec0080f7c3b7",
        "amendment_number":78,
        "author":"TRAN",
        "title":"Article 14 – paragraph 2",
        "text":"2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is 2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights or the environment that may emerge when a PE730.085v02-00 42\/70 used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter."
    },
    {
        "uuid":"f8dd9c9d-8c9b-4b47-99b2-5860429638b8",
        "amendment_number":79,
        "author":"TRAN",
        "title":"Article 14 – paragraph 4 – introductory part",
        "text":"4. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: 4. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate and proportionate to the circumstances:"
    },
    {
        "uuid":"47d2434e-a2a5-4d42-870e-9d8ce777154e",
        "amendment_number":80,
        "author":"TRAN",
        "title":"Article 14 – paragraph 4 – point a",
        "text":"(a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; (a) have an appropriate understanding of the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible;"
    },
    {
        "uuid":"d0ac6a3d-7e27-4118-aaa7-a08145803014",
        "amendment_number":81,
        "author":"TRAN",
        "title":"Article 14 – paragraph 4 – point d",
        "text":"(d) be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or (d) be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; reverse the output of the high-risk AI system, unless there is clear evidence that such human intervention is deemed to increase risks or otherwise negatively impact the system’s performance."
    },
    {
        "uuid":"a7541dd4-ba69-4351-b07a-3d1aec2ea5b9",
        "amendment_number":82,
        "author":"TRAN",
        "title":"Article 14 – paragraph 4 – point e",
        "text":"(e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure. (e) be able to intervene on the operation of the high-risk AI system put the system into fail-safe mode, put the system into manual control mode or stop the system through a “stop” button or a similar procedure unless there is clear evidence that such human intervention is deemed to increase risks or otherwise negatively impact the system’s performance."
    },
    {
        "uuid":"c6ff69ac-79f9-4065-a4e4-33039c7e01c4",
        "amendment_number":83,
        "author":"TRAN",
        "title":"Article 14 – paragraph 4 – point e a (new)",
        "text":"(ea) be able to comprehend when a high risk AI system decision is preferable to human oversight."
    },
    {
        "uuid":"8ac9485c-67cc-4fc2-881c-4edac8499b96",
        "amendment_number":84,
        "author":"TRAN",
        "title":"Article 14 – paragraph 5",
        "text":"5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as 5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as PE730.085v02-00 44\/70 to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons. to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons."
    },
    {
        "uuid":"62b495ad-601a-453d-88a7-427f32df33d3",
        "amendment_number":85,
        "author":"TRAN",
        "title":"Article 15 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness and cybersecurity, and perform consistently in those respects throughout their lifecycle. 1. High-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, the highest level of accuracy, robustness and cybersecurity possible, and perform consistently in those respects throughout their lifecycle."
    },
    {
        "uuid":"999b46ce-0033-4ff9-90fe-02b975a82164",
        "amendment_number":86,
        "author":"TRAN",
        "title":"Article 15 – paragraph 3 – subparagraph 1",
        "text":"High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Providers should take all appropriate and feasible measures to ensure that high-risk AI systems are resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems."
    },
    {
        "uuid":"d253225e-e5ec-4721-9f5e-82f7e6b24fe1",
        "amendment_number":87,
        "author":"TRAN",
        "title":"Article 15 – paragraph 3 – subparagraph 3",
        "text":"High-risk AI systems that continue to learn High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures. after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs influencing an input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures."
    },
    {
        "uuid":"44cda3b7-8543-4004-ab98-e1bb07bc841a",
        "amendment_number":88,
        "author":"TRAN",
        "title":"Article 28 – paragraph 1 – point c a (new)",
        "text":"(ca) they modify the intended purpose of an AI system which is not high-risk and is already placed on the market or put into service, in a way which makes the modified system a high-risk AI system."
    },
    {
        "uuid":"b98b0322-40a9-4718-b199-9f7952bcc785",
        "amendment_number":89,
        "author":"TRAN",
        "title":"Article 29 – paragraph 6 a (new)",
        "text":"6a. This Article only applies to users acting in their professional capacity and not to those using AI in the course of a personal non-professional activity."
    },
    {
        "uuid":"faeb741c-977e-4560-9c39-bf46e7f4b1c8",
        "amendment_number":90,
        "author":"TRAN",
        "title":"Article 29 – paragraph 6 b (new)",
        "text":"6b. Users of high risk AI systems, who modify or extend the purpose for which the conformity of the AI system was originally assessed, shall establish and document a post-market monitoring PE730.085v02-00 46\/70 system(Art. 61) and must undergo a new conformity assessment (Art. 43) involved by a notified body."
    },
    {
        "uuid":"45417fe6-2993-4bb2-831b-3ffaa00d834d",
        "amendment_number":91,
        "author":"TRAN",
        "title":"Article 30 – paragraph 8",
        "text":"8. Notifying authorities shall make sure that conformity assessments are carried out in a proportionate manner, avoiding unnecessary burdens for providers and that notified bodies perform their activities taking due account of the size of an undertaking, the sector in which it operates, its structure and the degree of complexity of the AI system in question. 8. Notifying authorities shall make sure that conformity assessments are carried out in a proportionate manner, avoiding unnecessary burdens for providers and that notified bodies perform their activities taking due account of the size of an undertaking, the sector in which it operates, its structure and the degree of complexity of the AI system in question. In this regard, particular attention shall be paid to micro, SMEs keeping compliance costs for them at a reasonable level."
    },
    {
        "uuid":"c8074e3f-d0fa-427e-afba-ec165621f1c9",
        "amendment_number":92,
        "author":"TRAN",
        "title":"Article 33 – paragraph 6",
        "text":"6. Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law. The staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in 6. Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law. The staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out. relation to the notifying authorities of the Member State in which their activities are carried out. Any information and documentation obtained by notified bodies pursuant to this Article shall be treated in compliance with the confidentiality obligations set out in Article 70."
    },
    {
        "uuid":"a064e6a8-cb07-41fa-88c9-487278363ed9",
        "amendment_number":93,
        "author":"TRAN",
        "title":"Article 39 a (new)",
        "text":"Article 39a Exchange of knowhow and best practices The Commission shall facilitate regular consultative meetings for the exchange of knowhow and best practices between the Member States' national authorities responsible for notification policy."
    },
    {
        "uuid":"585f0e25-c6f3-47d2-996f-7ee40cde8254",
        "amendment_number":94,
        "author":"TRAN",
        "title":"Article 40 – paragraph 1 a (new)",
        "text":"When issuing a standardisation request to European standardisation organisations in accordance with Article 10 of Regulation 1025\/2012, the Commission shall specify that standards are coherent, easy to implement and drafted in such a way that they aim to fulfil in particular the following objectives: a) ensure that AI systems placed on the market or put into service in the Union are safe and respect Union values and public interests, and strengthen the Union's digital leadership; b) promote investment and innovation in AI, as well as PE730.085v02-00 48\/70 competitiveness and growth of the Union market; c) enhance multi-stakeholder governance, by ensuring it is inclusive and representative of all relevant European stakeholders (e.g. civil society, researchers industry, SMEs). d) contribute to strengthening global cooperation on standardisation in the field of AI that is consistent with Union values and interests. The Commission shall request the European standardisation organisations to regularly report on their progress with regard to the above objectives."
    },
    {
        "uuid":"7c997b35-8b55-4d44-b448-0783c77ffd0b",
        "amendment_number":95,
        "author":"TRAN",
        "title":"Article 41 – paragraph 1",
        "text":"1. Where harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2). 1. Where harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, after consulting the AI Board referred to in Article 56 and the responsible authorities and organizations for a given sector, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2)."
    },
    {
        "uuid":"6f32b093-acbc-4d34-8788-0675ee95579b",
        "amendment_number":96,
        "author":"TRAN",
        "title":"Article 41 – paragraph 2",
        "text":"2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law. 2. When preparing the common specifications referred to in paragraph 1, the Commission shall fulfil the objectives referred of Article 40(2) and gather the views of relevant bodies or expert groups established under relevant sectorial Union law as well as relevant sector-specific stakeholders."
    },
    {
        "uuid":"1b749c56-aa11-44b7-a109-0e009ed552e2",
        "amendment_number":97,
        "author":"TRAN",
        "title":"Article 43 – paragraph 6",
        "text":"6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies. 6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health, safety, the environment and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies."
    },
    {
        "uuid":"92cbb452-0e62-44dd-96c8-dca6784bccc4",
        "amendment_number":98,
        "author":"TRAN",
        "title":"Article 52 – title",
        "text":"Transparency obligations for certain AI systems Transparency obligations for AI systems PE730.085v02-00 50\/70"
    },
    {
        "uuid":"826701a7-ac21-4b6b-8ab0-a00a8bd715f4",
        "amendment_number":99,
        "author":"TRAN",
        "title":"Article 52 – paragraph 3 a (new)",
        "text":"3a. Providers of any AI system should document and make available upon request the parameters regarding the environmental impact, including but not limited to resource consumption, resulting from the design, data management and training, the underlying infrastructures of the AI system, and of the methods to reduce such impact."
    },
    {
        "uuid":"1664a200-431a-4fb5-98a5-847d6ebdacc4",
        "amendment_number":100,
        "author":"TRAN",
        "title":"Article 53 – paragraph 1",
        "text":"1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox. 1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems and secure processing of personal data for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox."
    },
    {
        "uuid":"13cec6c4-ddde-425d-a8a8-6a03c2a8415c",
        "amendment_number":101,
        "author":"TRAN",
        "title":"Article 53 – paragraph 1 a (new)",
        "text":"1a. The organisers of AI regulatory sandboxes shall ensure an easy access for SMEs and start-ups by facilitating and supporting their participation."
    },
    {
        "uuid":"1d03cedf-6b61-486f-80b1-875c93cef13b",
        "amendment_number":102,
        "author":"TRAN",
        "title":"Article 53 – paragraph 1 b (new)",
        "text":"1b. The controllers of personal data referred to in Article 4 (7) of the Regulation (EU) 2016\/679 may further process personal data in an AI regulatory sandbox to the extent that it is necessary for the purposes of development, testing and validation of AI systems. Right of processing is subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. This processing shall not be considered incompatible with the initial purposes."
    },
    {
        "uuid":"8f3cb9a3-4641-4e48-b398-d5354355ccfa",
        "amendment_number":103,
        "author":"TRAN",
        "title":"Article 53 – paragraph 3",
        "text":"3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such 3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health, safety, the environment or fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes PE730.085v02-00 52\/70 mitigation takes place. place."
    },
    {
        "uuid":"52998c90-90d3-43f7-a629-2c1f9716acb4",
        "amendment_number":104,
        "author":"TRAN",
        "title":"Article 53 – paragraph 5",
        "text":"5. Member States’ competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board. They shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. 5. Member States’ competent authorities shall coordinate their activities with regards to AI regulatory sandboxes and cooperate within the framework of the European Artificial Intelligence Board. They shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox."
    },
    {
        "uuid":"41387ce0-e221-4318-b99d-eac85600b5e6",
        "amendment_number":105,
        "author":"TRAN",
        "title":"Article 54 – paragraph 1 – point a – point iii a (new)",
        "text":"(iiia) safety and resilience of transport systems, infrastructure and networks."
    },
    {
        "uuid":"7fe3a6e6-4cf6-4282-b131-62396cc26347",
        "amendment_number":106,
        "author":"TRAN",
        "title":"Article 55 – title",
        "text":"Measures for small-scale providers and users Measures for SMEs, start-ups and users"
    },
    {
        "uuid":"18448d64-bbcf-4b31-bc95-b7a3a1417754",
        "amendment_number":107,
        "author":"TRAN",
        "title":"Article 55 – paragraph 1 – point a",
        "text":"(a) provide small-scale providers and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions; (a) provide SMEs and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions;"
    },
    {
        "uuid":"bea66ece-9144-4475-9d61-e3bad8ab9751",
        "amendment_number":108,
        "author":"TRAN",
        "title":"Article 55 – paragraph 1 – point b",
        "text":"(b) organise specific awareness raising activities about the application of this Regulation tailored to the needs of the small-scale providers and users; (b) organise specific awareness raising activities about the application of this Regulation tailored to the needs of SMEs, start-ups and users;"
    },
    {
        "uuid":"4e4c124b-8375-4076-8a60-8a5e9e010aa3",
        "amendment_number":109,
        "author":"TRAN",
        "title":"Article 55 – paragraph 1 – point c",
        "text":"(c) where appropriate, establish a dedicated channel for communication with small-scale providers and user and other innovators to provide guidance and respond to queries about the implementation of this Regulation. (c) where appropriate, establish a dedicated channel for communication with SMEs and user, start-ups and other innovators to provide guidance and respond to queries about the implementation of this Regulation."
    },
    {
        "uuid":"a99c1954-adf6-4f68-9b7b-b15591cc7621",
        "amendment_number":110,
        "author":"TRAN",
        "title":"Article 55 – paragraph 2 a (new)",
        "text":"2a. Where appropriate, Member States shall find synergies and cooperate with relevant instruments funded by Union programmes, such as the European Digital Innovation Hubs."
    },
    {
        "uuid":"033832a5-769d-4408-90d9-d05ae462703e",
        "amendment_number":111,
        "author":"TRAN",
        "title":"Article 57 – paragraph 1",
        "text":"1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. 1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, the European Data Protection Supervisor, AI ethics experts and industry representatives. Other national, regional and local authorities may be invited to the meetings, where the issues discussed are of relevance for them."
    },
    {
        "uuid":"0974115a-fb32-4d9e-adf1-10dc79124247",
        "amendment_number":112,
        "author":"TRAN",
        "title":"Article 57 – paragraph 3",
        "text":"3. The Board shall be chaired by the Commission. The Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation. 3. The Board shall be co-chaired by the Commission and representative chosen from among the delegates of the Member States. The Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation."
    },
    {
        "uuid":"67353861-9623-4f4c-a604-a96f41f2fdcb",
        "amendment_number":113,
        "author":"TRAN",
        "title":"Article 57 – paragraph 3 a (new)",
        "text":"3a. The Board shall organise consultations with stakeholders at least twice a year. Such stakeholders shall include representatives from industry, SMEs and start-ups, civil society organisations such as NGOs, consumer associations, the social partners and academia, to assess the evolution of trends in technology, issues related to the implementation and the effectiveness of this Regulation, regulatory gaps or loopholes observed in practice."
    },
    {
        "uuid":"0e1b996b-3f20-48cc-afc2-0f721498697e",
        "amendment_number":114,
        "author":"TRAN",
        "title":"Article 57 – paragraph 4",
        "text":"4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. 4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end, the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. The Board shall actively reach out to and hear representatives from groups, which are more vulnerable to discriminatory effects posed by AI, such as people with disabilities."
    },
    {
        "uuid":"48d72108-a660-4300-8c57-b08beeee9662",
        "amendment_number":115,
        "author":"TRAN",
        "title":"Article 59 – paragraph 4",
        "text":"4. Member States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. 4. Member States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health, safety and environmental risks and knowledge of existing standards and legal requirements."
    },
    {
        "uuid":"be205d58-819f-48a5-a33e-9aca17f20c92",
        "amendment_number":116,
        "author":"TRAN",
        "title":"Article 59 – paragraph 4 a (new)",
        "text":"4a. Any information and documentation obtained by the national competent authorities pursuant to the provisions of this Article shall be treated in compliance with the confidentiality obligations set out in Article 70."
    },
    {
        "uuid":"90185d8a-b9d6-4808-a5cd-66b49a942441",
        "amendment_number":117,
        "author":"TRAN",
        "title":"Article 60 – paragraph 3",
        "text":"3. Information contained in the EU database shall be accessible to the public. 3. Information contained in the EU database shall be accessible to the public, user-friendly, easily navigable and machine-readable."
    },
    {
        "uuid":"26e397ed-8622-4b89-b0a8-81a779208401",
        "amendment_number":118,
        "author":"TRAN",
        "title":"Article 60 – paragraph 5 a (new)",
        "text":"5a. Any information and documentation obtained by the Commission and Member States pursuant to this Article shall be treated in compliance with the confidentiality obligations set out in Article 70."
    },
    {
        "uuid":"9898f76e-c5e9-4f24-8ec4-0e65207ee6e1",
        "amendment_number":119,
        "author":"TRAN",
        "title":"Article 61 – paragraph 2",
        "text":"2. The post-market monitoring system shall actively and systematically collect, document and analyse relevant data provided by users or collected through other sources on the performance of high- risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2. 2. The post-market monitoring system shall actively and systematically collect, document and analyse relevant data provided by users or collected through other sources on the performance of high- risk AI systems throughout their lifetime, and allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Title III, Chapter 2. Post-market monitoring must include continuous analysis of the AI environment, including other devices, software, and other AI systems that will interact with the AI system."
    },
    {
        "uuid":"b78994f9-1707-4954-91ed-60ff5b5c3489",
        "amendment_number":120,
        "author":"TRAN",
        "title":"Article 65 – paragraph 1",
        "text":"1. AI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019\/1020 insofar as risks to the health or safety or to the protection 1. AI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation(EU) 2019\/1020 insofar as risks to health, safety or the environment, or to PE730.085v02-00 58\/70 of fundamental rights of persons are concerned. the protection of fundamental rights of persons are concerned."
    },
    {
        "uuid":"66c73abf-8161-4320-876c-ea2929956611",
        "amendment_number":121,
        "author":"TRAN",
        "title":"Article 67 – paragraph 1",
        "text":"1. Where, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. 1. Where, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the environment, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe."
    },
    {
        "uuid":"9998ee42-857c-405d-8bd7-fe02042c07ed",
        "amendment_number":122,
        "author":"TRAN",
        "title":"Article 69 – paragraph 3",
        "text":"3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems. 3. Codes of conduct may be drawn up by national, regional or local authorities, by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems."
    },
    {
        "uuid":"df61a726-d87c-496e-aacc-ce6349d2b959",
        "amendment_number":123,
        "author":"TRAN",
        "title":"Article 72 – paragraph 1 – point a",
        "text":"(a) the nature, gravity and duration of the infringement and of its consequences; (a) the nature, gravity and duration of the infringement and of its consequences; taking into account the number of subjects affected and the level of damage suffered by them, the intentional or negligent character of the infringement and any relevant previous infringement;"
    },
    {
        "uuid":"0a676779-ae5f-488c-8798-2760c4ff5911",
        "amendment_number":124,
        "author":"TRAN",
        "title":"Article 72 – paragraph 1 – point b a (new)",
        "text":"(ba) the degree of cooperation with the supervisory authority, in order to remedy the infringement and mitigate the possible adverse effects of the infringement;"
    },
    {
        "uuid":"db40d482-2c37-4980-97a1-8d6d407f4afa",
        "amendment_number":125,
        "author":"TRAN",
        "title":"Article 72 – paragraph 1 – point b b (new)",
        "text":"(bb) any action taken by the provider to mitigate the damage suffered by subjects;"
    },
    {
        "uuid":"ac6e2643-ae5b-4e42-ba3d-6d50e19da612",
        "amendment_number":126,
        "author":"TRAN",
        "title":"Article 72 – paragraph 1 – point c a (new)",
        "text":"(ca) any other aggravating or mitigating factor applicable to the circumstances of the case, such as financial benefits gained, or losses avoided, directly or indirectly, from the infringement."
    },
    {
        "uuid":"158b0371-9fe1-4e8e-9f6c-012ddc5882e4",
        "amendment_number":127,
        "author":"TRAN",
        "title":"Article 75 – paragraph 1",
        "text":"When adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning Artificial Intelligence systems in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Chapter 2, Title III of that Regulation shall be taken into account. When adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning Artificial Intelligence systems in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, without interfering with existing governance, the requirements set out in Chapter 2, Title III of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"dcd181ce-bf1b-4134-b7c7-dc48a28f42e7",
        "amendment_number":128,
        "author":"TRAN",
        "title":"Article 76 – paragraph 1",
        "text":"When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, without interfering with existing governance, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"775e64c4-3089-4143-b1fb-da463b029c22",
        "amendment_number":129,
        "author":"TRAN",
        "title":"Article 78 – paragraph 1",
        "text":"4. “For Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, when carrying out its activities pursuant to paragraph 1 and when adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, the Commission shall take into account the requirements set out in Title III, Chapter 2 of that Regulation. 4. “For Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, when carrying out its activities pursuant to paragraph 1 and when adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, and without interfering with existing governance, the Commission shall take into account the requirements set out in Title III, Chapter 2 of that Regulation. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"f04a6d6a-fd8b-4c87-b08f-022f7b36007e",
        "amendment_number":130,
        "author":"TRAN",
        "title":"Article 79 – paragraph 1",
        "text":"12. “When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 12. “When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, without interfering with existing governance, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"58ef4992-0b24-42a2-8684-19b72b746642",
        "amendment_number":131,
        "author":"TRAN",
        "title":"Article 80 – paragraph 1",
        "text":"4. “When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council *, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 4. “When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council *, without interfering with existing governance, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"2f4d4589-f8d3-412e-bff2-60b93875b44e",
        "amendment_number":132,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 1",
        "text":"3. “Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 3. “Without prejudice to paragraph 2, and to the certification, oversight and enforcement system referred to in Article 62 of this Regulation, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"e8b6bd46-b23d-436e-be33-4b4c452791eb",
        "amendment_number":133,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 2",
        "text":"4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 4. Without prejudice to the certification, oversight and enforcement system referred to in Article 62 of this Regulation, when adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation(EU) YYY\/XX [on Artificial Intelligence], only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account."
    },
    {
        "uuid":"ae94c958-ffda-41e1-9b6d-0ce21b9c0158",
        "amendment_number":134,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 3",
        "text":"4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 4. Without prejudice to the certification, oversight and enforcement system referred to in Article 62 of this Regulation, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX[on Artificial Intelligence], only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account."
    },
    {
        "uuid":"3d3c7872-9ead-43c7-a4f0-471672cdba7c",
        "amendment_number":135,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 4",
        "text":"3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. 3. Without prejudice to the certification, oversight and enforcement system referred to in Article 62 of this Regulation, when adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation(EU) YYY\/XX [on Artificial Intelligence], only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account."
    },
    {
        "uuid":"2a45ae9c-3891-46c0-b6aa-9156f7f300a9",
        "amendment_number":136,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 5",
        "text":"When adopting those implementing acts concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. Without prejudice to the certification, oversight and enforcement system referred to in Article 62of this Regulation, when adopting those implementing acts concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence], only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.\""
    },
    {
        "uuid":"dc4b87be-d5d2-4cb8-aa8e-a3bbc84ab497",
        "amendment_number":137,
        "author":"TRAN",
        "title":"Article 81 – paragraph 1 – point 6",
        "text":"3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] , the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.. 3. Without prejudice to the certification, oversight and enforcement system referred to in Article 62 of this Regulation, when adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation(EU) YYY\/XX [on Artificial Intelligence], only the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account."
    },
    {
        "uuid":"4cdbf81c-2a0e-4f97-92ff-f1fc85235b1e",
        "amendment_number":138,
        "author":"TRAN",
        "title":"Article 82 – paragraph 1",
        "text":"3. “When adopting the implementing 3. “When adopting the implementing PE730.085v02-00 66\/70 acts pursuant to paragraph 2, concerning artificial intelligence systems which are safety components in the meaning of Regulation (EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. acts pursuant to paragraph 2, concerning artificial intelligence systems which are safety components in the meaning of Regulation(EU) YYY\/XX [on Artificial Intelligence] of the European Parliament and of the Council*, without interfering with existing governance, the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account. _______ ________ * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).” * Regulation (EU) YYY\/XX [on Artificial Intelligence] (OJ …).”"
    },
    {
        "uuid":"2a20854a-d5e8-407f-b497-e5ffb0f500e1",
        "amendment_number":139,
        "author":"TRAN",
        "title":"Article 84 – paragraph 3 a (new)",
        "text":"3a. Within [two years after the date of application of this Regulation referred to in Article 85(2)] and every two years thereafter, the Commission shall evaluate the impact and effectiveness of the Regulation with regards to the energy use and other environmental impact of AI systems and evaluate bringing legislation to regulate the energy efficiency of ICT systems in order for the sector to contribute to Union climate strategy and targets."
    },
    {
        "uuid":"754025ae-cf5f-4aa4-ab8d-4ac930be3dce",
        "amendment_number":140,
        "author":"TRAN",
        "title":"Article 84 – paragraph 7 a (new)",
        "text":"7a. Any relevant future delegated or implementing acts to Regulations listed in Annex II, section B, introducing mandatory requirements for High-Risk AI systems laid down in this Regulation, shall take into account the regulatory specificities of each sector and shall not overlap with existing governance, conformity assessment, and enforcement mechanisms and authorities established therein."
    },
    {
        "uuid":"aed3877d-bcac-4624-a6f9-ce4a19daa465",
        "amendment_number":141,
        "author":"TRAN",
        "title":"Annex III – paragraph 1 – point 2 – point a",
        "text":"(a) AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity. (a) AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, whose failure or malfunctioning would directly cause significant harm to the health, natural environment or safety of natural persons, unless these systems are regulated in harmonisation legislation or sectorial regulation. PE730.085v02-00 68\/70 ANNEX: LIST OF ENTITIES OR PERSONS FROM WHOM THE RAPPORTEUR FOR THE OPINION HAS RECEIVED INPUT  BEUC  ETF  Google  Amazon  Airbus  Hitachi  DG MOVE  DG Connect  EASA  AMCHAM  ACEA  CLEPA  Ericsson PROCEDURE – COMMITTEE ASKED FOR OPINION Title Harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts References COM(2021)0206 – C9-0146\/2021 – 2021\/0106(COD) Committees responsible Date announced in plenary IMCO 7.6.2021 LIBE 7.6.2021 Opinion by Date announced in plenary TRAN 7.6.2021 Rapporteur for the opinion Date appointed Josianne Cutajar 4.11.2021 Rule 58 – Joint committee procedure Date announced in plenary 16.12.2021 Discussed in committee 20.4.2022 Date adopted 12.7.2022 Result of final vote +: –: 0: 37 1 4 Members present for the final vote Magdalena Adamowicz, Andris Ameriks, Izaskun Bilbao Barandica, Karolin Braunsberger-Reinhold, Marco Campomenosi, Ciarán Cuffe, Karima Delli, Anna Deparnay-Grunenberg, Ismail Ertug, Gheorghe Falcă, Carlo Fidanza, Søren Gade, Isabel García Muñoz, Jens Gieseke, Elsi Katainen, Kateřina Konečná, Bogusław Liberadzki, Peter Lundgren, Benoît Lutgen, Elżbieta Katarzyna Łukacijewska, Marian- Jean Marinescu, Tilly Metz, Cláudia Monteiro de Aguiar, Jan-Christoph Oetjen, Rovana Plumb, Dominique Riquet, Massimiliano Salini, Barbara Thaler, István Ujhelyi, Petar Vitanov, Roberts Zīle, Kosma Złotowski Substitutes present for the final vote Josianne Cutajar, Nicola Danti, Vlad Gheorghe, Roman Haider, Pär Holmgren, Guido Reil, Marianne Vind, Jörgen Warborn Substitutes under Rule 209(7) present for the final vote Susanna Ceccardi, Salvatore De Meo PE730.085v02-00 70\/70 FINAL VOTE BY ROLL CALL IN COMMITTEE ASKED FOR OPINION 37 + ECR Carlo Fidanza, Peter Lundgren, Roberts Zīle, Kosma Złotowski PPE Magdalena Adamowicz, Karolin Braunsberger-Reinhold, Salvatore De Meo, Gheorghe Falcă, Jens Gieseke, Elżbieta Katarzyna Łukacijewska, Benoît Lutgen, Marian-Jean Marinescu, Cláudia Monteiro de Aguiar, Massimiliano Salini, Barbara Thaler, Jörgen Warborn, RENEW Izaskun Bilbao Barandica, Nicola Danti, Søren Gade, Vlad Gheorghe, Elsi Katainen, Jan-Christoph Oetjen, Dominique Riquet S&D Andris Ameriks, Josianne Cutajar, Ismail Ertug, Isabel García Muñoz, Bogusław Liberadzki, Rovana Plumb, István Ujhelyi, Marianne Vind, Petar Vitanov Verts\/ALE Ciarán Cuffe, Karima Delli, Anna Deparnay-Grunenberg, Pär Holmgren, Tilly Metz 1 - The Left Kateřina Konečná 4 0 ID Marco Campomenosi, Susanna Ceccardi, Roman Haider, Guido Reil Key to symbols: + : in favour - : against 0 : abstention"
    },
    {
        "uuid":"3af9b86d-c526-4341-a163-685ac63ddb00",
        "amendment_number":1,
        "author":"JURI",
        "title":"Recital 1",
        "text":"(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI- based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation. (1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union principles and democratic values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation."
    },
    {
        "uuid":"815b6d20-0e79-4c2f-b5e6-df032055de9e",
        "amendment_number":2,
        "author":"JURI",
        "title":"Recital 3",
        "text":"(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and (3) Artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities if developed in accordance with relevant general principles in line with the EU personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation. Charter of Fundamental Rights and the values on which the Union is founded. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation."
    },
    {
        "uuid":"5fec4f78-b7c1-4ee9-a8fc-9adfea1d3669",
        "amendment_number":3,
        "author":"JURI",
        "title":"Recital 4",
        "text":"(4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial. (4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial and might affect a person, a group of persons or society as a whole."
    },
    {
        "uuid":"b4c3f13f-de33-4536-b331-ab59b37835b7",
        "amendment_number":4,
        "author":"JURI",
        "title":"Recital 6 a (new)",
        "text":"(6a) Building on the seven key requirements set out by the High-Level Expert Group on Artificial Intelligence, it is important to note that AI systems should respect general principles establishing a high-level framework that promotes a coherent human-centric approach to ethical and trustworthy AI in line with the Charter of Fundamental Rights of the European Union and the values on which the Union is founded, including the protection of fundamental rights, human agency and oversight, technical robustness and safety, privacy and data governance, transparency, non- discrimination and fairness and societal and environmental wellbeing."
    },
    {
        "uuid":"7ad0fed4-fc04-4dab-b146-b2ed564c28fe",
        "amendment_number":5,
        "author":"JURI",
        "title":"Recital 13",
        "text":"(13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments. (13) In order to ensure a consistent and high level of protection of public interests as regards health, safety, fundamental rights and the environment, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter), the European Green Deal (The Green Deal) and the Joint Declaration on Digital Rights of the Union (the Declaration) and should be non-discriminatory and in line with the Union’s international commitments."
    },
    {
        "uuid":"757d673f-6fdf-4c19-a6a3-2fe5ddc356ce",
        "amendment_number":6,
        "author":"JURI",
        "title":"Recital 14 a (new)",
        "text":"(14a) For this Regulation to be effective, it is essential to address the issue of the digital divide and, therefore, it should be accompanied by a policy of education, training and awareness as regards these technologies that ensures a sufficient level of AI literacy."
    },
    {
        "uuid":"037446ee-dc21-4778-8bed-f9693674b744",
        "amendment_number":7,
        "author":"JURI",
        "title":"Recital 14 b (new)",
        "text":"(14b) ‘AI literacy’ refers to skills, knowledge and understanding that allows providers, users and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause and thereby promote its democratic control. AI literacy should not be limited to learning about tools and technologies, but should also aim to equip providers and users with the notions and skills required to ensure compliance with and enforcement of this Regulation. It is therefore necessary that the Commission, the Member States as well as providers and users of AI systems, in cooperation with all relevant stakeholders, promote the development of a sufficient level of AI literacy, in all sectors of society, for citizens of all ages, including women and girls, and that progress in that regard is closely followed."
    },
    {
        "uuid":"fab5f631-d050-433a-abfe-df425006687a",
        "amendment_number":8,
        "author":"JURI",
        "title":"Recital 15",
        "text":"(15) Aside from the many beneficial (15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child. uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy, gender equality and the rights of the child."
    },
    {
        "uuid":"ec4b8307-c7c2-41dc-8049-b8f8313fd7f3",
        "amendment_number":9,
        "author":"JURI",
        "title":"Recital 16",
        "text":"(16) The placing on the market, putting into service or use of certain AI systems intended to distort human behaviour, whereby physical or psychological harms are likely to occur, should be forbidden. Such AI systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of children and people due to their age, physical or mental incapacities. They do so with the intention to materially distort the behaviour of a person and in a manner that causes or is likely to cause harm to that or another person. The intention may not be presumed if the distortion of human behaviour results from factors external to the AI system which are outside of the control of the provider or the user. Research for legitimate purposes in relation to such AI systems should not be stifled by the prohibition, if such research does not amount to use of the AI system in human- machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research. (16) The placing on the market, putting into service or use of certain AI systems intended to distort human behaviour, whereby physical or psychological harms are likely to occur, should be forbidden. Such AI systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of children and people due to their age, physical or mental incapacities. They do so with the intention to materially distort the behaviour of a person and in a manner that causes or is likely to cause harm to that or another person. The intention may not be presumed if the distortion of human behaviour results from factors external to the AI system which are outside of the control of the provider or the user. Research for legitimate purposes in relation to such AI systems should not be stifled by the prohibition, if such research does not amount to use of the AI system in non- supervised human-machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research. If necessary and in accordance with this Regulation, further flexibilities in order to foster research, and thereby European innovation capacities, should be introduced by Member States."
    },
    {
        "uuid":"4efa935f-26c2-4446-bbec-25fbd685cb61",
        "amendment_number":10,
        "author":"JURI",
        "title":"Recital 28",
        "text":"(28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non- discrimination, consumer protection, workers’ rights, rights of persons with disabilities, right to an effective remedy (28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non- discrimination, education, consumer protection, workers’ rights, gender equality, rights of persons with disabilities, and to a fair trial, right of defence and the presumption of innocence, right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons. right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration, right to protection of intellectual property, cultural diversity. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons."
    },
    {
        "uuid":"d1befd17-a832-4a52-80ca-750328f2013a",
        "amendment_number":11,
        "author":"JURI",
        "title":"Recital 47 a (new)",
        "text":"(47a) Such requirements on transparency and on the explicability of AI decision-making should also help to counter the deterrent effects of digital asymmetry and so-called ‘dark patterns’ targeting individuals and their informed consent."
    },
    {
        "uuid":"11fb5e00-7416-4be2-ae63-10c488c3b3cc",
        "amendment_number":12,
        "author":"JURI",
        "title":"Recital 48 a (new)",
        "text":"(48a) Human oversight aims at serving human-centric objectives. The individuals to whom human oversight is assigned should be provided with adequate education and training on the functioning of the AI system, its capabilities to influence or make decisions, the possible harmful effects it can cause, notably on fundamental rights, and its probability of occurrence. The persons in charge of the assignment of these individuals should provide them with the necessary staff and psychological support and authority to exercise their function."
    },
    {
        "uuid":"16522077-dcac-40d1-a77e-d79d19d1f3d1",
        "amendment_number":13,
        "author":"JURI",
        "title":"Recital 57 a (new)",
        "text":"(57a) AI systems, which have been placed on the market but require further training or the use of a model not provided by the provider should be considered as general purpose AI system. The training of these systems after they have been placed in the market should be considered as adapting them to a specific purpose;"
    },
    {
        "uuid":"4e6a353f-6867-467c-8d1f-40aba4c1baec",
        "amendment_number":14,
        "author":"JURI",
        "title":"Recital 57 b (new)",
        "text":"(57b) Open Source software licences allow users to run, copy, distribute, study, change and improve software freely. By default the use of Open Source software in this manner attributes liability to the user, whereas when a provider provides Open Source software commercially under a Software as a Service (SaaS) or Professional Services model, then the provider may retain the liability instead of the user. Research by the European Commission shows that Open Source software contributes between €65bn - €95bn to the European Union’s GDP, and provides significant growth opportunities for the Union economy. Open Source providers should be able to adopt the same economic model for AI systems. Hence, the provisions of this Regulation should not apply to Open Source AI systems until those systems are put into service. To ensure that AI systems cannot be put into service without complying with this Regulation, when an Open Source AI System is put into service, the obligations associated with providers should be transferred to the person putting the system into service."
    },
    {
        "uuid":"c544d45b-c05d-4d03-be35-d926a1703c13",
        "amendment_number":15,
        "author":"JURI",
        "title":"Recital 73",
        "text":"(73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for (73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on AI literacy, awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation communication with operators is one which is broadly understood by the largest possible number of cross-border users. and for communication with operators is one which is broadly understood by the largest possible number of cross-border users."
    },
    {
        "uuid":"3b979673-e4c1-4462-ab73-dc92b33f9e77",
        "amendment_number":16,
        "author":"JURI",
        "title":"Recital 76",
        "text":"(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. (76) In order to avoid fragmentation and ensure the optimal functioning of the Single Market, it is essential to guarantee an effective and harmonised implementation of this Regulation. To this end, a European Artificial Intelligence Board should be established and entrusted with a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. However, such a solution might prove not to be sufficient to ensure a fully coherent cross-border action and, therefore, [within three years after the date of application of this Regulation], the Commission should be required to consider whether the creation of an EU Agency is necessary to ensure a consistent application of this Regulation at Union level."
    },
    {
        "uuid":"a1577ad4-517f-487e-8944-465b59f15d8c",
        "amendment_number":17,
        "author":"JURI",
        "title":"Recital 76 a (new)",
        "text":"(76a) The Commission should re- establish the High Level Expert Group or a similar body with a new and balanced membership comprising an equal number of experts from SMEs and start-ups, large enterprises, academia and Research, social partners and civil society. This new High Level Expert Group on Trustworthy AI should not only act as advisory body to the Commission but also to the Board. At least every quarter, the new High Level Expert Group on Trustworthy AI must have the chance to share its practical and technical expertise in a special meeting with the Board."
    },
    {
        "uuid":"60b9b7c6-1ccf-47ae-bf03-f0fc6195f47c",
        "amendment_number":18,
        "author":"JURI",
        "title":"Recital 77",
        "text":"(77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority. (77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority. In order to facilitate a consistent and coherent implementation of this Regulation, national supervisory authorities should engage in substantial and regular cooperation not only with the Board, but also among themselves to promote the exchange of relevant information and best practices. In this regard and also taking into account that, given the current lack of AI experts, it might be difficult to ensure at national level that the supervisory authorities are provided with adequate human resources to perform their tasks, Member States are also strongly encouraged to consider the possibility of creating transnational entities for the purpose of ensuring joint supervision of the implementation of this Regulation."
    },
    {
        "uuid":"65436e86-78d8-4125-9dd5-aec914b69202",
        "amendment_number":19,
        "author":"JURI",
        "title":"Recital 80 a (new)",
        "text":"(80a) Natural or legal persons affected by decisions made by AI systems which produce legal effects that adversely affect their health, safety, fundamental rights, socio-economic well-being or any other of their rights deriving from the obligations laid down in this Regulation, should be entitled to an explanation of that decision. Such an explanation is to be provided to the affected persons and, therefore, when providing such an explanation, providers and users should duly take into account that the level of expertise and knowledge of the average consumer or citizen regarding AI systems is limited and much lower than the one that they possess. On the other hand, some AI systems cannot provide an explanation for their decisions beyond the initial input data. When AI systems are required to provide an explanation and cannot, they should clearly state that an explanation cannot be provided. This should be taken into account by any administrative, non- administrative or judicial authority dealing with complaints from affected persons."
    },
    {
        "uuid":"489a4825-3bbf-4933-8360-ef19ae50cec4",
        "amendment_number":20,
        "author":"JURI",
        "title":"Recital 85",
        "text":"(85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making 58 . In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. (85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58. These consultations should involve the participation of a balanced selection of stakeholders, including consumer organisations, associations representing affected persons, business representatives from different sectors and of different sizes, trade unions as well as researchers and scientists. In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts."
    },
    {
        "uuid":"41288e8e-97d8-44dc-91e5-2fa99da1ba49",
        "amendment_number":21,
        "author":"JURI",
        "title":"Recital 86 a (new)",
        "text":"(86a) Given the rapid technological developments and the required technical expertise in conducting the assessment of high-risk AI systems, the powers delegated to the Commission and the implementing powers conferred on it should be exercised with as much flexibility as possible. The Commission should regularly review Annex III without undue delay while consulting with the relevant stakeholders."
    },
    {
        "uuid":"bddc7d1b-f5cd-452d-a09e-de70eb1bbc37",
        "amendment_number":22,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point a",
        "text":"(a) harmonised rules for the placing on the market, the putting into service and the use of artificial intelligence systems (‘AI systems’) in the Union; (a) harmonised rules for the development, placing on the market, the putting into service and the use of human- centric and trustworthy artificial intelligence systems (‘AI systems’) in the Union in compliance with democratic values;"
    },
    {
        "uuid":"f09bc824-17c8-426d-b15d-9731a8e958ba",
        "amendment_number":23,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point d",
        "text":"(d) harmonised transparency rules for (d) harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems and biometric categorisation systems, and AI systems used to generate or manipulate image, audio or video content; certain AI systems;"
    },
    {
        "uuid":"b0cfbcfa-f46d-48ba-b357-4a243416912b",
        "amendment_number":24,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point e",
        "text":"(e) rules on market monitoring and surveillance. (e) rules on governance, market monitoring, market surveillance and enforcement;"
    },
    {
        "uuid":"937c2c9a-dab6-4cf9-ac78-afb4a80550e2",
        "amendment_number":25,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point e a (new)",
        "text":"(ea) a high level protection of public interests, such as health, safety, fundamental rights and the environment, against potential harms caused by artificial intelligence;"
    },
    {
        "uuid":"d6578c02-4e1f-4352-a08f-7a572f5fbd9f",
        "amendment_number":26,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point e b (new)",
        "text":"(eb) measures in support of innovation with a particular focus on SMEs and start-ups, including but not limited to setting up regulatory sandboxes and targeted measures to reduce the compliance burden on SME’s and start- ups;"
    },
    {
        "uuid":"3de95e19-463e-4962-8943-8d29ca73c47d",
        "amendment_number":27,
        "author":"JURI",
        "title":"Article 1 – paragraph 1 – point e c (new)",
        "text":"(ec) provisions on the establishment of an independent ‘European Artificial Intelligence Board’ and on its activities supporting the enforcement of this Regulation."
    },
    {
        "uuid":"3dc5871a-70a6-4210-b222-222a8cc652ff",
        "amendment_number":28,
        "author":"JURI",
        "title":"Article 2 – paragraph 1 – point b",
        "text":"(b) users of AI systems located within the Union; (b) users of AI systems who are located or established within the Union;"
    },
    {
        "uuid":"ff6d6363-f954-480d-a579-a002d4f2cb29",
        "amendment_number":29,
        "author":"JURI",
        "title":"Article 2 – paragraph 1 – point c",
        "text":"(c) providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union; (c) providers and users of AI systems that are located in a third country, where the output, meaning predictions, recommendations or decisions produced by the system and influencing the environment it interacts with, is used in the Union and puts at risk the environment or the health, safety or fundamental rights of natural persons physically present in the Union, insofar as the provider or user has permitted, is aware or can reasonably expect such a use;"
    },
    {
        "uuid":"1cfbd880-a773-429b-b636-3e4d356d6d89",
        "amendment_number":30,
        "author":"JURI",
        "title":"Article 2 – paragraph 1 – point c a (new)",
        "text":"(ca) importers, distributors, and authorised representatives of providers of AI systems;"
    },
    {
        "uuid":"c871929a-1573-49ab-9c0f-ff28c96c7fea",
        "amendment_number":31,
        "author":"JURI",
        "title":"Article 2 – paragraph 2 – introductory part",
        "text":"2. For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems, falling within the scope of the following acts, only Article 84 of this Regulation shall apply: 2. For high-risk AI systems that are safety components of products or systems, or which are themselves products or systems and that fall within the scope of the listed acts in Annex II, section B, only Article 84 of this Regulation shall apply. (a) Regulation (EC) 300\/2008; (b) Regulation (EU) No 167\/2013; (c) Regulation (EU) No 168\/2013; (d) Directive 2014\/90\/EU; (e) Directive (EU) 2016\/797; (f) Regulation (EU) 2018\/858; (g) Regulation (EU) 2018\/1139; (h) Regulation (EU) 2019\/2144."
    },
    {
        "uuid":"d7d32204-2de9-483f-87ab-ad051eb0e687",
        "amendment_number":32,
        "author":"JURI",
        "title":"Article 2 – paragraph 3",
        "text":"3. This Regulation shall not apply to AI systems developed or used exclusively for military purposes. deleted"
    },
    {
        "uuid":"33251e13-360e-4b3a-b727-cdf0288ff2c8",
        "amendment_number":33,
        "author":"JURI",
        "title":"Article 2 – paragraph 3 a (new)",
        "text":"3a. This Regulation shall not affect research, testing and development activities regarding an AI system prior to this system being placed on the market or put into service, provided that these activities are conducted respecting fundamental rights and the applicable Union law. The Commission is empowered to adopt delegated acts in accordance with Article 73 to specify this exemption. The Board shall provide guidance on the governance of research and development pursuant to Article 56 (2) (cc), also aiming at coordinating the way this exemption is put in place by the Commission and the national supervisory authorities."
    },
    {
        "uuid":"bd8d0542-3312-4d17-b244-3d2631807e2a",
        "amendment_number":34,
        "author":"JURI",
        "title":"Article 2 – paragraph 3 b (new)",
        "text":"3b. Title III of this Regulation shall not apply to AI systems that are used in a strictly business-to-business environment and provided that those systems do not pose a risk of harm to the environment, health or safety or a risk of adverse impact on fundamental rights."
    },
    {
        "uuid":"16c62c53-f9fc-4d92-8505-252487eb75be",
        "amendment_number":35,
        "author":"JURI",
        "title":"Article 2 – paragraph 4 a (new)",
        "text":"4a. This regulation shall not apply to Open Source AI systems until those systems are put into service or made available on the market in return for payment, regardless of if that payment is for the AI system itself, the provision of the AI system as a service, or the provision of technical support for the AI system as a service."
    },
    {
        "uuid":"5b28a4f7-0229-42c0-89c4-89b5585adb49",
        "amendment_number":36,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 1 a (new)",
        "text":"(1a) general purpose AI system' means an AI system that - irrespective of the modality in which it is placed on the market or put into service including as open source software - is intended by the provider to perform generally applicable functions such as image or speech recognition, audio or video generation, pattern detection, question answering, translation or others; a general purpose AI system may be used in a plurality of contexts and may be integrated in a plurality of other AI systems;"
    },
    {
        "uuid":"3476dc9f-298a-4122-b1dd-59ed1dcdf8d4",
        "amendment_number":37,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 1 b (new)",
        "text":"(1b) ‘open source AI systems’ means AI systems, including test and training data, or trained models, distributed under open licenses."
    },
    {
        "uuid":"2015720a-9849-418a-b467-905d9ecf0eb1",
        "amendment_number":38,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 2",
        "text":"(2) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge; (2) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge or that adapts general purpose AI systems to a specific intended purpose;"
    },
    {
        "uuid":"a2b4125d-f7e7-4ae1-b779-b131eea7cd66",
        "amendment_number":39,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 4 a (new)",
        "text":"(4a) ‘affected person’ means any natural person or a group of persons who are subject to or affected by an AI system"
    },
    {
        "uuid":"67af7e36-7d66-4d57-b677-0909844fa314",
        "amendment_number":40,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 43",
        "text":"(43) ‘national competent authority’ means the national supervisory authority, the notifying authority and the market surveillance authority; deleted"
    },
    {
        "uuid":"998b6e1d-3c89-4081-98fe-a785e69a03a1",
        "amendment_number":41,
        "author":"JURI",
        "title":"Article 3 – paragraph 1 – point 44 a (new)",
        "text":"(44a) 'AI literacy' means the skills, knowledge and understanding regarding AI systems that are necessary for the compliance with and enforcement of this Regulation."
    },
    {
        "uuid":"202d20a4-0011-4844-8f18-a6cc8b4f8c04",
        "amendment_number":42,
        "author":"JURI",
        "title":"Article 4 a (new)",
        "text":"Article 4a General principles applicable to all AI systems 1. All AI operators shall respect the following general principles that establish a high-level framework that promotes a coherent human-centric European approach to ethical and trustworthy Artificial Intelligence, which is fully in line with the Charter as well as the values on which the Union is founded: • ‘human agency and oversight’ means that AI systems shall be developed and used as a tool that serves people, respects human dignity and personal autonomy, and that is functioning in a way that can be appropriately controlled and overseen by humans. • ‘technical robustness and safety’ means that AI systems shall be developed and used in a way to minimize unintended and unexpected harm as well as being robust in case of unintended problems and being resilient against attempts to alter the use or performance of the AI system so as to allow unlawful use by malicious third parties. • ‘privacy and data governance’ means that AI systems shall be developed and used in compliance with existing privacy and data protection rules, while processing data that meets high standards in terms of quality and integrity. • ‘transparency’ means that AI systems shall be developed and used in a way that allows appropriate traceability and explainability, while making humans aware that they communicate or interact with an AI system as well as duly informing users of the capabilities and limitations of that AI system and affected persons about their rights. • ‘diversity, non-discrimination and fairness’ means that AI systems shall be developed and used in a way that includes diverse actors and promotes equal access, gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are prohibited by Union or national law. • ‘social and environmental well-being’ means that AI systems shall be developed and used in a sustainable and environmentally friendly manner as well as in a way to benefit all human beings, while monitoring and assessing the long- term impacts on the individual, society and democracy. 2. Paragraph 1 is without prejudice to obligations set up by existing Union and national law. For high-risk AI systems, the general principles are translated into and complied with by providers or users by means of the requirements set out in Articles 8 to 15 of this Regulation. For all other AI systems, the voluntary application on the basis of harmonised standards, technical specifications and codes of conduct as referred to in Article 69 is strongly encouraged with a view to fulfilling the principles listed in paragraph 1. 3. The Commission and the Board shall issue recommendations that help guiding providers and users on how to develop and use AI systems in accordance with the general principles. European Standardisation Organisations shall take the general principles referred to in paragraph 1 into account as outcome- based objectives when developing the appropriate harmonised standards for high risk AI systems as referred to in Article 40(2b)."
    },
    {
        "uuid":"a71f6c63-3e39-4c99-baa0-65af54af72bc",
        "amendment_number":43,
        "author":"JURI",
        "title":"Article 4 b (new)",
        "text":"Article 4b AI literacy 1. When implementing this Regulation, the Union and the Member States shall promote measures and tools for the development of a sufficient level of AI literacy, across sectors and taking into account the different needs of groups of providers, users and affected persons concerned, including through education and training, skilling and reskilling programmes and while ensuring proper gender and age balance, in view of allowing a democratic control of AI systems. 2. Providers and user of AI systems shall promote tools and take measures to ensure a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the environment the AI systems are to be used in, and considering the persons or groups of persons on which the AI systems are to be used. 3. Such literacy tools and measures shall consist, in particular, of the teaching and learning of basic notions and skills about AI systems and their functioning, including the different types of products and uses, their risks and benefits and the severity of the possible harm they can cause and its probability of occurrence. 4. A sufficient level of AI literacy is one that contributes, as necessary, to the ability of providers and users to ensure compliance and enforcement of this Regulation."
    },
    {
        "uuid":"b73a076a-829a-4176-987c-ce496690d688",
        "amendment_number":44,
        "author":"JURI",
        "title":"Article 9 – paragraph 4 - subparagraph 2 – point c",
        "text":"(c) provision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to users. (c) provision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and training to users, as appropriate to ensure a sufficient level of AI literacy in line with Article 4b."
    },
    {
        "uuid":"bb43503b-2172-428d-845f-4e2f0770bfd5",
        "amendment_number":45,
        "author":"JURI",
        "title":"Article 9 – paragraph 9",
        "text":"9. For credit institutions regulated by Directive 2013\/36\/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those institutions pursuant to Article 74 of that Directive. 9. For providers of AI systems already covered by other acts of Union law that require them to put in place specific risk management systems, including credit institutions regulated by Directive 2013\/36\/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those acts of Union law."
    },
    {
        "uuid":"2b91ba2e-0440-4b31-b300-b8028ed82e7c",
        "amendment_number":46,
        "author":"JURI",
        "title":"Article 13 – title",
        "text":"Transparency and provision of information to users Transparency and provision of information"
    },
    {
        "uuid":"a11d2d11-2bbd-461f-b2b1-238df1231a3e",
        "amendment_number":47,
        "author":"JURI",
        "title":"Article 13 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the user and of the provider set out in Chapter 3 of this Title. 1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable providers and users to reasonably understand the system’s functioning. Appropriate transparency shall be ensured in accordance with the intended purpose of the AI system, with a view to achieving compliance with the relevant obligations of the provider and user set out in Chapter 3 of this Title. Transparency shall thereby mean that, at the time the high-risk AI system is placed on the market, all technical means available in accordance with the generally acknowledged state of art are used to ensure that the AI system’s output is interpretable by the provider and the user. The user shall be enabled to understand and use the AI system appropriately by generally knowing how the AI system works and what data it processes, allowing the user to explain the decisions taken by the AI system to the affected person pursuant to Article 68(c)."
    },
    {
        "uuid":"f9890d12-b9a6-48c5-b73f-f9f3dacd566f",
        "amendment_number":48,
        "author":"JURI",
        "title":"Article 13 – paragraph 2",
        "text":"2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users. 2. High-risk AI systems shall be accompanied by intelligible instructions for use in an appropriate digital format or made otherwise available in a durable medium that include concise, correct, clear and to the extent possible complete information that helps operating and maintaining the AI system as well as supporting informed decision-making by users and is reasonably relevant, accessible and comprehensible to users ."
    },
    {
        "uuid":"573a23d2-2536-4da0-ac53-83e07e70b49c",
        "amendment_number":49,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – introductory part",
        "text":"3. The information referred to in paragraph 2 shall specify: 3. To achieve the outcomes referred to in paragraph 1, information referred to in paragraph 2 shall specify:"
    },
    {
        "uuid":"4771ac39-f657-44a9-9669-e96bc9188de7",
        "amendment_number":50,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point a",
        "text":"(a) the identity and the contact details of the provider and, where applicable, of its authorised representative; (a) the identity and the contact details of the provider and, where applicable, of its authorised representatives;"
    },
    {
        "uuid":"9385ff4e-87b8-4443-81c2-52f38a6fdbab",
        "amendment_number":51,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point a a (new)",
        "text":"(aa) where it is not the same as the provider, the identity and the contact details of the entity that carried out the conformity assessment and, where applicable, of its authorised representative;"
    },
    {
        "uuid":"8bb3f2d6-2908-4536-857a-ad5796d2b927",
        "amendment_number":52,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point b – introductory part",
        "text":"(b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including: (b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including, where appropriate:"
    },
    {
        "uuid":"f29ac08e-0bd6-46c0-a680-1e7375095e88",
        "amendment_number":53,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point b – point ii",
        "text":"(ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (ii) the level of accuracy, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any clearly known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;"
    },
    {
        "uuid":"68f8943b-84a7-43a4-a77b-9f764bc499ab",
        "amendment_number":54,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point b – point iii",
        "text":"(iii) any known or foreseeable (iii) any clearly known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights; circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety, fundamental rights or the environment, including, where appropriate, illustrative examples of such limitations and of scenarios for which the system should not be used;"
    },
    {
        "uuid":"ddc6fd24-68e8-4bb1-a94b-a6b523952301",
        "amendment_number":55,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point b – point iii a (new)",
        "text":"(iiia) the degree to which the AI system can provide an explanation for decisions it takes;"
    },
    {
        "uuid":"e07f93c2-eeb6-4b98-87ad-defad1e7d446",
        "amendment_number":56,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point b – point v",
        "text":"(v) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system. (v) relevant information about user actions that may influence system performance, including type or quality of input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system."
    },
    {
        "uuid":"339fa700-90e7-4a22-8f6e-8688a3d91357",
        "amendment_number":57,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point e",
        "text":"(e) the expected lifetime of the high- risk AI system and any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates. (e) any necessary maintenance and care measures to ensure the proper functioning of that AI system, including as regards software updates, through its expected lifetime."
    },
    {
        "uuid":"bad77c09-02da-45f6-8c52-8b68070cdcaf",
        "amendment_number":58,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point e a (new)",
        "text":"(ea) a description of the mechanisms included within the AI system that allows users to properly collect, store and interpret the logs in accordance with Article 12(1)."
    },
    {
        "uuid":"4d90be43-8d79-4834-8f76-40daf56086bd",
        "amendment_number":59,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 – point e b (new)",
        "text":"(eb) The information shall be provided at least in the language of the country where the AI system is used."
    },
    {
        "uuid":"8ff7aa3c-afaf-43af-8b60-835e6f10b456",
        "amendment_number":60,
        "author":"JURI",
        "title":"Article 13 – paragraph 3 a (new)",
        "text":"3a. In order to comply with the obligations laid down in this Article, providers and users shall ensure a sufficient level of AI literacy in line with Article 4b."
    },
    {
        "uuid":"16b4d5ac-f391-4c86-b629-347b2a239347",
        "amendment_number":61,
        "author":"JURI",
        "title":"Article 14 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. 1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they be effectively overseen by natural persons as proportionate to the risks associated with those systems. Natural persons in charge of ensuring human oversight shall have sufficient level of AI literacy in accordance with Article 4b and the necessary support and authority to exercise that function, during the period in which the AI system is in use and to allow for thorough investigation after an incident."
    },
    {
        "uuid":"1ae4d089-f9e4-4a52-b818-546bdc153c16",
        "amendment_number":62,
        "author":"JURI",
        "title":"Article 14 – paragraph 2",
        "text":"2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. 2. Human oversight shall aim at preventing or minimising the risks to health, safety, fundamental rights or environment that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter and where decisions based solely on automated processing by AI systems produce legal or otherwise significant effects on the persons or groups of persons on which the system is to be used."
    },
    {
        "uuid":"9686e018-5255-410d-a88d-34e1240a289b",
        "amendment_number":63,
        "author":"JURI",
        "title":"Article 14 – paragraph 3 – introductory part",
        "text":"3. Human oversight shall be ensured through either one or all of the following measures: 3. Human oversight shall take into account the specific risks, the level of automation, and context of the AI system and shall be ensured through either one or all of the following types of measures:"
    },
    {
        "uuid":"9be62d53-00b0-45e5-96c3-847703c3880a",
        "amendment_number":64,
        "author":"JURI",
        "title":"Article 14 – paragraph 4 – introductory part",
        "text":"4. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: 4. For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be provided to the user in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate to the circumstances:"
    },
    {
        "uuid":"afdb7f7b-7468-4581-b507-fca41f412ad7",
        "amendment_number":65,
        "author":"JURI",
        "title":"Article 14 – paragraph 4 – point a",
        "text":"(a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; (a) be aware of and sufficiently understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible;"
    },
    {
        "uuid":"9df1da8a-3f0e-460e-917b-e2e488e8537f",
        "amendment_number":66,
        "author":"JURI",
        "title":"Article 14 – paragraph 4 – point e",
        "text":"(e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure. (e) be able to intervene on the operation of the high-risk AI system or interrupt, the system through a “stop” button or a similar procedure that allows the system to come to a halt in a safe state, except if the human interference increases the risks or would negatively impact the performance in consideration of generally acknowledged state-of-the- art."
    },
    {
        "uuid":"6eadec24-1906-4f7d-971b-be1bd236640b",
        "amendment_number":67,
        "author":"JURI",
        "title":"Article 14 – paragraph 5",
        "text":"5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons. 5. For high-risk AI systems referred to in point(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons with the necessary competence, training and authority."
    },
    {
        "uuid":"af3b0b56-68d7-4409-9e51-c72208f23df7",
        "amendment_number":68,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point a",
        "text":"(a) ensure that their high-risk AI systems are compliant with the requirements set out in Chapter 2 of this (a) ensure that their high-risk AI systems are compliant with the requirements set out in Chapter 2 of this Title; Title, before placing them on the market or putting them into service;"
    },
    {
        "uuid":"c498d938-a5bf-4e56-bcb3-a267a3d2f5da",
        "amendment_number":69,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point a a (new)",
        "text":"(aa) indicate their name, registered trade name or registered trade mark, the address at which they can be contacted on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable;"
    },
    {
        "uuid":"26fb4364-0e49-4780-a693-d644f89e81f0",
        "amendment_number":70,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point c",
        "text":"(c) draw-up the technical documentation of the high-risk AI system; (c) keep the documentation and, where not yet available, draw up the technical documentation referred to in Article 18;"
    },
    {
        "uuid":"bc3492b7-bed8-4324-806f-c5b7cae067e1",
        "amendment_number":71,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point d",
        "text":"(d) when under their control, keep the logs automatically generated by their high- risk AI systems; (d) when under their control, keep the logs automatically generated by their high- risk AI systems, in accordance with Article 20;"
    },
    {
        "uuid":"d5d0db96-9b7f-46c0-8cb0-aea2a32a5dd1",
        "amendment_number":72,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point e",
        "text":"(e) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service; (e) carry out the relevant conformity assessment procedure, as provided for in Article 19, prior to its placing on the market or putting into service;"
    },
    {
        "uuid":"28ff8edf-a7f9-45ab-b9cd-3a38e0187e56",
        "amendment_number":73,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point g",
        "text":"(g) take the necessary corrective actions, if the high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title; (g) take the necessary corrective actions as referred to in Article 21, if the high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title;"
    },
    {
        "uuid":"37fbc290-4d1d-4f44-b97b-7190eec55c74",
        "amendment_number":74,
        "author":"JURI",
        "title":"Article 16 – paragraph 1 – point j",
        "text":"(j) upon request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title. (j) upon reasoned request of a national competent authority, provide the relevant information and documentation to demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title."
    },
    {
        "uuid":"20373d20-6335-42cc-b5f2-c1530d9e7592",
        "amendment_number":75,
        "author":"JURI",
        "title":"Article 23 a (new)",
        "text":"Article 23a the product manufacturer after the product has been placed on the market. 5. Third parties involved in the sale and the supply of software including general purpose application programming interfaces (API), software tools and components, or providers of network services shall not be considered providers for the purposes of this Regulation."
    },
    {
        "uuid":"eb5aafba-a440-4368-8729-b14bb48cdcb9",
        "amendment_number":76,
        "author":"JURI",
        "title":"Article 29 – paragraph 1",
        "text":"1. Users of high-risk AI systems shall use such systems in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5. 1. Users of high-risk AI systems shall take appropriate organisational measures and ensure that the use of such systems takes place in accordance with the instructions of use accompanying the systems pursuant to paragraphs 1a to 5 of this Article. Users shall bear responsibility in case of any use of the AI system that is not in accordance with the instructions of use accompanying the systems."
    },
    {
        "uuid":"38829332-01d4-48ce-b333-bfd83f06b559",
        "amendment_number":77,
        "author":"JURI",
        "title":"Article 29 – paragraph 1 a (new)",
        "text":"1a. To the extent the user exercises control over the high-risk AI system, that user shall assign human oversight to natural persons who have the necessary AI literacy in accordance with Article 4b."
    },
    {
        "uuid":"28eccb57-4555-41c7-8701-d81811ea11bb",
        "amendment_number":78,
        "author":"JURI",
        "title":"Article 29 – paragraph 2",
        "text":"2. The obligations in paragraph 1 are without prejudice to other user obligations under Union or national law and to the user’s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. 2. The obligations in paragraphs 1 and 1a are without prejudice to other obligations of the user under Union or national law and to the user’s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider."
    },
    {
        "uuid":"fc878c29-b743-4949-a690-59f4dedb5e1f",
        "amendment_number":79,
        "author":"JURI",
        "title":"Article 29 – paragraph 3",
        "text":"3. Without prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system. 3. Without prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system."
    },
    {
        "uuid":"8fa5d257-ce74-449a-94f6-8474f04bb4d3",
        "amendment_number":80,
        "author":"JURI",
        "title":"Article 29 – paragraph 4 – subparagraph 1",
        "text":"4. Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use. When they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system. They shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use 4. Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use and, when relevant, inform the provider in accordance with Article 61. To the extent the user exercises control over the high-risk AI system, it shall also perform a risk assessment in accordance with Article 9 but limited to the potential adverse effects of using the high-risk AI system as well as the respective mitigation measures. When they have reasons to consider that the use in accordance with the instructions of use of the AI system. In case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis. may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system. They shall also inform the provider or distributor and competent supervisory authority when they have identified any serious incident or malfunctioning and interrupt the use of the AI system. In case the user is not able to reach the provider importer or distributer, Article 62 shall apply mutatis mutandis."
    },
    {
        "uuid":"5f4f7e1e-6c12-445a-8638-219e29b6e07b",
        "amendment_number":81,
        "author":"JURI",
        "title":"Article 29 – paragraph 5 – subparagraph 1",
        "text":"5. Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control. The logs shall be kept for a period that is appropriate in the light of the intended purpose of the high-risk AI system and applicable legal obligations under Union or national law. 5. Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control and is feasible from a technical point of view. They shall keep them for a period of at least six months, unless provided otherwise in applicable Union or national law."
    },
    {
        "uuid":"02c73a8f-1d55-40e7-8b2c-b25263ab6ace",
        "amendment_number":82,
        "author":"JURI",
        "title":"Article 29 – paragraph 6",
        "text":"6. Users of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016\/679 or Article 27 of Directive (EU) 2016\/680, where applicable. 6. Users of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016\/679 or Article 27 of Directive (EU) 2016\/680 and may revert, where applicable, to those data protection impact assessments for fulfilling the obligations set out in this Article."
    },
    {
        "uuid":"a888ee49-f328-4a8f-9d56-cae08c67a495",
        "amendment_number":83,
        "author":"JURI",
        "title":"Article 29 – paragraph 6 a (new)",
        "text":"6a. The provider shall be obliged to cooperate closely with the user and in particular provide the user with the necessary and appropriate information to allow the fulfilment of the obligations set out in this Article."
    },
    {
        "uuid":"2ace9577-fb15-4270-9051-8f1ad570c77b",
        "amendment_number":84,
        "author":"JURI",
        "title":"Article 29 – paragraph 6 b (new)",
        "text":"6b. Users shall cooperate with national competent authorities on any action those authorities take in relation to an AI system."
    },
    {
        "uuid":"49272bf2-9473-4bfa-87aa-561e1438499c",
        "amendment_number":85,
        "author":"JURI",
        "title":"Article 40 – paragraph 1",
        "text":"High-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements. 1. High-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements."
    },
    {
        "uuid":"ad4455b5-d0f5-4e58-9e48-fc9a45de09bd",
        "amendment_number":86,
        "author":"JURI",
        "title":"Article 40 – paragraph 1 a (new)",
        "text":"1a. When issuing a standardisation request to European standardisation organisations in accordance with Article 10 of Regulation (EU)1025\/2012, the Commission shall specify that standards are coherent, easy to implement and drafted in such a way that they aim to fulfil in particular the following objectives: a) ensure that AI systems placed on the market or put into service in the Union are safe, trustworthy and respect Union values and strengthen the Union's digital sovereignty; b) take into account the general principles for trustworthy AI set out in Article 4a; c) promote investment and innovation in AI, as well as competitiveness and growth of the Union market; d) enhance multistakeholder governance, representative of all relevant European stakeholders (e.g. industry, SMEs, civil society, social partners, researchers); e) contribute to strengthening global cooperation on standardisation in the field of AI that is consistent with Union values, fundamental rights and interests. The Commission shall request the European standardisation organisations to provide evidence of their best efforts to fulfil the above objectives. 1b. The Commission shall issue standardisation requests covering all requirements of this Regulation in accordance with Article 10 of Regulation (EU) No1025\/2012 before the date of entry into force of this Regulation."
    },
    {
        "uuid":"ec02321a-69d1-4aed-8258-557b76cf87d1",
        "amendment_number":87,
        "author":"JURI",
        "title":"Article 52 – title",
        "text":"Transparency obligations for certain AI systems Transparency obligations"
    },
    {
        "uuid":"4ecac0e9-d6a6-4da4-9449-22bd1b15ad94",
        "amendment_number":88,
        "author":"JURI",
        "title":"Article 52 – paragraph 1",
        "text":"1. Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence. 1. Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that the AI system, the provider itself or the user informs the natural person exposed to an AI system that they are interacting with an AI system in a timely, clear and intelligible manner, unless this is obvious from the circumstances and the context of use. Where appropriate and relevant, this information shall also include which functions are AI enabled, if there is human oversight, and who is responsible for the decision-making process, as well as the existing rights and processes that, according to Union and national law, allow natural persons or their representatives to object against the application of such systems to them and to seek judicial redress against decisions taken by or harm caused by AI systems, including their right to seek an explanation. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence."
    },
    {
        "uuid":"1dd8d63f-4795-424e-b4f9-bef0b95df80a",
        "amendment_number":89,
        "author":"JURI",
        "title":"Article 52 – paragraph 2",
        "text":"2. Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences. 2. Users of an emotion recognition system or a biometric categorisation system which is not prohibited pursuant to Article 5 shall inform in a timely, clear and intelligible manner of the operation of the system the natural persons exposed thereto and obtain their consent prior to the processing of their biometric and other personal data in accordance with Regulation (EU) 2016\/679, Regulation (EU) 2016\/1725 and Directive (EU) 2016\/280, as applicable. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences."
    },
    {
        "uuid":"9c9ef2ce-33f7-4742-8b0c-a4419fd2450a",
        "amendment_number":90,
        "author":"JURI",
        "title":"Article 52 – paragraph 3 – subparagraph 1",
        "text":"3. Users of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose that the content has been artificially generated or manipulated. 3. Users of an AI system that generates or manipulates text, audio or visual content that would falsely appear to be authentic or truthful and which features depictions of people appearing to say or do things they did not say or do, without their consent (‘deep fake’), shall disclose in an appropriate, timely, clear and visible manner that the content has been artificially generated or manipulated, as well as, whenever possible, the name of the natural or legal person that generated or manipulated it. Disclosure shall mean labelling the content in a way that informs that the content is inauthentic and that is clearly visible for the recipient of that content. To label the content, users shall take into account the generally acknowledged state of the art and relevant harmonised standards and specifications."
    },
    {
        "uuid":"bd8b0012-5255-4683-aeb1-cce1e9bb4914",
        "amendment_number":91,
        "author":"JURI",
        "title":"Article 52 – paragraph 3 – subparagraph 2",
        "text":"However, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. 3a. Paragraph 3 shall not apply where the use of an AI system that generates or manipulates text, audio or visual content is authorized by law or if it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. Where the content forms part of an evidently creative, satirical, artistic or fictional cinematographic, video games visuals and analogous work or programme, transparency obligations set out in paragraph 3 are limited to disclosing of the existence of such generated or manipulated content in an appropriate clear and visible manner that does not hamper the display of the work and disclosing the applicable copyrights, where relevant. It shall also not prevent law enforcement authorities from using AI systems intended to detect deep fakes and prevent, investigate and prosecute criminal offences linked with their use"
    },
    {
        "uuid":"f5c87fbd-2500-477c-8dfd-40b59ca8b688",
        "amendment_number":92,
        "author":"JURI",
        "title":"Article 52 – paragraph 3 b (new)",
        "text":"3b. The information referred to in paragraphs 1 to 3 shall be provided to the natural persons at the latest at the time of the first interaction or exposure. It shall be accessible to vulnerable persons, such as persons with disabilities or children, complete, where relevant and appropriate, with intervention or flagging procedures for the exposed natural person taking into account the generally acknowledged state of the art and relevant harmonised standards and common specifications."
    },
    {
        "uuid":"d482db7c-a213-4e25-a2de-9cf45c260e7f",
        "amendment_number":93,
        "author":"JURI",
        "title":"Article 56 – paragraph 1",
        "text":"1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established. 1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established as an independent body with its own legal personality to promote a trustworthy, effective and competitive internal market for artificial intelligence. The Board shall be organised in a way that guarantees the independence, objectivity and impartiality of its activities and shall have a secretariat, a strong mandate as well as sufficient resources and skilled personnel at its disposal for assistance in the proper performance of its tasks laid down in Article 58."
    },
    {
        "uuid":"3e3723d7-3657-4716-b936-b52ecd449069",
        "amendment_number":94,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – introductory part",
        "text":"2. The Board shall provide advice and assistance to the Commission in order to: 2. The Board shall provide advice and assistance to the Commission and the Member States, when implementing Union law related to artificial intelligence as well as cooperate with the providers and users of AI systems in order to:"
    },
    {
        "uuid":"e4305960-7f56-4bcf-a817-cb5908c8279c",
        "amendment_number":95,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – point a",
        "text":"(a) contribute to the effective cooperation of the national supervisory authorities and the Commission with regard to matters covered by this Regulation; (a) promote and support the effective cooperation of the national supervisory authorities and the Commission;"
    },
    {
        "uuid":"5cc8ac14-0a66-4ea9-a644-12fae865ac3e",
        "amendment_number":96,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – point c",
        "text":"(c) assist the national supervisory authorities and the Commission in ensuring the consistent application of this Regulation. (c) assist the Commission, national supervisory authorities and other national competent authorities in ensuring the consistent application of this Regulation, in particular in line with the consistency mechanism referred to in Article 59a (3)."
    },
    {
        "uuid":"86be06dc-266b-462b-80ba-53aa5d2032f0",
        "amendment_number":97,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – point c a (new)",
        "text":"(ca) assist providers and users of AI systems to meet the requirements of this Regulation, as well as those set out in present and future Union legislation, in particular SMEs and start-ups."
    },
    {
        "uuid":"93ad26b3-f8ad-4cdf-b705-71dba1431a2e",
        "amendment_number":98,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – point c b (new)",
        "text":"(cb) provide particular oversight, monitoring and regular dialogue with the providers of general purpose AI systems about their compliance with this Regulation. Any such meeting shall be open to national supervisory authorities, notified bodies and market surveillance authorities to attend and contribute;"
    },
    {
        "uuid":"f5c25f7d-b1e7-4db1-8a82-27261fa194aa",
        "amendment_number":99,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 – point c c (new)",
        "text":"(cc) propose amendments to Annex I and III."
    },
    {
        "uuid":"7e1bf8d5-5a69-4c08-9748-020b8324bf57",
        "amendment_number":100,
        "author":"JURI",
        "title":"Article 56 – paragraph 2 a (new)",
        "text":"2a. The Board shall act as a reference point for advice and expertise for Union institutions, bodies, offices and agencies as well as for other relevant stakeholders on matters related to artificial intelligence."
    },
    {
        "uuid":"da15a33e-4c75-4f27-a75d-f80d3247b537",
        "amendment_number":101,
        "author":"JURI",
        "title":"Article 57 – title",
        "text":"Structure of the Board Mandate and structure of the Board"
    },
    {
        "uuid":"5a046408-0753-41bd-aec1-6b7b87fd2044",
        "amendment_number":102,
        "author":"JURI",
        "title":"Article 57 – paragraph 1",
        "text":"1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. 1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. The Board composition shall be gender balanced. The European Data Protection Supervisor, the Chairperson of the EU Agency for Fundamental Rights, the Executive director of the EU Agency for Cybersecurity, the Chair of the High Level Expert Group on AI, the Director- General of the Joint Research Centre, and the presidents of the European Committee for Standardization, the European Committee for Electrotechnical Standardization, and the European Telecommunications Standards Institute shall be invited as permanent observers with the right to speak but without voting rights."
    },
    {
        "uuid":"4829058a-54fc-497a-9dfb-b048053325fb",
        "amendment_number":103,
        "author":"JURI",
        "title":"Article 57 – paragraph 2",
        "text":"2. The Board shall adopt its rules of procedure by a simple majority of its members, following the consent of the Commission. The rules of procedure shall also contain the operational aspects related to the execution of the Board’s tasks as listed in Article 58. The Board may establish sub-groups as appropriate for the purpose of examining specific questions. 2. The Board shall adopt its rules of procedure by a simple majority of its members with the assistance of its secretariat. The rules of procedure shall also contain the operational aspects related to the execution of the Board’s tasks as listed in Article 58. The Board may establish standing or temporary sub- groups as appropriate for the purpose of examining specific questions."
    },
    {
        "uuid":"ff129a0e-f9c6-4c7f-ba04-78d040d26a27",
        "amendment_number":104,
        "author":"JURI",
        "title":"Article 57 – paragraph 3",
        "text":"3. The Board shall be chaired by the Commission. The Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation. 3. The Board shall be co-chaired by the Commission and a representative chosen from among the delegates of the Member States. The Board’s secretariat shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Board’s secretariat shall also provide administrative and analytical support for the activities of the Board pursuant to this Regulation."
    },
    {
        "uuid":"ec3b7083-0809-44ce-a70b-20cea6d7efc6",
        "amendment_number":105,
        "author":"JURI",
        "title":"Article 57 – paragraph 4",
        "text":"4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that 4. The Board shall regularly invite external experts, in particular from organisations representing the interests of the providers and users of AI systems, SMEs and start-ups, civil society end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. organisations, trade unions, representatives of affected persons, academia and researchers, testing and experimentation facilities and standardisation organisations, to attend its meetings in order to ensure accountability and appropriate participation of external actors. The agenda and the minutes of its meetings shall be published online. The Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups."
    },
    {
        "uuid":"300327d0-1456-474f-960e-5b7afc65bd15",
        "amendment_number":106,
        "author":"JURI",
        "title":"Article 57 – paragraph 4 a (new)",
        "text":"4a. Without prejudice to paragraph 4, the Board’s Secretariat shall organise four additional meetings between the Board and the High Level Expert Group on Trustworthy AI to allow them to share their practical and technical expertise every quarter of the year."
    },
    {
        "uuid":"1033f3ed-02ce-4c29-a8c9-eea2e039df79",
        "amendment_number":107,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – introductory part",
        "text":"When providing advice and assistance to the Commission in the context of Article 56(2), the Board shall in particular: When providing advice and assistance to the Commission and the Member States in the context of Article 56(2), the Board shall in particular:"
    },
    {
        "uuid":"663399c0-c0d6-4201-ad01-ee24a70ca931",
        "amendment_number":108,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point a",
        "text":"(a) collect and share expertise and best practices among Member States; (a) collect and share expertise and best practices among Member States, including on the promotion of AI literacy and awareness raising initiatives on Artificial Intelligence and this Regulation;"
    },
    {
        "uuid":"52b5254c-ac0f-4df9-a499-7581d17ecf81",
        "amendment_number":109,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point a a (new)",
        "text":"(aa) promote and support the cooperation among national supervisory authorities and the Commission;"
    },
    {
        "uuid":"c28dee0c-13db-4a4b-bfc5-656725da126d",
        "amendment_number":110,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point b",
        "text":"(b) contribute to uniform administrative practices in the Member States, including for the functioning of regulatory sandboxes referred to in Article 53; (b) contribute to uniform administrative practices in the Member States, including for the assessment, establishing, managing with the meaning of fostering cooperation and guaranteeing consistency among regulatory sandboxes, and functioning of regulatory sandboxes referred to in Article 53;"
    },
    {
        "uuid":"17c349cb-6f5d-415a-9a10-bbf3d4944dde",
        "amendment_number":111,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – introductory part",
        "text":"(c) issue opinions, recommendations or written contributions on matters related (c) issue guidelines, recommendations or written contributions on matters related to the implementation of this Regulation, in particular to the implementation of this Regulation, in particular"
    },
    {
        "uuid":"e3ad16cc-b7a6-4904-81d5-a0d7979595bf",
        "amendment_number":112,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point ii a (new)",
        "text":"(iia) on the provisions related to post market monitoring as referred to in Article 61,"
    },
    {
        "uuid":"3e742052-ac94-40d3-a0ba-41df33c6f1ac",
        "amendment_number":113,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point iii a (new)",
        "text":"(iiia) on the need for the amendment of each of the Annexes as referred to in Article 73, as well as all other provisions in this Regulation that the Commission can amend, in light of the available evidence."
    },
    {
        "uuid":"0408be1c-2ffb-4ba6-9ec1-ab0c26ed85e4",
        "amendment_number":114,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point iii b (new)",
        "text":"(iiib) on activities and decisions of Member States regarding post-market monitoring, information sharing, market surveillance referred to in Title VIII;"
    },
    {
        "uuid":"995c41fa-b16c-4a1b-a597-2ba3cd6ff50c",
        "amendment_number":115,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point iii c (new)",
        "text":"(iiic) on common criteria for market operators and competent authorities having the same understanding of concepts such as the 'generally acknowledged state of the art' referred to in Articles 9(3), 13(1), 14(4), 23a(3) or 52(3a), 'foreseeable risks' referred to in Articles 9(2), point (a), and 'foreseeable misuse' referred to in Article 3(13), Article 9(2), point (b), Article 9(4), Article 13(3), point (b)(iii), Article 14(2) and Article 23a(3c);"
    },
    {
        "uuid":"02ef8f95-62cb-40eb-9dee-8d5320070004",
        "amendment_number":116,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point iii d (new)",
        "text":"(iiid) on the verification of the alignment with the legal acts listed in Annex II, including with the implementation matters related to those acts."
    },
    {
        "uuid":"064050d6-c34b-49ff-b1a6-7054bb6385ec",
        "amendment_number":117,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c – point iii e (new)",
        "text":"(iiie) on the respect of the general principles applicable to all AI systems referred to in Article 4a;"
    },
    {
        "uuid":"58350e66-2db5-4eb6-a6b0-97c7c730536c",
        "amendment_number":118,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c a (new)",
        "text":"(ca) carry out annual reviews and analyses of the complaints sent to and findings made by national supervisory authorities, of the serious incidents and malfunctioning reports referred to in Article 62, and of the new registration in the EU Database referred to in Article 60 to identify trends and potential emerging issues threatening the future health and safety and fundamental rights of citizens that are not adequately addressed by this Regulation;"
    },
    {
        "uuid":"ca2cd48f-b992-4ebf-9f84-b361015e9523",
        "amendment_number":119,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c b (new)",
        "text":"(cb) carry out biannual horizontal scanning and foresight exercises to extrapolate the impact that scientific developments, trends and emerging issues can have on the Union;"
    },
    {
        "uuid":"09745f3d-d26b-4d0c-8159-62dddb4c7cc0",
        "amendment_number":120,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c c (new)",
        "text":"(cc) annually publish recommendations to the Commission, in particular on the categorisation of prohibited practices, high-risk systems, and codes of conduct for AI systems that are not classified as high-risk;"
    },
    {
        "uuid":"c9156a88-8783-4855-aa42-55353ac385cd",
        "amendment_number":121,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c d (new)",
        "text":"(cd) encourage and facilitate the drawing up of codes of conduct as referred to in Article 69;"
    },
    {
        "uuid":"58b2bd04-7064-4273-beb3-e6301696f078",
        "amendment_number":122,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c e (new)",
        "text":"(ce) coordinate among national competent authorities and make sure that the consistency mechanism in Article 59a(3) is observed, in particular for all major cross-border cases;"
    },
    {
        "uuid":"ff032ca8-50ac-4b2c-b56c-75cd3adc3aa6",
        "amendment_number":123,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c f (new)",
        "text":"(cf) adopt binding decisions for national supervisory authorities in case the consistency mechanism is not able to solve the conflict among national supervisory authorities as it is clarified in Article 59a (6)."
    },
    {
        "uuid":"245fa359-0040-4152-976e-c952df2b6450",
        "amendment_number":124,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c g (new)",
        "text":"(cg) provide guidance material to providers and users regarding the compliance with the requirements set out in this Regulation. In particular, it shall issue guidelines: i) for the trustworthy AI technical assessment referred to in Article 4a, ii) for the methods for performing the conformity assessment based on internal control referred to Article 43; iii) to facilitate compliance with the reporting of serious incidents or malfunctioning referred to in Article 62; iv) on any other concrete procedures to be performed by providers and users when complying with this Regulation, in particular those regarding the documentation to be delivered to notified bodies and methods to provide authorities with other relevant information."
    },
    {
        "uuid":"de9cc504-dafa-4732-83e1-c6c679d95ff9",
        "amendment_number":125,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c h (new)",
        "text":"(ch) provide specific guidance to support SMEs and start-ups in complying with the obligations set out in this Regulation;"
    },
    {
        "uuid":"acf94671-c211-4b4f-821f-ec40c97c82be",
        "amendment_number":126,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c i (new)",
        "text":"(ci) raise awareness and provide guidance material to providers and users regarding the compliance with the requirement to put in place tools and measures to ensure a sufficient level of AI literacy in line with Article 4b;"
    },
    {
        "uuid":"c578e7d5-b319-4024-b632-9e80d595e06b",
        "amendment_number":127,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c j (new)",
        "text":"(cj) contribute to the Union efforts to cooperate with third countries and international organisations in view of promoting a common global approach towards trustworthy AI;"
    },
    {
        "uuid":"a6c79d9b-f4ec-42b6-9259-a3e0cd5783e1",
        "amendment_number":128,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c k (new)",
        "text":"(ck) issue yearly reports on the implementation of this Regulation, including an assessment of its impact on economic operators;"
    },
    {
        "uuid":"36cad8f7-c028-430f-b253-1f0f6e44e94d",
        "amendment_number":129,
        "author":"JURI",
        "title":"Article 58 – paragraph 1 – point c l (new)",
        "text":"(cl) provide guidance on the governance of research and development."
    },
    {
        "uuid":"f68d4643-bd61-4155-9e70-71e3cbc4cedf",
        "amendment_number":130,
        "author":"JURI",
        "title":"Article 59 – title",
        "text":"Designation of national competent Designation of national supervisory authorities authorities"
    },
    {
        "uuid":"33f533ad-67df-4551-940c-c647f4396da9",
        "amendment_number":131,
        "author":"JURI",
        "title":"Article 59 – paragraph 1",
        "text":"1. National competent authorities shall be established or designated by each Member State for the purpose of ensuring the application and implementation of this Regulation. National competent authorities shall be organised so as to safeguard the objectivity and impartiality of their activities and tasks. 1. Each Member State shall establish or designate one national supervisory authority, which shall be organised so as to safeguard the objectivity and impartiality of its activities and tasks."
    },
    {
        "uuid":"8b1a765c-f38e-44a7-ad79-76a830c58891",
        "amendment_number":132,
        "author":"JURI",
        "title":"Article 59 – paragraph 2",
        "text":"2. Each Member State shall designate a national supervisory authority among the national competent authorities. The national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority. 2. The national supervisory authority shall be in charge to ensure the application and implementation of this Regulation. With regard to high-risk AI systems, related to products to which legal acts listed in Annex II apply, the competent authorities designated under those legal acts shall continue to lead the administrative procedures. However, to the extent a case involves aspects covered by this Regulation, the competent authorities shall be bound by measures issued by the national supervisory authority designated under this Regulation. The national supervisory authority shall also act as notifying authority and market surveillance authority."
    },
    {
        "uuid":"45dba2a8-80d4-4135-bb58-42ad86a241d1",
        "amendment_number":133,
        "author":"JURI",
        "title":"Article 59 – paragraph 3",
        "text":"3. Member States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority. 3. The national competent authority in each Member State shall be the lead authority, ensure adequate coordination and act as single point of contact for this Regulation. Member States shall inform the Commission of their designations. In addition, the central contact point of each Member State should be contactable through electronic communications means."
    },
    {
        "uuid":"e5e8e3e3-b3e7-44d3-8e86-27d5aca1340a",
        "amendment_number":134,
        "author":"JURI",
        "title":"Article 59 – paragraph 4",
        "text":"4. Member States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. 4. Member States shall ensure that national supervisory authority is provided with adequate financial and human resources to fulfil its tasks under this Regulation. In particular, national supervisory authorities shall have a sufficient number of permanently available personnel, whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data, data protection and data computing, cybersecurity, competition law, fundamental rights, health and safety risks as well as knowledge of existing standards and legal requirements."
    },
    {
        "uuid":"2f04b625-5fc6-4619-8d0a-8d99dd3ff132",
        "amendment_number":135,
        "author":"JURI",
        "title":"Article 59 – paragraph 4 a (new)",
        "text":"4a. The national competent authority shall satisfy the minimum cybersecurity requirements set out for public administration entities identified as operators of essential services pursuant to Directive (…) on measures for a high common level of cybersecurity across the Union, repealing Directive (EU) 2016\/1148."
    },
    {
        "uuid":"f42ee440-ce22-4f5d-ac22-eda5535e7f6a",
        "amendment_number":136,
        "author":"JURI",
        "title":"Article 59 – paragraph 4 b (new)",
        "text":"4b. Any information and documentation obtained by the national supervisory authority pursuant to the provisions of this Article shall be treated in compliance with the confidentiality obligations set out in Article 70."
    },
    {
        "uuid":"a12d800c-745a-467f-b7e2-e643d4e45906",
        "amendment_number":137,
        "author":"JURI",
        "title":"Article 59 – paragraph 5",
        "text":"5. Member States shall report to the Commission on an annual basis on the status of the financial and human resources of the national competent authorities with an assessment of their adequacy. The Commission shall transmit that information to the Board for discussion and possible recommendations. 5. Member States shall report to the Commission on an annual basis on the status of the financial and human resources of the national supervisory authority with an assessment of their adequacy. The Commission shall transmit that information to the Board for discussion and possible recommendations."
    },
    {
        "uuid":"3f35ac9e-b062-4634-8b31-51c777044cd9",
        "amendment_number":138,
        "author":"JURI",
        "title":"Article 59 – paragraph 6",
        "text":"6. The Commission shall facilitate the exchange of experience between national competent authorities. 6. The Commission and the Board shall facilitate the exchange of experience between national supervisory authorities."
    },
    {
        "uuid":"6fe59dde-9944-4846-b90a-bf5e9ad18917",
        "amendment_number":139,
        "author":"JURI",
        "title":"Article 59 – paragraph 7",
        "text":"7. National competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate. Member States may also establish one central contact point for communication with operators. 7. National supervisory authorities may provide guidance and advice on the implementation of this Regulation, including to SMEs and start-ups, as long as it is not in contradiction with the Board’s or the Commission’s guidance and advice. Whenever national supervisory authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted as appropriate."
    },
    {
        "uuid":"7d8fe713-4add-4d70-92b8-ae337fe17c69",
        "amendment_number":140,
        "author":"JURI",
        "title":"Article 59 – paragraph 8",
        "text":"8. When Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision. 8. When Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision and coordination."
    },
    {
        "uuid":"c4d986a3-c0ca-4b75-a921-c4ed6f0af3a8",
        "amendment_number":141,
        "author":"JURI",
        "title":"Article 59 a (new)",
        "text":"Article 59a Consistency mechanism for cross-border cases 1. Each national supervisory authority shall perform the tasks assigned to and the exercise of the powers conferred on it in accordance with this Regulation on the territory of its own Member State. 2. In the event of a cross-border case involving two or more national supervisory authorities, the national supervisory authority of the Member State where the provider’s or user’s place of central administration in the Union is established or where the authorised representative is appointed, shall be competent to act as lead national supervisory authority for a cross-border case that involves an AI-system. 3. In the case referred to in paragraph 2, the national supervisory authorities shall cooperate, exchange all relevant information with each other in due time, provide mutual assistance and execute joint operations. National supervisory authorities shall cooperate in order to reach a consensus. 4. In case of a serious disagreement between two or more national supervisory authorities, the lead national supervisory authority shall notify the Board and communicate without delay all relevant information related to the case to the Board. 5. The Board shall within three months of the notification referred to in paragraph 4, issue a binding decision to the national supervisory authorities."
    },
    {
        "uuid":"aef5db77-04e3-4403-9f0e-1a455d743737",
        "amendment_number":142,
        "author":"JURI",
        "title":"Article 62 – paragraph 1 – subparagraph 1",
        "text":"Such notification shall be made immediately after the provider has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the providers becomes aware of the serious incident or of the malfunctioning. Such notification shall be made without undue delay after the provider has established a causal link between the AI system and the serious incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 72 hours after the provider becomes aware of the serious incident or of the malfunctioning."
    },
    {
        "uuid":"4a9a9869-7704-4dac-b6f7-b00fe3c75c7a",
        "amendment_number":143,
        "author":"JURI",
        "title":"Article 62 – paragraph 1 – subparagraph 1 a (new)",
        "text":"No report under this Article is required if the serious incident or malfunctioning is also to be reported by providers to comply with obligations laid down by other acts of Union law. In that case, the authorities competent under those acts of Union law shall forward the received report to the national supervisory authority designated under this Regulation."
    },
    {
        "uuid":"14bbf8c1-a9a0-40f8-9357-41768db2182e",
        "amendment_number":144,
        "author":"JURI",
        "title":"Article 69 – paragraph 1",
        "text":"1. The Commission and the Member States shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI 1. The Commission, the Board and the Member States shall encourage and facilitate the drawing up of codes of conduct intended, including where they systems other than high-risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems. are drawn up in order to demonstrate how AI systems respect the principles set out in Article 4a and can thereby be considered trustworthy, to foster the voluntary application to AI systems other than high- risk AI systems of the requirements set out in Title III, Chapter 2 on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems."
    },
    {
        "uuid":"e0813b35-fac2-48af-9060-f1cdebacb7e5",
        "amendment_number":145,
        "author":"JURI",
        "title":"Article 69 – paragraph 2",
        "text":"2. The Commission and the Board shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to AI systems of requirements related for example to environmental sustainability, accessibility for persons with a disability, stakeholders participation in the design and development of the AI systems and diversity of development teams on the basis of clear objectives and key performance indicators to measure the achievement of those objectives. 2. Codes of conduct intended to foster the voluntary compliance with the principles underpinning trustworthy AI systems, shall, in particular: (a) aim for a sufficient level of AI literacy among their staff and other persons dealing with the operation and use of AI systems in order to observe such principles; (b) assess to what extent their AI systems may affect vulnerable persons or groups of persons, including children, the elderly, migrants and persons with disabilities or whether measures could be put in place in order to increase accessibility, or otherwise support such persons or groups of persons; (c) consider the way in which the use of their AI systems may have an impact or can increase diversity, gender balance and equality; (d) have regard to whether their AI systems can be used in a way that, directly or indirectly, may residually or significantly reinforce existing biases or inequalities; (e) reflect on the need and relevance of having in place diverse development teams in view of securing an inclusive design of their systems; (f) give careful consideration to whether their systems can have a negative societal impact, notably concerning political institutions and democratic processes; (g) evaluate how AI systems can contribute to environmental sustainability and in particular to the Union’s commitments under the European Green Deal and the European Declaration on Digital Rights and Principles."
    },
    {
        "uuid":"8b2d2df0-30fe-41d1-b3ea-7d9a161adfcb",
        "amendment_number":146,
        "author":"JURI",
        "title":"Article 69 – paragraph 3",
        "text":"3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems. 3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders, including scientific researchers, and their representative organisations, in particular trade unions, and consumer organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems. Providers adopting codes of conduct will designate at least one natural person responsible for internal monitoring."
    },
    {
        "uuid":"e4e2fa72-fb39-4e45-b670-5ef94a225a6c",
        "amendment_number":147,
        "author":"JURI",
        "title":"Article 69 – paragraph 4",
        "text":"4. The Commission and the Board shall take into account the specific interests and needs of the small-scale providers and start-ups when encouraging and facilitating the drawing up of codes of conduct. 4. The Commission and the Board shall take into account the specific interests and needs of SMEs and start-ups when encouraging and facilitating the drawing up of codes of conduct."
    },
    {
        "uuid":"f1bfd45a-7bf3-449d-9e14-c8d27c628731",
        "amendment_number":148,
        "author":"JURI",
        "title":"Article 69 a (new)",
        "text":"Article 69a Right to lodge a complaint before a supervisory authority 1. Without prejudice to any other administrative or judicial remedy, every natural or legal person shall have the right to lodge a complaint with a supervisory authority, in particular in the Member State of his or her habitual residence, place of work or place of the alleged infringement if the natural or legal person considers that their health, safety, fundamental rights, their right to an explanation or any other of their rights deriving from the obligations laid down in this Regulation have been breached by the provider or the user of an AI system falling within the scope of this Regulation. Such complaint may be lodged through a representative action for the protection of the collective interests of consumers as provided under Directive (EU) 2020\/1828. 2. Natural or legal persons shall have a right to be heard in the complaint handling procedure and in the context of any investigations conducted by the national supervisory authority as a result of their complaint. 3. The national supervisory authority with which the complaint has been lodged shall inform the complainants about the progress and outcome of their complaint. In particular, the national supervisory authority shall take all the necessary actions to follow up on the complaints it receives and, within three months of the reception of a complaint, give the complainant a preliminary response indicating the measures it intends to take and the next steps in the procedure, if any. 4. The national supervisory authority shall take a decision on the complaint and inform the complainant on the progress and the outcome of the complaint, including the possibility of a judicial remedy pursuant to Article 68b, without delay and no later than six months after the date on which the complaint was lodged."
    },
    {
        "uuid":"c3e1336c-7610-4a85-abf5-09c7241f6962",
        "amendment_number":149,
        "author":"JURI",
        "title":"Article 69 b (new)",
        "text":"Article 69b Right to an effective judicial remedy against a national supervisory authority 1. Without prejudice to any other administrative or non-judicial remedy, each natural or legal person shall have the right to an effective judicial or non- judicial remedy, including repair, replacement, price reduction, contract termination, reimbursement of the price paid or compensation for material and immaterial damages, against a legally binding decision of a national supervisory authority concerning them that infringes their rights. 2. Without prejudice to any other administrative or non-judicial remedy, each affected person shall have the right to a an effective judicial remedy where the national supervisory authority does not handle a complaint, does not inform the complainant on the progress or preliminary outcome of the complaint lodged within three months pursuant to Article 68a(3) or does not comply with its obligation to reach a final decision on the complaint within six months pursuant to Article 68a(4) or its obligations under Article 65. 3. Proceedings against a supervisory authority shall be brought before the courts of the Member State where the national supervisory authority is established."
    },
    {
        "uuid":"271d114b-0798-4c23-bd6b-011775a224bb",
        "amendment_number":150,
        "author":"JURI",
        "title":"Article 69 c (new)",
        "text":"Article 69c Right to an explanation 1. Any affected persons subject to a decision taken by a provider or an user, on the basis of an output from an AI system falling within the scope of this Regulation, which produces legal effects that they consider to adversely impact their health, safety, fundamental rights, socio-economic well-being or any other of their rights deriving from the obligations laid down in this Regulation, shall receive from the provider or the user, at the time when the decision is communicated, a clear and meaningful explanation pursuant to Article 13(1) on the role of the AI system in the decision-making procedure, the main parameters of the decision taken and on the related input data. 2. Paragraph 1 shall not apply to the use of AI systems: (a) for which exceptions from, or restrictions to, the obligation under paragraph 1 follow from Union or national law, which lays down other appropriate safeguards for the affected persons’ rights, freedoms and legitimate interests; or (b) where the affected person has given free, explicit, specific and informed consent not to receive an explanation. The affected person shall have the right to withdraw his or her consent not to receive an explanation at any time. Prior to giving consent, the affected person shall be informed thereof. It shall be as easy to withdraw as to give consent."
    },
    {
        "uuid":"9c1c0994-0c56-4583-894a-ed8283c139ce",
        "amendment_number":151,
        "author":"JURI",
        "title":"Article 69 d (new)",
        "text":"Article 69d Representative actions 1. The following is added to Annex I of Directive (EU) 2020\/1828 on Representative actions for the protection of the collective interests of consumers: “Regulation xxxx\/xxxx of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain Union legislative acts”."
    },
    {
        "uuid":"35f36019-0448-45ff-99b4-674df4a591f8",
        "amendment_number":152,
        "author":"JURI",
        "title":"Article 84 – paragraph 4 a (new)",
        "text":"4a. Within [three years after the date of application of this Regulation referred to in Article 85(2)], the Commission shall evaluate the effectiveness of the Board to carry out its tasks and assess whether an EU Agency would be best placed to ensure an effective and harmonised implementation of this Regulation."
    },
    {
        "uuid":"429eba88-9027-454b-bb93-eef4597fb984",
        "amendment_number":1,
        "author":"ITRE",
        "title":"Recital 3 a (new)",
        "text":"(3a) Furthermore, in order for the Member States to reach their climate targets and to meet the United Nation’s Sustainable Development Goals (SDGs), Union companies should be encouraged to utilise available technological advancements in realising this goal. AI is a well-developed and ready-to-use technology that can be used to process ever-growing amount of data created along industrial processes. To facilitate investments in AI-based analysis and optimisation solutions that can help to achieve the climate goals, this Regulation should provide a predictable and proportionate environment for low- risk industrial solutions. To ensure coherence, this requires that AI systems themselves need to be designed sustainably to reduce resource usage and energy consumption, thereby limiting the damage to the environment."
    },
    {
        "uuid":"9bfee517-b8f9-4699-a58e-7b22a212babe",
        "amendment_number":2,
        "author":"ITRE",
        "title":"Recital 3 b (new)",
        "text":"(3b) Furthermore, in order to foster the development of artificial intelligence in line with Union values, the Union needs to address the main gaps and barriers blocking the potential of the digital transformation including the shortage of digitally skilled workers, cybersecurity concerns, lack of investment and access to investment, and existing and potential gaps between large companies and SMEs. Special attention should be paid to ensuring that the benefits of artificial intelligence and innovation in new technologies are felt across all regions of the Union and that sufficient investment and resources are provided especially to those regions that may be lagging behind in some digital indicators."
    },
    {
        "uuid":"a5f40f8e-23af-4c13-bde8-3fac8d6720fa",
        "amendment_number":3,
        "author":"ITRE",
        "title":"Recital 6",
        "text":"(6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand- alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list. (6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. This definition should be in line with definitions that have been accepted internationally. The definition should be based on the key functional characteristics of the AI system, in particular the ability, for a given set of human-defined objectives, to make predictions, recommendations, or decisions influencing real or virtual environments. More specifically, the definition of AI system should take into account key features such as the ability to perceive real and\/or virtual environments, to abstract such perceptions into models through analysis in an automated manner and to use model inference to formulate options for information or action. AI systems are designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list. While drafting these delegated acts, the Commission should insure the input of all relevant stakeholders including the technical experts and developers of AI systems. This consultation could take place through existing bodies such as the High Level Expert Group on AI or a newly established similar advisory body that is closely included in the work of the European Artificial Intelligence Board. Furthermore, the Commission should engage in dialogue with key international organisations such as the Organisation for Economic Cooperation and Development and other key organisations working on the definition of AI systems to ensure alignment between definitions of AI, while keeping the prerogative of the Union to set its own definition and standards through enacting legislation."
    },
    {
        "uuid":"421e9cc5-874f-433d-8dab-dc4806c65176",
        "amendment_number":4,
        "author":"ITRE",
        "title":"Recital 12 a (new)",
        "text":"(12a) This Regulation should not undermine research and development activity and should respect freedom of science. It is therefore necessary to ensure that this Regulation does not otherwise affect scientific research and development activity on AI systems. As regards product oriented research activity by providers, this Regulation should apply insofar as such research leads to or entails placing an AI system on the market or putting it into service. Under all circumstances, any research and development activity should be carried out in accordance with recognised ethical standards for scientific research."
    },
    {
        "uuid":"1e4c1290-313e-4950-b5a5-fb481ca87b20",
        "amendment_number":5,
        "author":"ITRE",
        "title":"Recital 29",
        "text":"(29) As regards high-risk AI systems that are safety components of products or (29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300\/2008 of the European Parliament and of the Council39 , Regulation (EU) No 167\/2013 of the European Parliament and of the Council40 , Regulation (EU) No 168\/2013 of the European Parliament and of the Council41 , Directive 2014\/90\/EU of the European Parliament and of the Council42 , Directive (EU) 2016\/797 of the European Parliament and of the Council43 , Regulation (EU) 2018\/858 of the European Parliament and of the Council44 , Regulation (EU) 2018\/1139 of the European Parliament and of the Council45 , and Regulation (EU) 2019\/2144 of the European Parliament and of the Council46 , it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts. systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300\/2008 of the European Parliament and of the Council39 , Regulation (EU) No 167\/2013 of the European Parliament and of the Council40 , Regulation (EU) No 168\/2013 of the European Parliament and of the Council41 , Directive 2014\/90\/EU of the European Parliament and of the Council42 , Directive (EU) 2016\/797 of the European Parliament and of the Council43 , Regulation (EU) 2018\/858 of the European Parliament and of the Council44 , Regulation (EU) 2018\/1139 of the European Parliament and of the Council45 , and Regulation (EU) 2019\/2144 of the European Parliament and of the Council46 , it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts. In addition, effective standardisation rules are needed to make the requirements of this Regulation operational. The Union’s institutions, in particular the Commission, should, together with enterprises, identify the AI sectors where there is the greatest need for standardisation, to avoid fragmentation of the market and maintain and further strengthen the integration of the European Standardisation System (ESS) within the International Standardisation System (ISO, IEC). __________________ __________________ 39 Regulation (EC) No 300\/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the 39 Regulation (EC) No 300\/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the European Parliament and of the Council and Council Regulation (EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1). European Parliament and of the Council and Council Regulation (EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1). 46 Regulation (EU) 2019\/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018\/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78\/2009, (EC) No 79\/2009 and (EC) No 661\/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631\/2009, (EU) No 406\/2010, (EU) No 672\/2010, (EU) No 1003\/2010, (EU) No 1005\/2010, (EU) No 1008\/2010, (EU) No 1009\/2010, (EU) No 19\/2011, (EU) No 109\/2011, (EU) No 458\/2011, (EU) No 65\/2012, (EU) No 130\/2012, (EU) No 347\/2012, (EU) No 351\/2012, (EU) No 1230\/2012 and (EU) 2015\/166 (OJ L 325, 16.12.2019, p. 1). 46 Regulation (EU) 2019\/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018\/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78\/2009, (EC) No 79\/2009 and (EC) No 661\/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631\/2009, (EU) No 406\/2010, (EU) No 672\/2010, (EU) No 1003\/2010, (EU) No 1005\/2010, (EU) No 1008\/2010, (EU) No 1009\/2010, (EU) No 19\/2011, (EU) No 109\/2011, (EU) No 458\/2011, (EU) No 65\/2012, (EU) No 130\/2012, (EU) No 347\/2012, (EU) No 351\/2012, (EU) No 1230\/2012 and (EU) 2015\/166 (OJ L 325, 16.12.2019, p. 1)."
    },
    {
        "uuid":"7d10692a-3ed1-418a-abb9-cca5d4776416",
        "amendment_number":6,
        "author":"ITRE",
        "title":"Recital 44",
        "text":"(44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing (44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high- risk AI systems. data sets are designed with the best possible efforts to ensure that they are relevant, representative, free of errors and appropriately vetted for errors in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used, with specific attention to the mitigation of possible biases in the datasets, that might lead to risks to fundamental rights or discriminatory outcomes for the persons affected by the high-risk AI system. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural, contextual or functional setting or context within which the AI system is intended to be used, with specific attention to women, vulnerable groups and children. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers should be able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high-risk AI systems."
    },
    {
        "uuid":"a9df45b1-4679-4f2e-ae6f-3d3ff01c484a",
        "amendment_number":7,
        "author":"ITRE",
        "title":"Recital 46",
        "text":"(46) Having information on how high- risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation. This requires keeping records and the availability of a technical documentation, containing information (46) Having comprehensible information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation and to allow users to make informed and autonomous decisions about their use. which is necessary to assess the compliance of the AI system with the relevant requirements. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk management system. The technical documentation should be kept up to date. This requires keeping records and the availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk management system. The technical documentation should be kept up to date."
    },
    {
        "uuid":"9d5f6a1b-3f2b-48d7-809e-45cc5beadc63",
        "amendment_number":8,
        "author":"ITRE",
        "title":"Recital 49",
        "text":"(49) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. The level of accuracy and accuracy metrics should be communicated to the users. (49) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. Accuracy metrics and their expected level should be defined with the primary objective to mitigate risks and negative impact of the AI system to individuals and the society as a whole. The expected level of accuracy and accuracy metrics should be communicated in a clear, transparent, easily understandable and intelligible way to the users. The declaration of accuracy metrics cannot however be considered proof of future levels but relevant methods need to be applied to ensure sustainable levels during use. While standardisation organisations exist to establish standards, coordination on benchmarking is needed to establish how these standards should be met and measured. The European Artificial Intelligence Board should bring together national metrology and benchmarking authorities and provide non-binding guidance to address the technical aspects of to how to measure the appropriate levels of accuracy and robustness."
    },
    {
        "uuid":"60dbab63-010f-43b6-be98-f52e07dfedc0",
        "amendment_number":9,
        "author":"ITRE",
        "title":"Recital 50",
        "text":"(50) The technical robustness is a key requirement for high-risk AI systems. They should be resilient against risks connected to the limitations of the system (e.g. errors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behaviour. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system. (50) The technical robustness is a key requirement for high-risk AI systems. They should be resilient against risks connected to the limitations of the system (e.g. errors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behaviour. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system. Users of the AI system should take steps to ensure that the possible trade-off between robustness and accuracy does not lead to discriminatory or negative outcomes for minority subgroups."
    },
    {
        "uuid":"2a9a78ca-01cd-4e6a-b92f-ff433f9ab793",
        "amendment_number":10,
        "author":"ITRE",
        "title":"Recital 51",
        "text":"(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data (51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure. poisoning) or trained models (e.g. adversarial attacks or confidentiality attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, as well as the notified bodies, competent national authorities and market surveillance authorities, also taking into account as appropriate the underlying ICT infrastructure. High-risk AI should be accompanied by security solutions and patches for the lifetime of the product, or in case of the absence of dependence on a specific product, for a time that needs to be stated by the manufacturer."
    },
    {
        "uuid":"67257874-cd12-4919-af1a-4325f39ebda7",
        "amendment_number":11,
        "author":"ITRE",
        "title":"Recital 61",
        "text":"(61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025\/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. However, the Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient. (61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025\/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. In addition to technical details, the standardisation process should also take into account risks to fundamental rights, the environment, and society as a whole and other democratic and sociotechnical aspects of the AI system, and should ensure that the relevant subject-matter experts are included and consulted in the standardisation process. The standardisation process should be transparent in terms of legal and natural persons participating in the standardisation activities. However, the Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient. In developing these common specifications Commission should involve views of relevant stakeholders, in particular when the common specifications address specific fundamental rights concerns. In particular, the Commission should adopt common specifications setting out how risk management systems give specific consideration to impact on children. __________________ __________________ 54 Regulation (EU) No 1025\/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89\/686\/EEC and 93\/15\/EEC and Directives 94\/9\/EC, 94\/25\/EC, 95\/16\/EC, 97\/23\/EC, 98\/34\/EC, 2004\/22\/EC, 2007\/23\/EC, 2009\/23\/EC and 2009\/105\/EC of the European Parliament and of the Council and repealing Council Decision 87\/95\/EEC and Decision No 1673\/2006\/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12). 54 Regulation (EU) No 1025\/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89\/686\/EEC and 93\/15\/EEC and Directives 94\/9\/EC, 94\/25\/EC, 95\/16\/EC, 97\/23\/EC, 98\/34\/EC, 2004\/22\/EC, 2007\/23\/EC, 2009\/23\/EC and 2009\/105\/EC of the European Parliament and of the Council and repealing Council Decision 87\/95\/EEC and Decision No 1673\/2006\/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12)."
    },
    {
        "uuid":"29fe36a3-8c43-4045-9c8c-d9bb441ca5bb",
        "amendment_number":12,
        "author":"ITRE",
        "title":"Recital 71",
        "text":"(71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member (71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate and ethically justified safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent States should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes and make such regulatory sandboxes widely available throughout the Union, in order to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. Any significant risks identified during the development and testing of AI systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place."
    },
    {
        "uuid":"5255763a-0191-4d32-b2c6-357b3a7d153f",
        "amendment_number":13,
        "author":"ITRE",
        "title":"Recital 72",
        "text":"(72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups. To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. This Regulation should provide the legal basis (72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation, as well as with the Charter of fundamental rights of the European Union and the General Data Protection Regulation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, to provide safeguards needed to build trust and reliance on AI systems and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups; to contribute to achieving the targets on AI as set in the Policy Programme “Path to the Digital Decade\"; to contribute to the development of for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016\/679, and Article 6 of Regulation (EU) 2018\/1725, and without prejudice to Article 4(2) of Directive (EU) 2016\/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016\/679 and Article 57 of Directive 2016\/680. ethical, socially responsible and environmentally sustainable AI systems; to permit effective participation of SMEs and start-ups in regulatory sandboxes, compliance costs should be kept to a reasonable level to ensure the development of trustworthy European artificial intelligence solutions; it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes, while encouraging innovation. This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016\/679, and Article 6 of Regulation (EU) 2018\/1725, and without prejudice to Article 4(2) of Directive (EU) 2016\/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016\/679 and Article 57 of Directive 2016\/680."
    },
    {
        "uuid":"d85d49b7-712e-455c-b150-49f6a6aef6b9",
        "amendment_number":14,
        "author":"ITRE",
        "title":"Recital 72 a (new)",
        "text":"(72a) It is desirable for the establishment of regulatory sandboxes, which is at present left to the discretion of Member States, as a next step to be made obligatory, with properly established criteria, to ensure both the effectiveness of the AI system and easier access for enterprises, in particular SMEs. Research enterprises and institutions should be involved in developing the conditions for the creation of regulatory sandboxes."
    },
    {
        "uuid":"9e119246-56a0-4088-bc28-aead070e4527",
        "amendment_number":15,
        "author":"ITRE",
        "title":"Recital 73",
        "text":"(73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users. (73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on AI literacy, awareness raising and information communication. Member States should utilise existing channels and where appropriate, establish new dedicated channels for communication with SMEs, start-ups, users and other innovators to provide guidance and respond to queries about the implementation of this Regulation. Such existing channels could include, inter alia, ENISA’s Computer Security Incident Response Teams, National Data Protection Agencies, the AI-on demand platform, the European Digital Innovation Hubs and other relevant instruments funded by EU programmes as well as the Testing and Experimentation Facilities established by the Commission and the Member States at national or Union level. Where appropriate, these channels should work together to create synergies and ensure homogeneity in their guidance to start- ups, SMEs and users. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. The Commission should regularly assess the certification and compliance costs for SMEs and start-ups, including through transparent consultations with SMEs, start-ups and users and work with Member States to lower such costs. For example, translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users. Medium-sized enterprises which recently changed from the small to medium-size category within the meaning of Recommendation 2003\/361\/EC should have access to these initiatives and guidance for a period of time deemed appropriate by the Member States, as these new medium-sized enterprises may sometimes lack the legal resources and training necessary to ensure proper understanding and compliance with provisions."
    },
    {
        "uuid":"0e204c03-891e-4f9b-84f7-d30cb4eb4d7b",
        "amendment_number":16,
        "author":"ITRE",
        "title":"Recital 76 a (new)",
        "text":"(76a) An AI advisory council (‘the Advisory Council’) should be established as a sub-group of the Board consisting of relevant representatives from industry, research, academia, civil society, standardisation organisations, social partners, SMEs, fundamental rights experts and other relevant stakeholders representing all Member States to maintain geographical balance. The Advisory Council should support the work of the Board by providing advice relating to the tasks of the Board. The Advisory Council should nominate a representative to attend meetings of the Board and to participate in its work."
    },
    {
        "uuid":"a8d9a9ea-ba79-44e9-bba1-4f53d31a0f84",
        "amendment_number":17,
        "author":"ITRE",
        "title":"Recital 81",
        "text":"(81) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of trustworthy artificial intelligence in the Union. Providers of non-high-risk AI systems should be encouraged to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems. Providers should also be encouraged to apply on a voluntary basis additional requirements related, for example, to environmental sustainability, accessibility to persons with disability, stakeholders’ participation in the design and development of AI systems, and diversity of the development teams. The Commission may develop initiatives, including of a sectorial nature, to facilitate the lowering of technical barriers hindering cross-border exchange of data for AI development, including on data access infrastructure, semantic and technical interoperability of different types of data. (81) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of trustworthy, socially responsible and environmentally sustainable artificial intelligence in the Union. Providers of non- high-risk AI systems should be encouraged to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems. Providers should also be encouraged to apply on a voluntary basis additional requirements related, for example, to environmental sustainability, accessibility to persons with disability, stakeholders’ participation in the design and development of AI systems, and diversity of the development teams. The Commission may develop initiatives, including of a sectorial nature, to facilitate the lowering of technical barriers hindering cross-border exchange of data for AI development, including on data access infrastructure, semantic and technical interoperability of different types of data."
    },
    {
        "uuid":"ff2b9220-724c-409d-9a44-2814c464a52c",
        "amendment_number":18,
        "author":"ITRE",
        "title":"Article 2 – paragraph 5 a (new)",
        "text":"5a. This Regulation shall not affect research activities regarding AI systems insofar as such activities do not lead to or entail placing an AI system on the market or putting it into service. These research activities shall not violate the fundamental rights of the affected persons."
    },
    {
        "uuid":"3a69267a-f385-4466-93fe-eea99227cacc",
        "amendment_number":19,
        "author":"ITRE",
        "title":"Article 2 – paragraph 5 b (new)",
        "text":"5b. This Regulation shall not apply to AI systems, including their output, specifically developed and put into service for the sole purpose of scientific research in the general interest of the Union."
    },
    {
        "uuid":"191a269f-4533-45b3-ae21-c3b30b19ba45",
        "amendment_number":20,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 1",
        "text":"(1) ‘artificial intelligence system’ (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with; (1) ‘artificial intelligence system’ (AI system) means a machine-based system that can, with varying levels of autonomy, for a given set of human-defined objectives, make predictions, content, recommendations, or decisions influencing real or virtual environments they interact with;"
    },
    {
        "uuid":"d82d2bda-e8db-48ad-b057-9eb390866390",
        "amendment_number":21,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 1 a (new)",
        "text":"(1a) ‘autonomy’ means that an AI system operates by interpreting certain input and by using a set of pre-determined objectives, without being limited to such instructions, despite the system’s behaviour being constrained by, and targeted at, fulfilling the goal it was given and other relevant design choices made by its developer;"
    },
    {
        "uuid":"26cab06c-92cd-47ea-8f6a-f63f6fff1468",
        "amendment_number":22,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 2",
        "text":"(1) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge; (2) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed and places that system on the market or puts it into service under its own name or trademark, whether for payment or free of charge;"
    },
    {
        "uuid":"18c51a96-3768-48e9-98a2-92f49266b8a3",
        "amendment_number":23,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 14",
        "text":"(14) ‘safety component of a product or system’ means a component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property; (14) ‘safety component of a product or system’ means a component of a product or of a system which fulfils a safety function for that product or system and the failure or malfunctioning of which endangers the health and safety of persons or property;"
    },
    {
        "uuid":"0a51a6a5-53b2-4a9e-9c82-3188fcb5cc98",
        "amendment_number":24,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 – introductory part",
        "text":"(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led or might lead to any of the following: (44) ‘serious incident’ means any incident or malfunctioning of an AI system that directly or indirectly leads, might have led or might lead to any of the following:"
    },
    {
        "uuid":"28f9258e-019a-4d15-a3a9-672e1206403d",
        "amendment_number":25,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 – point a",
        "text":"(a) the death of a person or serious damage to a person’s health, to property or the environment, (a) the death of a person or serious damage to a person’s fundamental rights, health, safety, property or the environment,"
    },
    {
        "uuid":"52ed20c4-4a45-49a1-b62d-d31d8de96db6",
        "amendment_number":26,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 – point b a (new)",
        "text":"(ba) breach of obligations under Union law intended to protect fundamental rights."
    },
    {
        "uuid":"487eca40-b4d0-4623-b9d3-8dcc57931059",
        "amendment_number":27,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 a (new)",
        "text":"(44a) ‘regulatory sandbox’ means a facility established by one or more Member States’ competent authorities in collaboration with the Commission or by the European Data Protection Supervisor, that provides an appropriate controlled and flexible environment to facilitate the safe development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan;"
    },
    {
        "uuid":"616a593d-709b-41ce-b2bf-1cc1539dcc22",
        "amendment_number":28,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 b (new)",
        "text":"(44b) ‘AI literacy’ means the skills, knowledge and understanding regarding AI systems that are necessary for compliance with and enforcement of this Regulation;"
    },
    {
        "uuid":"229f0637-2dc0-4a33-9c94-07f9845ea7eb",
        "amendment_number":29,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 c (new)",
        "text":"(44c) ‘deep fake’ means manipulated or synthetic audio and\/or visual material that gives an authentic impression, in which events appear to be taking place, which never happened, and which has been produced using techniques in the field of artificial intelligence, including machine learning and deep learning, without the user, or end-user being aware that the audio and\/or visual material has been produced using artificial intelligence;"
    },
    {
        "uuid":"10e889d3-f2cd-43c1-a5f0-ae1bd95d9112",
        "amendment_number":30,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 d (new)",
        "text":"(44d) ‘critical infrastructure’ means an asset, system or part thereof which is necessary for the delivery of a service that is essential for the maintenance of vital societal functions or economic activities within the meaning of Article 2(4) and (5) of Directive ____ on the resilience of critical entities (2020\/0365(COD));"
    },
    {
        "uuid":"eac42f0c-5222-476c-a098-1edd7e49cec6",
        "amendment_number":31,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 e (new)",
        "text":"(44e) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation (EU) 2016\/679;"
    },
    {
        "uuid":"d51b7303-b649-4075-9896-9ca236895b09",
        "amendment_number":32,
        "author":"ITRE",
        "title":"Article 3 – paragraph 1 – point 44 f (new)",
        "text":"(44f) ‘non personal data’ means data other than personal data as defined in point (1) of Article 4 of Regulation (EU) 2016\/679."
    },
    {
        "uuid":"6781f799-b851-4f8e-a826-21a2e6a5a6c8",
        "amendment_number":33,
        "author":"ITRE",
        "title":"Article 4 – paragraph 1",
        "text":"The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend the list of techniques and approaches listed in Annex I, in order to update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein. approaches listed in Annex I within the scope of the AI system as defined in Article 3, point (1), in order to update that list to market and technological developments on the basis of characteristics that are similar to the techniques and approaches listed therein."
    },
    {
        "uuid":"5e384654-2de5-4be3-8832-a6bbb1078bd1",
        "amendment_number":34,
        "author":"ITRE",
        "title":"Article 4 – paragraph 1 a (new)",
        "text":"When drafting these delegated acts, the Commission shall ensure the input of all relevant stakeholders such as technical experts and developers of AI systems."
    },
    {
        "uuid":"5603b2c2-3012-4265-bf74-be9e898be998",
        "amendment_number":35,
        "author":"ITRE",
        "title":"Article 10 – paragraph 1",
        "text":"1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5. 1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, assessment, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 as far as this is feasible from a technical point of view while taking into account the latest state-of-the-art measures, according to the specific market segment or scope of application."
    },
    {
        "uuid":"6b23c9a7-713f-4a47-b72d-407a69eb72f3",
        "amendment_number":36,
        "author":"ITRE",
        "title":"Article 10 – paragraph 1 a (new)",
        "text":"1a. Techniques such as unsupervised learning and reinforcement learning, that do not use validation and testing data sets, shall be developed on the basis of training data sets that meet the quality criteria referred to in paragraphs 2 to 5."
    },
    {
        "uuid":"fab5f909-e51b-4a83-bcad-8e2f806a20dd",
        "amendment_number":37,
        "author":"ITRE",
        "title":"Article 10 – paragraph 1 b (new)",
        "text":"1b. Providers of high-risk AI systems that utilise data collected and\/or managed by third parties may rely on representations from those third parties with regard to quality criteria referred to in paragraph 2, points (a), (b) and (c)"
    },
    {
        "uuid":"393cd4c4-ad70-4370-b2ef-586a274a214f",
        "amendment_number":38,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – introductory part",
        "text":"2. Training, validation and testing data sets shall be subject to appropriate data governance and management practices. Those practices shall concern in particular, 2. Training, assessment, validation and testing data sets shall be subject to appropriate data governance and management practices for the entire lifecycle of data processing. Those practices shall concern in particular, the following elements:"
    },
    {
        "uuid":"fc2842dc-8de3-494b-87c7-579bb2ba57d2",
        "amendment_number":39,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – point a a (new)",
        "text":"(aa) transparency as regards the original purpose of data collection;"
    },
    {
        "uuid":"6e201b8c-bbde-43cb-9446-3315535c28ea",
        "amendment_number":40,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – point b",
        "text":"(b) data collection; (b) data collection processes;"
    },
    {
        "uuid":"b8588b4f-c274-4eb3-85c4-53f7a93874cb",
        "amendment_number":41,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – point f",
        "text":"(f) examination in view of possible biases; (f) examination in view of possible biases that are likely to affect health and safety of persons, negatively impact fundamental rights or lead to discrimination prohibited by Union law; including the cases where data outputs are used as an input for future operations (‘feedback loops’);"
    },
    {
        "uuid":"c29dcb26-b33f-497c-a1d3-89f10aa24be0",
        "amendment_number":42,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – point g",
        "text":"(g) the identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed. (g) the identification of possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed;"
    },
    {
        "uuid":"1110764e-5736-4b1b-b7e1-4cfb8cb3f3d4",
        "amendment_number":43,
        "author":"ITRE",
        "title":"Article 10 – paragraph 2 – point g a (new)",
        "text":"(ga) the purpose and the environment in which the system is to be used."
    },
    {
        "uuid":"8fea504d-27bb-4717-a83b-a09027dd2dc2",
        "amendment_number":44,
        "author":"ITRE",
        "title":"Article 10 – paragraph 3",
        "text":"3. Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof. 3. Training, validation and testing datasets are designed with the best possible efforts to ensure that they are relevant, representative and appropriately vetted for errors in view of the intended purpose of the AI system. In particular, they shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof."
    },
    {
        "uuid":"e14069a2-f941-4bbd-b686-186e5bdcfa27",
        "amendment_number":45,
        "author":"ITRE",
        "title":"Article 10 – paragraph 4",
        "text":"4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used. 4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural, contextual or functional setting within which the high-risk AI system is intended to be used."
    },
    {
        "uuid":"406320cd-cc98-402e-addb-ed2bee3c4b2a",
        "amendment_number":46,
        "author":"ITRE",
        "title":"Article 15 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way that they achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness and cybersecurity, and perform consistently in those respects throughout their lifecycle. 1. High-risk AI systems shall be designed and developed following the principle of security by design and by default. In the light of their intended purpose, they should achieve an appropriate level of accuracy, robustness, safety, and cybersecurity, and perform consistently in those respects throughout their lifecycle. Compliance with these requirements shall include implementation of state-of-the-art measures, according to the specific market segment or scope of application."
    },
    {
        "uuid":"4a386c15-ab44-4c3d-a4e7-7b4639179823",
        "amendment_number":47,
        "author":"ITRE",
        "title":"Article 15 – paragraph 1 a (new)",
        "text":"1a. To address the technical aspects of to how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 of this Article, the European Artificial Intelligence Board shall bring together national metrology and benchmarking authorities and provide non-binding guidance on the matter as set out in Article 56, paragraph 2, point (a)."
    },
    {
        "uuid":"f19a1b8e-45b6-4451-8f29-905638fdb1fc",
        "amendment_number":48,
        "author":"ITRE",
        "title":"Article 15 – paragraph 1 b (new)",
        "text":"1b. To address any emerging issues across the internal market with regard to cybersecurity, the European Union Agency for Cybersecurity (ENISA) shall be involved alongside the European Artificial Intelligence Board as set out Article 56, paragraph 2, point (b)."
    },
    {
        "uuid":"2ace1cd8-bfb4-474d-9ac7-99ac5fe1aab6",
        "amendment_number":49,
        "author":"ITRE",
        "title":"Article 15 – paragraph 2",
        "text":"2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use. 2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use. The language used shall be clear, free of misunderstandings or misleading statements."
    },
    {
        "uuid":"cc3ff85a-798c-4768-b7c0-87b7b76e8e78",
        "amendment_number":50,
        "author":"ITRE",
        "title":"Article 15 – paragraph 3 – subparagraph 1",
        "text":"High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Technical and organisational measures shall be taken to ensure that high-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems."
    },
    {
        "uuid":"c4a128ed-17b2-4a6a-a4a9-52ba73031359",
        "amendment_number":51,
        "author":"ITRE",
        "title":"Article 15 – paragraph 3 – subparagraph 2",
        "text":"The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans. The robustness of high-risk AI systems may be achieved by the appropriate provider with input from the user, where necessary, through technical redundancy solutions, which may include backup or fail-safe plans."
    },
    {
        "uuid":"f05407bb-add2-4766-89ae-36a712b47d07",
        "amendment_number":52,
        "author":"ITRE",
        "title":"Article 15 – paragraph 3 – subparagraph 3",
        "text":"High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures. High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs influencing input for future operations (‘feedback loops’) and malicious manipulation of inputs used in learning during operation are duly addressed with appropriate mitigation measures."
    },
    {
        "uuid":"d2809fdf-353c-4268-99a6-e3bf6a48704c",
        "amendment_number":53,
        "author":"ITRE",
        "title":"Article 15 – paragraph 4 – subparagraph 1",
        "text":"High-risk AI systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities. High-risk AI systems shall be resilient as regards to attempts by unauthorised third parties to alter their use, behaviour, outputs or performance by exploiting the system vulnerabilities."
    },
    {
        "uuid":"a1faf626-9ba5-40b2-b67f-d5187aa4b7d5",
        "amendment_number":54,
        "author":"ITRE",
        "title":"Article 15 – paragraph 4 – subparagraph 3",
        "text":"The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent and control for attacks trying to manipulate the training dataset (‘data poisoning’), inputs designed to cause the model to make a mistake (‘adversarial examples’), or model flaws. The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training dataset (‘data poisoning’), or pre-trained components used in training (‘model poisoning’) , inputs designed to cause the model to make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws, which could lead to harmful decision- making."
    },
    {
        "uuid":"1c689a2d-f8cf-4d8f-b012-4114ce509aaf",
        "amendment_number":55,
        "author":"ITRE",
        "title":"Article 40 – paragraph 1 a (new)",
        "text":"The Commission shall ensure that the process of developing harmonised standards takes into account risks to fundamental rights, environment and society as a whole."
    },
    {
        "uuid":"97eef0c9-630d-483d-92e5-1b45d995772c",
        "amendment_number":56,
        "author":"ITRE",
        "title":"Article 40 – paragraph 1 b (new)",
        "text":"The Commission shall ensure that the process of developing harmonised standards on AI systems is open to stakeholders, including SMEs in accordance with Articles 5 and 6 of Regulation (EU) No 1025\/2012."
    },
    {
        "uuid":"6bc90a13-a567-42e1-b13e-68cbc3b46852",
        "amendment_number":57,
        "author":"ITRE",
        "title":"Article 40 – paragraph 1 c (new)",
        "text":"To this end the Commission shall direct funds in accordance with Article 17 of Regulation (EU) No 1025\/2012 to facilitate their effective participation."
    },
    {
        "uuid":"c9c16695-6c73-4e16-b750-48709f47f314",
        "amendment_number":58,
        "author":"ITRE",
        "title":"Article 40 – paragraph 1 d (new)",
        "text":"The Commission shall review the harmonised standards before their publication in the Official Journal and prepare a report outlining their adequacy with paragraphs 1a and 1b of this Article."
    },
    {
        "uuid":"08297af0-8c9a-4ec0-a6b4-91a9d19ef257",
        "amendment_number":59,
        "author":"ITRE",
        "title":"Article 41 – paragraph 1",
        "text":"1. Where harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2). 1. Where harmonised standards referred to in Article 40 do not exist or where the Commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific and pressing safety or fundamental right concern that cannot be sufficiently settled by development of harmonised standards, the Commission may, by means of implementing acts, adopt common specifications in respect of the requirements set out in Chapter 2 of this Title. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 74(2)."
    },
    {
        "uuid":"6fc4ab41-171c-4369-92b1-85611810dfce",
        "amendment_number":60,
        "author":"ITRE",
        "title":"Article 41 – paragraph 2",
        "text":"2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law. 2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of developers and providers of High-risk AI systems as well as relevant stakeholders, such as SME's and start-ups, civil society and social partners or expert groups established under relevant sectorial Union law."
    },
    {
        "uuid":"2ad62765-12e3-4eaf-b687-97d7f4d756c1",
        "amendment_number":61,
        "author":"ITRE",
        "title":"Article 42 – paragraph 1",
        "text":"1. Taking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in Article 10(4). 1. Taking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural, contextual and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in Article 10(4)."
    },
    {
        "uuid":"534bb675-8ea5-4e5a-9a6c-b7553550e9cd",
        "amendment_number":62,
        "author":"ITRE",
        "title":"Article 42 – paragraph 2",
        "text":"2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a 2. High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019\/881 of the European Parliament and of the Council63 and the references of which have been published in the Official Journal of the European Union shall be presumed to be in compliance with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements. cybersecurity scheme pursuant to Regulation (EU) 2019\/881 of the European Parliament and of the Council63 and the references of which have been published in the Official Journal of the European Union shall be presumed to be in compliance with the cybersecurity requirements set out in Article 15 of this Regulation, where applicable, in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements. __________________ __________________ 63 Regulation (EU) 2019\/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526\/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 1). 63 Regulation (EU) 2019\/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526\/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 1)."
    },
    {
        "uuid":"aeae2b5a-e4f9-41d9-a424-e8b51a735d3b",
        "amendment_number":63,
        "author":"ITRE",
        "title":"Article 43 – paragraph 1 – subparagraph 1 a (new)",
        "text":"Should the provider already have established internal organisation and structures for existing conformity assessments or requirements under other existing rules, the provider may utilise those, or parts of those, existing compliance structures, so long as they also have the capacity and competence needed to fulfil the requirements for the product set out in this Regulation."
    },
    {
        "uuid":"f558169d-a213-4d11-8c57-5cf170ebfbee",
        "amendment_number":64,
        "author":"ITRE",
        "title":"Article 43 – paragraph 5",
        "text":"5. The Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress. 5. The Commission is empowered to adopt delegated acts in accordance with Article 73 for the purpose of updating Annexes VI and Annex VII in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress. The Commission shall consult the European Artificial Intelligence Board established in Article 56 as well as all relevant stakeholders."
    },
    {
        "uuid":"b5793280-3ceb-4c68-bc63-394c5d5f4826",
        "amendment_number":65,
        "author":"ITRE",
        "title":"Article 43 – paragraph 6",
        "text":"6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies. 6. The Commission is empowered to adopt delegated acts to amend paragraphs 1 and 2 in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies. The Commission shall consult the European Artificial Intelligence Board established in Article 56 as well as all relevant stakeholders."
    },
    {
        "uuid":"b3871bec-83ca-4076-838f-2945c2b9b42d",
        "amendment_number":66,
        "author":"ITRE",
        "title":"Article 44 – paragraph 1",
        "text":"1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in an official Union language determined by the Member State in which the notified body is established or in an official Union language otherwise acceptable to the notified body. 1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in one or several official languages determined by the Member State in which the notified body is established or in one or several official languages otherwise acceptable to the notified body."
    },
    {
        "uuid":"114bd086-d8e8-441a-b330-bcb365e61934",
        "amendment_number":67,
        "author":"ITRE",
        "title":"Article 48 – paragraph 1",
        "text":"1. The provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request. 1. The provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be given to the relevant national competent authority in the Member State of main establishment of the provider, upon that competent authority’s request."
    },
    {
        "uuid":"be7fb447-b7bc-41c2-82ff-76ba246e6b87",
        "amendment_number":68,
        "author":"ITRE",
        "title":"Article 51 – paragraph 1 a (new)",
        "text":"A high-risk AI system designed, developed, trained, validate, tested or approved to be placed on the market or put into service, outside the Union, can be registered in the EU database referred to in Article 60 and placed on the market or put into service in the Union only if it is proven that at all the stages of its design, development, training, validation, testing or approval, all the obligations required from such AI systems in the Union have been met."
    },
    {
        "uuid":"2c368443-6d61-4232-91ab-1b5d5c227e3f",
        "amendment_number":69,
        "author":"ITRE",
        "title":"Article 51 – paragraph 1 b (new)",
        "text":"Before using a high-risk AI system referred to in Article 6(2) the user or where applicable the authorised representative shall register the uses of that system in the EU database referred to in the Article 60. A new registration entry shall be complemented by the user for each high risk use of the AI system."
    },
    {
        "uuid":"aed616cb-200a-4ea7-b781-9ce7812bfafb",
        "amendment_number":70,
        "author":"ITRE",
        "title":"Article 53 – paragraph 1",
        "text":"1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation 1. AI regulatory sandboxes established by one or more Member States competent authorities in collaboration with the Commission, or the European Data Protection Supervisor shall provide a controlled environment that facilitates the safe development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan, SMEs, start-ups, enterprises, innovators or other relevant actors could be included as partners in the regulatory sandboxes. This shall take place under the direct supervision and guidance of the supervised within the sandbox. Commission in collaboration with the competent authorities with a view to identifying risks, in particular to health, safety, and fundamental rights, and ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox. The Commission shall play a complementary role, allowing those Member States with demonstrated experience with sandboxing to build on their expertise and, on the other hand, assisting and providing technical understanding and resources to those Member States that seek guidance on the set-up and running of these regulatory sandboxes."
    },
    {
        "uuid":"5a916644-8d0c-43b1-b1c5-a2cbdb06a278",
        "amendment_number":71,
        "author":"ITRE",
        "title":"Article 53 – paragraph 2",
        "text":"2. Member States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox. 2. Member States, in collaboration with the Commission, shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox."
    },
    {
        "uuid":"a4db4f3e-e84f-4d1b-a3f2-a69b50afb9f3",
        "amendment_number":72,
        "author":"ITRE",
        "title":"Article 53 – paragraph 3",
        "text":"3. The AI regulatory sandboxes shall not affect the supervisory and corrective 3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place. powers of the competent authorities, including at regional or local level. Any significant risks to health and safety, fundamental rights, democracy or the environment, identified during the development and testing of AI systems, shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place."
    },
    {
        "uuid":"365ac7d9-7fbe-41ef-93f2-4a794fe07be7",
        "amendment_number":73,
        "author":"ITRE",
        "title":"Article 53 – paragraph 5",
        "text":"5. Member States’ competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board. They shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. 5. Member States’ competent authorities and the Commission shall coordinate their activities with regard to AI regulatory sandboxes and cooperate within the framework of the European Artificial Intelligence Board. The Commission shall submit annual reports to the European Artificial Intelligence Board on the results from the implementation of those schemes, including best practices, computational energy use and efficiency, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. SMEs, start-ups, enterprises and other innovators shall be invited to share their good practices, lessons learnt and recommendations on their AI sandboxes with Member States’ competent authorities."
    },
    {
        "uuid":"fe0b5045-e41b-4709-8544-2195246b93d3",
        "amendment_number":74,
        "author":"ITRE",
        "title":"Article 53 – paragraph 6 a (new)",
        "text":"6a. The Commission shall establish an EU AI Regulatory Sandboxing Work Programme whose modalities referred to in Article 53(6) shall cover the elements set out in Annex IXa. The Commission shall proactively coordinate with national and local authorities, where relevant."
    },
    {
        "uuid":"2d210f09-46ed-45d2-96d1-a425f3cc312e",
        "amendment_number":75,
        "author":"ITRE",
        "title":"Article 55 – title",
        "text":"Measures for small-scale providers and users Measures for SMEs, start-ups and users"
    },
    {
        "uuid":"8a75d730-85f4-461f-b6ca-623f6473e2ac",
        "amendment_number":76,
        "author":"ITRE",
        "title":"Article 55 – paragraph 1 – point a",
        "text":"(a) provide small-scale providers and start-ups with priority access to the AI regulatory sandboxes to the extent that they fulfil the eligibility conditions; (a) provide SMEs and start-ups, established in the Union, with priority access to the AI regulatory sandboxes, to the extent that they fulfil the eligibility conditions;"
    },
    {
        "uuid":"e8d03ad7-637d-4183-9ab3-9753f98fb5d5",
        "amendment_number":77,
        "author":"ITRE",
        "title":"Article 55 – paragraph 1 – point b",
        "text":"(b) organise specific awareness raising activities about the application of this Regulation tailored to the needs of the small-scale providers and users; (b) organise specific awareness raising and enhanced digital skills development activities on the application of this Regulation tailored to the needs of SMEs, start-ups and users;"
    },
    {
        "uuid":"7cbc5388-987e-4b41-8545-2b3a7a5d8b2c",
        "amendment_number":78,
        "author":"ITRE",
        "title":"Article 55 – paragraph 1 – point c",
        "text":"(c) where appropriate, establish a dedicated channel for communication with small-scale providers and user and other innovators to provide guidance and respond to queries about the implementation of this Regulation. (c) utilise existing dedicated channels and where appropriate, establish new dedicated channels for communication with SMEs, start-ups, users and other innovators to provide guidance and respond to queries about the implementation of this Regulation;"
    },
    {
        "uuid":"b0ac91d2-cd82-4ee7-aee4-3620ea4566fe",
        "amendment_number":79,
        "author":"ITRE",
        "title":"Article 55 – paragraph 1 – point c a (new)",
        "text":"(ca) foster the participation of SMEs and other relevant stakeholders in the standardisation development process."
    },
    {
        "uuid":"7c622009-df35-4721-b135-5c596fafd832",
        "amendment_number":80,
        "author":"ITRE",
        "title":"Article 55 – paragraph 2",
        "text":"2. The specific interests and needs of the small-scale providers shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size and market size. 2. The specific interests and needs of the SMEs, start-ups and users shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to development stage, their size, market size and market demand. The Commission shall regularly assess the certification and compliance costs for SMEs and start-ups, including through transparent consultations with SMEs, start-ups and users and shall work with Member States to lower such costs where possible. The Commission shall report on these findings to the European Parliament and to the Council as part of the report on the evaluation and review of this Regulation provided for in Article 84(2)."
    },
    {
        "uuid":"b13152a7-ddd3-4ad9-807f-b5d9c423d3c4",
        "amendment_number":81,
        "author":"ITRE",
        "title":"Article 57 – paragraph 3 a (new)",
        "text":"3a. The Board shall establish an AI Advisory Council (Advisory Council). The Advisory Council shall be composed of relevant representatives from industry, research, academia, civil society, standardisation organisations, and other relevant stakeholders or third parties appointed by the Board, representing all Member States to maintain geographical balance. The Advisory Council shall support the work of the Board by providing advice relating to the tasks of the Board. The Advisory Council shall nominate a relevant representative, depending on the configuration in which the Board meets, to attend meetings of the Board and to participate in its work. The composition of the Advisory Council and its recommendations to the Board shall be made public."
    },
    {
        "uuid":"56a32204-1035-45ec-b216-457673067f29",
        "amendment_number":1,
        "author":"CULT",
        "title":"Recital 1",
        "text":"(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI- based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation. (1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework, based on ethical principles in particular for the development, marketing and use of artificial intelligence in conformity with Union values, minimising any risk of adverse and discriminatory impact on people and without hindering innovation. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, democracy, the rule of law and the environment, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation."
    },
    {
        "uuid":"200ba94a-125e-45c4-82c7-b821609558d1",
        "amendment_number":2,
        "author":"CULT",
        "title":"Recital 2",
        "text":"(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of protection (2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is trustworthy and safe and is developed and used in compliance with fundamental rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board. protection throughout the Union should therefore be ensured in order to achieve trustworthy AI, while divergences hampering the free circulation, innovation, deployment and uptake of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board."
    },
    {
        "uuid":"50ccb6e9-6e50-47f6-a63e-21212be8d101",
        "amendment_number":3,
        "author":"CULT",
        "title":"Recital 3",
        "text":"(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in (3) Artificial intelligence is a fast evolving family of technologies that can contribute and is already contributing to a wide array of economic and societal benefits across the entire spectrum of industries and social activities, if developed in accordance with ethical principles. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation. support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, media, sports, culture infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation."
    },
    {
        "uuid":"984e204f-77db-4125-b818-3345e8629105",
        "amendment_number":4,
        "author":"CULT",
        "title":"Recital 4",
        "text":"(4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial. (4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights, that are protected by Union law including fundamental rights of workers, people in learning processes and socially engaged people, privacy, data protection and informational self-determination, societal or environmental rights. Such harm might be material or immaterial."
    },
    {
        "uuid":"cfd75311-63f2-4638-a9ec-d465b5197844",
        "amendment_number":5,
        "author":"CULT",
        "title":"Recital 5",
        "text":"(5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that (5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law, democracy, the objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33 , and it ensures the protection of ethical principles, as specifically requested by the European Parliament34 . rule of law and of the environment. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence based on fundamental rights, as stated by the European Council33, and it ensures the protection of ethical principles, as specifically requested by the European Parliament34, with a human-centric approach and in compliance with freedom of expression, freedom of speech, media freedom, pluralism and diversity. __________________ __________________ 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL). 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL)."
    },
    {
        "uuid":"aba8b259-7e9f-4b64-811b-571cd2d19d92",
        "amendment_number":6,
        "author":"CULT",
        "title":"Recital 5 a (new)",
        "text":"(5a) In order to help promote the development, uptake and understanding of AI, the Union needs to put further effort into education and training, thus, inter alia, addressing the shortage of ICT professionals and AI undergraduate courses, digitally skilled workers as well as lack of even basic digital skills amongst a significant share of the population of the Union."
    },
    {
        "uuid":"2ca5d8bd-d767-4100-869f-1dedd6b97041",
        "amendment_number":7,
        "author":"CULT",
        "title":"Recital 5 b (new)",
        "text":"(5b) Lack of both public and private investment is currently undermining development and use of AI systems across the Union, especially when compared to other major industrial economies. Special attention, incentives and support should be devised to promote AI uptake amongst SMEs, including those in education and cultural and creative sectors and industries."
    },
    {
        "uuid":"f5073be4-480d-40fd-accf-eac13709d476",
        "amendment_number":8,
        "author":"CULT",
        "title":"Recital 9",
        "text":"(9) For the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned. Therefore, the notion does not cover places that are private in nature and normally not freely accessible for third parties, including law enforcement authorities, unless those parties have been specifically invited or authorised, such as homes, private clubs, offices, warehouses and factories. Online spaces are not covered either, as they are not physical spaces. However, the mere fact that certain conditions for accessing a (9) For the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical or virtual place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned. Therefore, the notion does not cover places that are private in nature and normally not freely accessible for third parties, including law enforcement authorities, unless those parties have been specifically invited or authorised, such as homes, private clubs, offices, warehouses and factories, and other private spaces. The same principle should apply to protected virtual publicly accessible particular space may apply, such as admission tickets or age restrictions, does not mean that the space is not publicly accessible within the meaning of this Regulation. Consequently, in addition to public spaces such as streets, relevant parts of government buildings and most transport infrastructure, spaces such as cinemas, theatres, shops and shopping centres are normally also publicly accessible. Whether a given space is accessible to the public should however be determined on a case-by-case basis, having regard to the specificities of the individual situation at hand. spaces. However, the mere fact that certain conditions for accessing a particular space may apply, such as admission tickets or age restrictions, does not mean that the space is not publicly accessible within the meaning of this Regulation. Consequently, in addition to public spaces such as streets, parks, sport complexes relevant parts of government buildings and most transport infrastructure, spaces such as cinemas, theatres, shops, museums, libraries monuments, cultural sites, cultural institutions and shopping centres are normally also publicly accessible. Whether a given space is accessible to the public should however be determined on a case- by-case basis, having regard to the specificities of the individual situation at hand."
    },
    {
        "uuid":"126981e9-c716-47ef-b063-48db955f7f85",
        "amendment_number":9,
        "author":"CULT",
        "title":"Recital 13",
        "text":"(13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments. (13) In order to ensure a consistent and high level of protection of public interests as regards health, safety, fundamental rights, democracy, the rule of law, as well as the environment, a set of ethical principles and common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter), the communication of the Commission of 11 December 2019 entitled ‘The European Green Deal’ and the European Declaration of 26 January 2022 on Digital Rights and Principles for the Digital Decade and should be non- discriminatory and in line with the Union’s international trade commitments."
    },
    {
        "uuid":"f2b2ce31-288f-486d-8c25-db35fcd6e9e6",
        "amendment_number":10,
        "author":"CULT",
        "title":"Recital 14 b (new)",
        "text":"(14b) AI literacy refers to skills, knowledge and understanding that allows both citizens and operators in the context of the obligations set out in this Regulation to make an informed deployment and use of AI systems, as well as to gain awareness about the opportunities and risks of AI and thereby promote its democratic control. AI literacy should not be limited to learning about tools and technologies, but should also aim to equip citizens and operators in the context of the obligations set out in this Regulation with the critical thinking skills required to identify harmful or manipulative uses, as well as to improve their agency and their ability to fully comply with and benefit from trustworthy AI. It is therefore necessary that the Commission, the Member States and operators of AI systems, in cooperation with all relevant stakeholders, promote the development of AI literacy, in all sectors of society, for citizens of all ages, including women and girls, and that progress in that regard is closely followed."
    },
    {
        "uuid":"615a9ce7-62ca-45d9-a35d-44837ccafd87",
        "amendment_number":11,
        "author":"CULT",
        "title":"Recital 15",
        "text":"(15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are (15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child. particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, the protection of employees and workers, data protection and privacy and gender equality and the rights of the child."
    },
    {
        "uuid":"dce426ee-5eb9-4327-9323-9b95e19ad51c",
        "amendment_number":12,
        "author":"CULT",
        "title":"Recital 27",
        "text":"(27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any. (27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a harmful impact on the health, safety and fundamental rights of persons in the Union, as well as on society and on the environment, and such limitation minimises any potential restriction to international trade, if any."
    },
    {
        "uuid":"4ffe5150-5c7e-436c-8cb4-1032cb56a1a1",
        "amendment_number":13,
        "author":"CULT",
        "title":"Recital 28",
        "text":"(28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with (28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non- discrimination, consumer protection, workers’ rights, rights of persons with disabilities, right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non- discrimination, right to education, consumer protection, workers’ rights. Special attention should be paid to gender equality, rights of persons with disabilities, right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration, protection of intellectual property rights and ensuring cultural diversity. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons. well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the harm that an AI system can cause, including in relation to the health and safety of persons or to the environment, taking into account the extraction and consumption of natural resources, waste and the carbon footprint of those AI systems."
    },
    {
        "uuid":"0c644ba3-9212-4b8b-b3ce-b1277a0a8000",
        "amendment_number":14,
        "author":"CULT",
        "title":"Recital 33",
        "text":"(33) Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. This is particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, ‘real-time’ and ‘post’ remote biometric identification systems should be classified as high-risk. In view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight. (33) Technical inaccuracies of AI systems intended for the biometric identification of natural persons can lead to biased results and entail discriminatory effects. This is particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, ‘real-time’ and ‘post’ remote biometric identification systems should be classified as high-risk. In view of the risks that they pose, both types of biometric identification systems should be subject to specific requirements on logging capabilities and human oversight. Non- remote biometric identification systems intended to be used in publicly accessible spaces, workplaces and education and training institutions can also present a high risk. The high risk of non-remote biometric identification systems intended to be used in publicly accessible spaces, workplaces and education and training institutions should be determined on a case-by-case basis."
    },
    {
        "uuid":"a10da396-a099-435f-9a03-956bb7ec9fde",
        "amendment_number":15,
        "author":"CULT",
        "title":"Recital 34 a (new)",
        "text":"(34a) Deployment of AI systems in education is crucial in order to help modernise entire education systems, to increase educational quality, both offline and online, and to accelerate digital education, thus also making it available to a broader audience. AI-aided digital education, whilst not a replacement for an in-person learning, is increasingly necessary to promote societal and economic growth, promote inclusiveness and increase educational attainment and accessibility to individuals."
    },
    {
        "uuid":"dab10277-b4d9-4242-9933-9bebf55c889e",
        "amendment_number":16,
        "author":"CULT",
        "title":"Recital 35",
        "text":"(35) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood. When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination. (35) AI systems used in education or training, notably for determining access or assigning persons to educational and training institutions to evaluate persons on tests as part of or as a precondition for their education or for determining the areas of study a student should follow should be considered high-risk, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood. When improperly designed, developed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor students’ behaviour and emotion during tests at education and training institutions should be considered high-risk, since they are also interfering with students’ rights to privacy and data protection. The use of AI to check assessments, such as exam papers for plagiarism, should not be considered high-risk."
    },
    {
        "uuid":"c274e8e5-30d9-462e-a432-cb13b8f3a454",
        "amendment_number":17,
        "author":"CULT",
        "title":"Recital 36",
        "text":"(36) AI systems used in employment, workers management and access to self- employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects and livelihoods of these persons. Relevant work-related contractual relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Such persons should in principle not be considered users within the meaning of this Regulation. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of these persons may also impact their rights to data protection and privacy. (36) AI systems used in employment, employment support workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, for monitoring compliance with workplace rules and for monitoring or evaluation of persons in work-related relationships, should also be classified as high-risk, since those systems may appreciably impact the health, safety and security rules applicable in their work and at their workplaces and future career prospects and livelihoods of these persons. Relevant work-related relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Such persons should in principle not be considered users within the meaning of this Regulation. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of these persons may also impact their rights to data protection and privacy. In this regard, specific requirements on transparency, information and human oversight should apply. Trade unions and workers’ representatives should be informed and they should have access to any relevant documentation created under this Regulation for high-risk AI systems deployed or used in their work or at their workplace."
    },
    {
        "uuid":"33e62950-15a5-4686-b6b2-597bbd0098da",
        "amendment_number":18,
        "author":"CULT",
        "title":"Recital 70",
        "text":"(70) Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. Moreover, natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system. Such information and notifications should be provided in accessible formats for persons with disabilities. Further, users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and (70) Certain AI systems used to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use or where the content is evidently used to form part of a creative, artistic or fictional cinematographic work. Moreover, natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system. Such information and notifications should include a disclaimer and should be provided inaccessible formats for children, the elderly, migrants and persons with disabilities or other vulnerabilities. Further, users, who use an AI system to generate or manipulate image, audio or video content, texts or scripts that appreciably resembles existing persons, places or events and would falsely appear disclosing its artificial origin. to a person to be authentic, should disclose in a clear manner that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin."
    },
    {
        "uuid":"2fc1ea57-56e1-491f-89d2-7adff9104f3e",
        "amendment_number":19,
        "author":"CULT",
        "title":"Recital 73",
        "text":"(73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users. (73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on AI literacy, awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users."
    },
    {
        "uuid":"86ed4b17-c98c-4c3e-92e9-810d8ac30605",
        "amendment_number":20,
        "author":"CULT",
        "title":"Recital 74",
        "text":"(74) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI- on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation. Within their respective mission and fields of competence, they may provide in particular technical and scientific support to providers and notified bodies. (74) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI- on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation. Within their respective mission and fields of competence, they may provide in particular technical and scientific support to providers and notified bodies. It is necessary for the Commission to also create a pan-European network of universities and researchers focused on AI for enhanced studying and research on the impact of AI and to update the Digital Education Action Plan established in the communication of the Commission of 30 September 2020 entitled ‘Digital Education Action Plan 2021-2027 – Resetting education and training for the digital age’, in order to integrate AI and robotics innovation in education."
    },
    {
        "uuid":"712b99eb-4667-4b58-8961-df913dc238b1",
        "amendment_number":21,
        "author":"CULT",
        "title":"Recital 76",
        "text":"(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, (76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. including on technical specifications or existing standards regarding the requirements established in this Regulation and providing expert advice to and assisting the Commission on specific questions related to artificial intelligence and to addressing the challenges rising from the fast evolving development of AI technologies."
    },
    {
        "uuid":"7a8f4b09-bb5b-4dc3-ad76-5d02c942a230",
        "amendment_number":22,
        "author":"CULT",
        "title":"Recital 83",
        "text":"(83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks. (83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks. It is appropriate for a new set of common European guidelines and standards to be set up in order to protect privacy while making an effective use of the data available."
    },
    {
        "uuid":"7f968ed8-93a7-4a4e-b062-c2e8ad56dfbf",
        "amendment_number":23,
        "author":"CULT",
        "title":"Recital 85",
        "text":"(85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union (85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58 . In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58. Such consultations should involve qualified specialists from different areas of society including from the private sector, researchers and scientists, education, media and culture representatives, trade unions, consumer, parental and data protection organizations with skills and knowledge relevant to the task. In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. __________________ __________________ 58 OJ L 123, 12.5.2016, p. 1. 58 OJ L 123, 12.5.2016, p. 1."
    },
    {
        "uuid":"3e37c61b-d6ca-4cea-a966-0bb769be314e",
        "amendment_number":24,
        "author":"CULT",
        "title":"Recital 86 a (new)",
        "text":"(86a) Given the rapid technological developments and the required technical expertise in conducting the assessment of high-risk AI systems, the delegation of powers and the implementing powers of the Commission should be exercised with as much flexibility as possible. The Commission should regularly review Annex III without undue delay, at least every six months, while consulting with the relevant stakeholders, including ethics experts, anthropologists, sociologists, mental health specialists and any other relevant scientists and researchers, as well as with parent associations."
    },
    {
        "uuid":"e6e00a77-88a6-4923-8efb-183a6efb5c02",
        "amendment_number":25,
        "author":"CULT",
        "title":"Article 1 – paragraph 1 – point c",
        "text":"(c) harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems and biometric categorisation systems, and AI systems used to generate or manipulate image, audio or video content; (c) harmonised transparency rules for AI systems;"
    },
    {
        "uuid":"c1ac0b5c-7b0d-497b-88fd-5d4080e7eca2",
        "amendment_number":26,
        "author":"CULT",
        "title":"Article 2 – paragraph 4 a (new)",
        "text":"4a. This Regulation shall not affect or undermine academic research or development of AI systems and their outputs for the purpose of academic research."
    },
    {
        "uuid":"08cb62c0-9087-4c90-b7ee-691c1f8cf65b",
        "amendment_number":27,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 4",
        "text":"(4) ‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non- professional activity; (4) ‘user’ means any natural or legal person, public authority, educational and training institution, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity;"
    },
    {
        "uuid":"3e25b18e-8822-47a6-9888-688b35a4c6d5",
        "amendment_number":28,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 35",
        "text":"(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data; (35) ‘biometric categorisation system’ means an AI system that uses biometric data, or other physical, physiological or behavioural data, for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data;"
    },
    {
        "uuid":"e23767bb-d1b2-450d-a175-3c122d118d40",
        "amendment_number":29,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 39",
        "text":"(39) ‘publicly accessible space’ means any physical place accessible to the public, regardless of whether certain conditions for (39) ‘publicly accessible space’ means any place accessible to the public, regardless of whether certain conditions for access may apply; access may apply;"
    },
    {
        "uuid":"9329df96-badd-4d81-90d3-627cd235e39e",
        "amendment_number":30,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 44 – point a",
        "text":"(a) the death of a person or serious damage to a person’s health, to property or the environment, (a) the death of a person or serious damage to a person’s fundamental rights, health, to property or the environment, to democracy or the democratic rule of law,"
    },
    {
        "uuid":"3b5efb8f-f5a7-4b43-9020-04c409d67afe",
        "amendment_number":31,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 44 a (new)",
        "text":"(44a) ‘education and training institutions’ means providers of education and training, irrespective of the age of the persons receiving the education and training, including preschools, childcare, primary schools, secondary schools, tertiary education providers, vocational education and training and any type of lifelong learning providers;"
    },
    {
        "uuid":"206ae68a-0728-41fb-b598-2591941d3c24",
        "amendment_number":32,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 44 b (new)",
        "text":"(44b) ‘cultural institutions’ means institutions such as libraries, museums, theatres, concert halls, exhibition centres, architectural ensembles and multi- purpose arts venues, as well as their virtual sections, which organise cultural education, democratic exchanges and research and provide ways and means of engaging with cultural heritage;"
    },
    {
        "uuid":"bbae1c20-40b5-4242-8cb0-0619293f32a2",
        "amendment_number":33,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 44 c (new)",
        "text":"(44c) 'deep fake' means manipulated or synthetic audio, visual or audiovisual content, text or scripts which feature persons purported to be authentic and truthful;"
    },
    {
        "uuid":"c93cb5d9-36c8-4480-b729-779f198b953a",
        "amendment_number":34,
        "author":"CULT",
        "title":"Article 3 – paragraph 1 – point 44 d (new)",
        "text":"(44d) 'AI literacy' means the skills, knowledge and understanding regarding AI systems."
    },
    {
        "uuid":"0c6f605f-0c1e-4a7e-a50d-3a9ddcd23ad2",
        "amendment_number":35,
        "author":"CULT",
        "title":"Article 4 a (new)",
        "text":"Article 4a Trustworthy AI 1. All AI systems in the Union shall be developed, deployed and used in full respect of the Charter of Fundamental environmental footprint, including with regard to the extraction and consumption of natural resources (‘the principle of environmental sustainability’); (i) in a socially responsible manner that minimises their negative societal impact, especially with regard to social and gender inequalities and democratic processes (‘the principle of social responsibility’)."
    },
    {
        "uuid":"bf1cb7ec-dbc5-4509-bb84-96a7e50f67a9",
        "amendment_number":36,
        "author":"CULT",
        "title":"Article 4 b (new)",
        "text":"Article 4b AI literacy 1. When implementing this Regulation, the Union and the Member States shall promote measures and tools for the development of a sufficient level of AI literacy, across sectors and groups of operators concerned, including through education and training, skilling and reskilling programmes and while ensuring a proper gender and age balance, in view of allowing a democratic control of AI systems. 2. Providers and users of AI systems shall promote tools and shall take measures to ensure a sufficient level of AI literacy of their staff and any other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the environment in which the AI systems are to be used, and considering the persons or groups of persons on which the AI systems are to be used. 3. Such literacy tools and measures shall consist, in particular, of the teaching and learning of basic notions and skills about AI systems and their functioning, including the different types of products and uses, their risks and benefits and the severity of the harm they can cause and its probability of occurrence. 4. The level of AI literacy shall be considered to be sufficient where it contributes to the ability of operators to fully comply with and benefit from trustworthy AI, and in particular with the requirements laid down in this Regulation."
    },
    {
        "uuid":"572ca3d9-befc-4598-9f46-2015091d01a9",
        "amendment_number":37,
        "author":"CULT",
        "title":"Article 6 – paragraph 2",
        "text":"2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk. 2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall also be considered high-risk due to their risk to cause harm to health, safety, the environment, fundamental rights or to democracy and the rule of law."
    },
    {
        "uuid":"15ddf35c-7eb0-4e1b-a885-8131de6f3416",
        "amendment_number":38,
        "author":"CULT",
        "title":"Article 7 – paragraph 1 – point a",
        "text":"(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 of Annex III; (a) the AI systems are intended to be used in any of the areas listed in Annex III;"
    },
    {
        "uuid":"8b2c9ac9-b40a-47bc-b278-b4c038973602",
        "amendment_number":39,
        "author":"CULT",
        "title":"Article 7 – paragraph 1 – point b",
        "text":"(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III. (b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, democracy and the rule of law, or the environment that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III."
    },
    {
        "uuid":"541b4cec-4f5c-413e-8d22-321e7ecbdd3e",
        "amendment_number":40,
        "author":"CULT",
        "title":"Article 7 – paragraph 2 – introductory part",
        "text":"2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria: 2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights, democracy and the rule of law, or the environment that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria:"
    },
    {
        "uuid":"0fee2179-2ddc-41bd-a03a-fb8345cbc60c",
        "amendment_number":41,
        "author":"CULT",
        "title":"Article 7 – paragraph 2 a (new)",
        "text":"2a. The Commission shall conduct the assessment referred to in paragraph 2 annually under the consultation conditions laid down in Article 73."
    },
    {
        "uuid":"9f02d28c-5b46-4f6a-921f-0417a1065760",
        "amendment_number":42,
        "author":"CULT",
        "title":"Article 9 – paragraph 4 – subparagraph 2 – point d a (new)",
        "text":"(da) provision of a sufficient level of AI literacy;"
    },
    {
        "uuid":"829646c6-234f-4a49-9fd1-53f03cb3f8a8",
        "amendment_number":43,
        "author":"CULT",
        "title":"Article 9 – paragraph 8",
        "text":"8. When implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children. 8. When implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children, the elderly, migrants or other vulnerable groups."
    },
    {
        "uuid":"1da6c5ab-84e2-48a5-a670-e03fd4d22664",
        "amendment_number":44,
        "author":"CULT",
        "title":"Article 10 – paragraph 2 – point f",
        "text":"(f) examination in view of possible biases; (f) examination in view of possible biases, in particular deviations that could affect the health and safety of people or could lead to discrimination;"
    },
    {
        "uuid":"c51de98c-15b4-4ed6-8117-d080f26b66b6",
        "amendment_number":45,
        "author":"CULT",
        "title":"Article 10 – paragraph 2 – point g a (new)",
        "text":"(ga) the purpose and the environment in which the system is to be used;"
    },
    {
        "uuid":"fc98127f-7d93-4319-ae5b-021909c2a65c",
        "amendment_number":46,
        "author":"CULT",
        "title":"Article 13 – paragraph 1",
        "text":"1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured, with a view to achieving compliance with the relevant obligations of the user and of the provider set out in Chapter 3 of this Title. 1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable providers and users and other relevant stakeholders to easily interpret the system’s functioning and output and use it appropriately on the basis of informed decisions, with a view to achieving compliance with the relevant obligations set out in Chapter 3 of this Title."
    },
    {
        "uuid":"b52e49e2-b0a7-43c7-87fe-0da969613b0e",
        "amendment_number":47,
        "author":"CULT",
        "title":"Article 13 – paragraph 3 a (new)",
        "text":"3a. In order to comply with the obligations laid down in this Article, providers and users shall ensure a sufficient level of AI literacy in accordance with Article 4b."
    },
    {
        "uuid":"96f8fff3-b6bd-4b19-a59d-f04ff8879261",
        "amendment_number":48,
        "author":"CULT",
        "title":"Article 14 – paragraph 5 a (new)",
        "text":"5a. In order to comply with the obligations laid down in this Article, providers and users shall ensure a sufficient level of AI literacy in accordance with Article 4b."
    },
    {
        "uuid":"c725c786-1065-49c4-9f2b-ace6d9f0a330",
        "amendment_number":49,
        "author":"CULT",
        "title":"Article 29 – paragraph 1 a (new)",
        "text":"1a. In order to comply with the obligations laid down in this Article, as well as to be able to justify their possible non-compliance, users of high-risk AI systems shall ensure a sufficient level of AI literacy in accordance with Article 4b."
    },
    {
        "uuid":"d9a455c8-0402-4a1b-a4a7-f6d002008b12",
        "amendment_number":50,
        "author":"CULT",
        "title":"Article 41 – paragraph 2",
        "text":"2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law. 2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant stakeholders, including industry representatives, SMEs and other relevant bodies or expert groups established under relevant sectorial Union law."
    },
    {
        "uuid":"42af59de-cf35-48d0-ae69-f872acd7572a",
        "amendment_number":51,
        "author":"CULT",
        "title":"Article 52 – paragraph 1",
        "text":"1. Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence. 1. Providers shall ensure that AI systems used to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence."
    },
    {
        "uuid":"9749e322-c1a7-46d8-9d01-596acbf1536c",
        "amendment_number":52,
        "author":"CULT",
        "title":"Article 52 – paragraph 2",
        "text":"2. Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences. 2. Users of an emotion recognition system or a biometric categorisation system shall inform, in a timely, clear and intelligible manner, of the operation of the system the natural persons exposed thereto. That information shall also include, as appropriate, the rights and processes to allow natural persons to appeal against the application of such AI systems to them. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences."
    },
    {
        "uuid":"e2f65e80-f737-435e-8823-392b142019ea",
        "amendment_number":53,
        "author":"CULT",
        "title":"Article 52 – paragraph 3 – subparagraph 1",
        "text":"Users of an AI system that generates or Users of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose that the content has been artificially generated or manipulated. manipulates image, audio, text, scripts or video content that appreciably resembles existing persons, objects, places, text, scripts or other entities or events and would falsely appear to a person to be authentic or truthful (‘deep fake’), shall disclose, in an appropriate clear, repetitive, timely and visible manner, that the content has been artificially generated or manipulated."
    },
    {
        "uuid":"1ababa13-77ab-4df3-b432-96f8ec2cf2c5",
        "amendment_number":54,
        "author":"CULT",
        "title":"Article 52 – paragraph 3 – subparagraph 2",
        "text":"However, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. However, the first subparagraph shall not apply where the use forms part of an evidently artistic, creative or fictional cinematographic or analogous work or where it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties."
    },
    {
        "uuid":"1bc43474-a16e-402a-9256-8241a045636a",
        "amendment_number":55,
        "author":"CULT",
        "title":"Article 52 – paragraph 3 a (new)",
        "text":"3a. Providers and users of AI systems that recommend, disseminate and order news or creative and cultural content shall disclose, in an appropriate, easily accessible, clear and visible manner, the main parameters used for the moderation of content and personalized suggestions. That information shall include a disclaimer."
    },
    {
        "uuid":"223253ac-f589-438b-b9b4-5db4cc011867",
        "amendment_number":56,
        "author":"CULT",
        "title":"Article 52 – paragraph 3 b (new)",
        "text":"3b. The information referred to in this Article shall be provided to the natural persons in a timely, clear and visible manner, at the latest at the time of the first interaction or exposure. Such information shall be made accessible when the exposed natural person is a person with disabilities, a child or where he or she belongs to a vulnerable group. It shall be complete, where possible, with intervention or flagging procedures for the exposed natural person, taking into account the generally acknowledged state of the art and relevant harmonised standards and common specifications."
    },
    {
        "uuid":"7f28769c-4972-4d23-a569-2e72566ad580",
        "amendment_number":57,
        "author":"CULT",
        "title":"Article 56 – paragraph 2 – point a a (new)",
        "text":"(aa) work towards promoting uptake of AI within the Union, especially amongst SMEs;"
    },
    {
        "uuid":"f5029d7d-4863-4de7-8c7c-adebf1aafbeb",
        "amendment_number":58,
        "author":"CULT",
        "title":"Article 57 – paragraph 1",
        "text":"1. The Board shall be composed of the national supervisory authorities, who shall 1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national or international authorities and relevant stakeholders, including from the private sector, shall be invited to the meetings, where the issues discussed are of relevance for them."
    },
    {
        "uuid":"392b5d15-b426-4e0b-ab2c-dadff2b87708",
        "amendment_number":59,
        "author":"CULT",
        "title":"Article 57 – paragraph 4",
        "text":"4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. 4. The Board shall, where relevant, invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups, including the High-Level Expert Group on AI."
    },
    {
        "uuid":"5ac60827-a795-4071-8bac-c86ed3f70ded",
        "amendment_number":60,
        "author":"CULT",
        "title":"Article 69 – paragraph 3",
        "text":"3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems. 3. Codes of conduct may be drawn up by individual providers of AI systems or by organisations representing them or by both, including with the involvement of users and any interested stakeholders and their representative organisations, including in particular trade unions and consumers organisations. Codes of conduct may cover one or more AI systems taking into account the similarity of the intended purpose of the relevant systems."
    },
    {
        "uuid":"a4a280c6-b78b-49c4-ae8e-262b88e42889",
        "amendment_number":61,
        "author":"CULT",
        "title":"Article 69 – paragraph 3 b (new)",
        "text":"3b. In order to comply with the obligations laid down in this Article, providers and users shall ensure a sufficient level of AI literacy in accordance with Article 4b."
    },
    {
        "uuid":"5ad4d020-a4fd-4570-ab92-a626b5c73da8",
        "amendment_number":62,
        "author":"CULT",
        "title":"Article 71 – paragraph 1",
        "text":"1. In compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, including administrative fines, applicable to infringements of this Regulation and shall take all measures necessary to ensure that they are properly and effectively implemented. The penalties provided for shall be effective, proportionate, and dissuasive. They shall take into particular account the interests of small-scale providers and start-up and their economic viability. 1. In compliance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties, including administrative fines, applicable to infringements of this Regulation and shall take all measures necessary to ensure that they are properly and effectively implemented. The penalties provided for shall be effective, proportionate, and dissuasive. They shall take into particular account the interests and market position of small-scale providers and start-up and their economic viability."
    },
    {
        "uuid":"e92d5cac-3e3a-4aac-9983-cea02e48731c",
        "amendment_number":63,
        "author":"CULT",
        "title":"Article 73 – paragraph 3 a (new)",
        "text":"3a. Before adopting a delegated act, the Commission shall consult with the relevant institutions and stakeholders in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making."
    },
    {
        "uuid":"39161a96-984e-46d2-b3c6-0261c5e824c9",
        "amendment_number":1,
        "author":"IMCO-LIBE",
        "title":"Recital 1",
        "text":"(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI- based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation. (1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, the placing on the market, the putting into service and the use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety, fundamental rights and the Union values enshrined in Article 2 of the Treaty on European Union (TEU), and it ensures the free movement of AI- based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation. Or. en"
    },
    {
        "uuid":"6579f05e-1aad-4fd1-9472-a920b0bfc2eb",
        "amendment_number":2,
        "author":"IMCO-LIBE",
        "title":"Recital 2",
        "text":"(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental (2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of protection throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board. rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of protection throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). Or. en Justification A new recital 2a has been created to explain the reference to Article 16 TFEU."
    },
    {
        "uuid":"c8027570-1642-458f-8845-4bdbdf2d03a4",
        "amendment_number":3,
        "author":"IMCO-LIBE",
        "title":"Recital 2 a (new)",
        "text":"(2a) Artificial intelligence often relies on the processing of large volumes of data, and many AI systems and applications process personal data. This Regulation is therefore also based on Article 16 TFEU, which enshrines the right of everyone to the protection of personal data concerning them and provides for the adoption of rules on the protection of individuals with regard to the processing of personal data. In light of the reliance on Article 16 TFEU, it is appropriate to consult the European Data Protection Board. Or. en"
    },
    {
        "uuid":"218a4527-aaa3-44a4-b963-66ae3bcf4251",
        "amendment_number":4,
        "author":"IMCO-LIBE",
        "title":"Recital 2 b (new)",
        "text":"(2b) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU) 2016\/679 and (EU) 2018\/1725 and Directive 2016\/680. Directive 2002\/58\/EC additionally protects private life and the confidentiality of communications, including providing conditions for any personal and non-personal data storing in and access from terminal equipment. Those legal acts provide the basis for sustainable and responsible data processing, including where datasets include a mix of personal and non- personal data. This Regulation complements and does not affect Union law on data protection and privacy, in particular those other Regulations and Directives. This Regulation does not seek to affect the application of existing Union law governing the processing of personal data, including the tasks and powers of the independent supervisory authorities competent to monitor compliance with those instruments. This Regulation does not affect the fundamental rights to private life and data protection as provided for by Union law on data protection and privacy and enshrined in the Charter of Fundamental Rights of the European Union (the ‘Charter’). Or. en"
    },
    {
        "uuid":"8d049699-2f61-4653-9beb-c4aeda8a7543",
        "amendment_number":5,
        "author":"IMCO-LIBE",
        "title":"Recital 4",
        "text":"(4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial. (4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and fundamental rights of natural persons that are protected by Union law. Such harm might be material or immaterial. Or. en"
    },
    {
        "uuid":"5bf0286f-389a-4dc3-9099-ee1577a074bd",
        "amendment_number":6,
        "author":"IMCO-LIBE",
        "title":"Recital 4 a (new)",
        "text":"(4a) Given the major impact that artificial intelligence can have on society and the need to build trust, it is vital for artificial intelligence and its regulatory framework to be developed according to Union values enshrined in Article 2 TEU, the fundamental rights and freedoms enshrined in the Treaties, the Charter, and international human rights law. As a pre-requisite, artificial intelligence should be a human-centric technology. It should not substitute human autonomy or assume the loss of individual freedom and should primarily serve the needs of the people and the common good. Safeguards should be provided to ensure the development and use of ethically embedded artificial intelligence that respects Union values and the Charter. Or. en"
    },
    {
        "uuid":"329a2792-dcf6-43ed-a8a9-196b11c2fd6e",
        "amendment_number":7,
        "author":"IMCO-LIBE",
        "title":"Recital 5",
        "text":"(5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33, and it ensures the protection of ethical principles, as specifically requested by the European Parliament34. (5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development,use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety, the protection of fundamental rights, as recognised and protected by Union law and the Union values enshrined in Article 2 TEU. To achieve that objective, rules regulating the development, the placing on the market, the putting into service and the use of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33, and it ensures the protection of ethical principles, as specifically requested. __________________ __________________ 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 33 European Council, Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13\/20, 2020, p. 6. 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL). 34 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020\/2012(INL). Or. en"
    },
    {
        "uuid":"22b9cb71-04ae-4d00-9c4c-0b55ee602ffb",
        "amendment_number":8,
        "author":"IMCO-LIBE",
        "title":"Recital 6",
        "text":"(6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand- alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the (6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list. Commission to amend that list. Or. en"
    },
    {
        "uuid":"45facb57-43a9-4c05-8845-cb0ded01d6dd",
        "amendment_number":9,
        "author":"IMCO-LIBE",
        "title":"Recital 7",
        "text":"(7) The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016\/679 of the European Parliament and of the Council35, Article 3(18) of Regulation (EU) 2018\/1725 of the European Parliament and of the Council36 and Article 3(13) of Directive (EU) 2016\/680 of the European Parliament and of the Council37. (7) The notion of biometric data used in this Regulation is the same as that in Article 4(14) of Regulation (EU) 2016\/679 of the European Parliament and of the Council35, Article 3(18) of Regulation (EU) 2018\/1725 of the European Parliament and of the Council36 and Article 3(13) of Directive (EU) 2016\/680 of the European Parliament and of the Council37 and should therefore be interpreted consistently with those provisions. Biometrics-based data are additional data resulting from specific technical processing relating to physical, physiological or behavioural signals of a natural person. __________________ __________________ 35 Regulation (EU) 2016\/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95\/46\/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). 35 Regulation (EU) 2016\/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95\/46\/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). 36 Regulation (EU) 2018\/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45\/2001 and Decision No 1247\/2002\/EC (OJ L 295, 36 Regulation (EU) 2018\/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45\/2001 and Decision No 1247\/2002\/EC (OJ L 295, 21.11.2018, p. 39) 21.11.2018, p. 39) 37 Directive (EU) 2016\/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008\/977\/JHA (Law Enforcement Directive) (OJ L 119, 4.5.2016, p. 89). 37 Directive (EU) 2016\/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008\/977\/JHA (Law Enforcement Directive) (OJ L 119, 4.5.2016, p. 89). Or. en Justification The reference to biometrics-based data has been added, in alignment with the new definition inserted in article 3."
    },
    {
        "uuid":"53d9a8d6-8a33-4fc0-9e03-7f0acfdf0c52",
        "amendment_number":10,
        "author":"IMCO-LIBE",
        "title":"Recital 12",
        "text":"(12) This Regulation should also apply to Union institutions, offices, bodies and agencies when acting as a provider or user of an AI system. AI systems exclusively developed or used for military purposes should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU). This Regulation should be without prejudice to the provisions regarding the liability of intermediary service providers set out in Directive 2000\/31\/EC of the European Parliament and of the Council [as amended by the Digital Services Act]. (12) This Regulation should also apply to Union institutions, offices, bodies and agencies when acting as a provider or user of an AI system. Or. en Justification This recital has been split in three separate recitals for clarity."
    },
    {
        "uuid":"1aad2ad9-f66c-4a4f-b2d8-74f6aa63d9b3",
        "amendment_number":11,
        "author":"IMCO-LIBE",
        "title":"Recital 12 a (new)",
        "text":"(12a) AI systems developed or used exclusively for military purposes should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the TEU. Or. en"
    },
    {
        "uuid":"2bd64d90-409c-4ab4-b76c-1ca2162d05ca",
        "amendment_number":12,
        "author":"IMCO-LIBE",
        "title":"Recital 12 b (new)",
        "text":"(12b) This Regulation should be without prejudice to the provisions regarding the liability of intermediary service providers set out in Directive 2000\/31\/EC of the European Parliament and of the Council1a [as amended by the Digital Services Act]. __________________ 1a Directive 2000\/31\/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of information society services, in particular electronic commerce, in the Internal Market ('Directive on electronic commerce') (OJ L 178, 17.7.2000, p. 1). Or. en"
    },
    {
        "uuid":"8b915087-48fc-4d39-8007-33b47f7734b6",
        "amendment_number":13,
        "author":"IMCO-LIBE",
        "title":"Recital 13",
        "text":"(13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments. (13) In order to ensure a consistent and high level of protection of public interests as regards health, safety, fundamental rights and the Union values enshrined in Article 2 TEU, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter and should be non-discriminatory. and in line with the Union’s international trade commitments. Or. en"
    },
    {
        "uuid":"ed86dfd0-3dac-457f-8257-1c490db4a33e",
        "amendment_number":14,
        "author":"IMCO-LIBE",
        "title":"Recital 14",
        "text":"(14) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk- based approach should be followed. That approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate. It is therefore necessary to prohibit certain artificial intelligence practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems. (14) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk- based approach should be followed. That approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate. It is therefore necessary to prohibit certain unacceptable artificial intelligence practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems. Or. en"
    },
    {
        "uuid":"6d6f2422-c818-43ce-ad67-e6623b1434a5",
        "amendment_number":15,
        "author":"IMCO-LIBE",
        "title":"Recital 15",
        "text":"(15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child. (15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and abusive and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child. Or. en"
    },
    {
        "uuid":"53b8dcc2-b393-4d69-9c36-9112d8eeafa8",
        "amendment_number":16,
        "author":"IMCO-LIBE",
        "title":"Recital 17 a (new)",
        "text":"(17a) AI systems used by law enforcement authorities or on their behalf to predict the probability of a natural person to offend or to reoffend, based on profiling and individual risk-assessment hold a particular risk of discrimination against certain persons or groups of persons, as they violate human dignity as well as the key legal principle of presumption of innocence. Such AI systems should therefore be prohibited. Or. en Justification predictive policing should be added among the prohibited practices as it violates the presumption of innocence as well as human dignity."
    },
    {
        "uuid":"8621bf06-765b-4d97-8f17-4362aade9b61",
        "amendment_number":17,
        "author":"IMCO-LIBE",
        "title":"Recital 26 a (new)",
        "text":"(26a) Practices that are prohibited by Union legislation, including under data protection law, non-discrimination law, consumer protection law, and competition law, are not affected by this Regulation. Or. en"
    },
    {
        "uuid":"4d8dd168-bb49-49ee-b988-567fe60c8b58",
        "amendment_number":18,
        "author":"IMCO-LIBE",
        "title":"Recital 26 b (new)",
        "text":"(26b) In accordance with the risk-based approach of this Regulation, a list of high-risk AI systems should be established in an annex to this Regulation and should be regularly evaluated and reviewed, subject to the appropriate involvement and consultation of stakeholders and civil society. Or. en"
    },
    {
        "uuid":"5d5a9582-8672-465c-ae90-ab5d480f4772",
        "amendment_number":19,
        "author":"IMCO-LIBE",
        "title":"Recital 27",
        "text":"(27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any. (27) High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law and do not contravene the Union values enshrined in Article 2 TEU. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety, and the fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any. Or. en"
    },
    {
        "uuid":"8029e31a-f3ef-47fa-b7bc-26ea9b054ce4",
        "amendment_number":20,
        "author":"IMCO-LIBE",
        "title":"Recital 28",
        "text":"(28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, (28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as safety components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety and security risks that may be generated by a product as a whole due to Justification This recital has been split in two parts to highlight the fundamental rights element."
    },
    {
        "uuid":"4196db08-89c7-48bf-b0b6-906810d5df12",
        "amendment_number":21,
        "author":"IMCO-LIBE",
        "title":"Recital 28 a (new)",
        "text":"(28a) The extent of the adverse impact caused by an AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk, regardless of the field of application. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, non- discrimination, consumer protection, workers’ rights, rights of persons with disabilities, the right to an effective remedy and to a fair trial, the presumption of innocence, the right of defence and the right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the Charter and in the United Nations Convention on the Rights of the Child, further elaborated in the UNCRC General Comment No 25 as regards the digital environment, both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection implemented in Union law and policies and enshrined in the Charter should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons. Or. en"
    },
    {
        "uuid":"d535465c-f173-48c0-a51c-6e82f11e922a",
        "amendment_number":22,
        "author":"IMCO-LIBE",
        "title":"Recital 32",
        "text":"(32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems. (32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of natural persons or to the Union values enshrined in Article 2 TEU, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in this Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems. Or. en"
    },
    {
        "uuid":"26f075fa-417f-46be-ac52-672602ad7504",
        "amendment_number":23,
        "author":"IMCO-LIBE",
        "title":"Recital 35",
        "text":"(35) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk, (35) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be classified as since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood. When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination. considered high-risk AI systems, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood. When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination. Children, in particular, constitute an especially vulnerable group of people and require additional safeguards. AI systems intended to shape children’s development through personalised education or cognitive or emotional development should therefore be classified as high-risk AI systems. Or. en"
    },
    {
        "uuid":"95fc4562-b250-4d42-aec0-7a4c25fad952",
        "amendment_number":24,
        "author":"IMCO-LIBE",
        "title":"Recital 37",
        "text":"(37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of (37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of"
    },
    {
        "uuid":"4247b3f3-a24e-4722-90d9-7e66937c047c",
        "amendment_number":25,
        "author":"IMCO-LIBE",
        "title":"Recital 38",
        "text":"(38) Actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of natural person, to detect ‘deep fakes’, for the evaluation of the reliability of evidence in criminal proceedings, for predicting the occurrence or reoccurrence of an actual or potential (38) Actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities, polygraphs and similar tools or to detect the emotional state of natural person, to detect ‘deep fakes’, for the evaluation of the reliability of evidence in criminal proceedings, for profiling in the course of detection, investigation or prosecution of criminal offences, as well as criminal offence based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups, for profiling in the course of detection, investigation or prosecution of criminal offences, as well as for crime analytics regarding natural persons. AI systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be considered high-risk AI systems used by law enforcement authorities for the purposes of prevention, detection, investigation and prosecution of criminal offences. for crime analytics regarding natural persons. AI systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be classified as high-risk AI systems used by law enforcement authorities for the purposes of prevention, detection, investigation and prosecution of criminal offences. Or. en Justification The use of AI systems for predictive policing should be prohibited, not high-risk."
    },
    {
        "uuid":"2f0494d3-861b-4b35-a15b-f6ecafb62e10",
        "amendment_number":26,
        "author":"IMCO-LIBE",
        "title":"Recital 40",
        "text":"(40) Certain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. Such qualification should not extend, however, to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of (40) Certain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. Such qualification should not extend, however, to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks or allocation of resources. justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks or allocation of resources. In order to address the risks to the right to vote enshrined in Article 39 of the Charter of undue external interference, and of disproportionate effects on democratic processes, democracy, and the rule of law, AI systems used in political campaigns to influence the votes of natural persons in local, national or European Parliament elections or for the purpose of vote counting and processing in such elections should be classified as high-risk AI systems. Or. en Justification To align with the additions in the Justice and Democracy area in Annex III."
    },
    {
        "uuid":"b5da7c5d-f427-4a46-b905-23e0222238ae",
        "amendment_number":27,
        "author":"IMCO-LIBE",
        "title":"Recital 40 a (new)",
        "text":"(40a) Certain AI systems should at the same time be subject to transparency requirements and be classified as high- risk AI systems, given their potential to deceive and cause both individual and societal harm. In particular, AI systems that generate deep fakes representing existing persons have the potential to both manipulate the natural persons that are exposed to those deep fakes and harm the persons they are representing or misrepresenting, while AI systems that, based on limited human input, generate complex text such as news articles, opinion articles, novels, scripts, and scientific articles (“AI authors”) have the potential to manipulate, deceive, or to expose natural persons to built-in biases or inaccuracies. Or. en"
    },
    {
        "uuid":"e29e0e4e-4999-48f6-88a1-d672e26b56aa",
        "amendment_number":28,
        "author":"IMCO-LIBE",
        "title":"Recital 41",
        "text":"(41) The fact that an AI system is classified as high risk under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant. (41) The fact that an AI system is classified a high risk AI system under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data. Or. en"
    },
    {
        "uuid":"27a4816e-7193-4309-a229-2a84d2da9e83",
        "amendment_number":29,
        "author":"IMCO-LIBE",
        "title":"Recital 44",
        "text":"(44) High data quality is essential for the (44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high-risk AI systems. performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative, up-to-date and, to the best extent possible, free of errors and as complete as possible, in view of the intended purpose or reasonably foreseeable uses of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended or reasonably foreseeable to be used. In particular, training, validation and testing datasets should take into account, to the extent required in the light of their intended purpose or reasonably foreseeable uses, the features, characteristics or elements that are particular to the specific geographical, cultural, behavioural or functional setting or context within which the AI system is intended to be used or within which its use is reasonably foreseeable. Or. en Justification alignment with the changes in art. 10"
    },
    {
        "uuid":"d13a628e-fe37-45ed-9a87-e478a1b78a79",
        "amendment_number":30,
        "author":"IMCO-LIBE",
        "title":"Recital 45 a (new)",
        "text":"(45a) The right to privacy and to data protection must be guaranteed throughout the entire lifecycle of the AI system. In this regard, the principles of data minimisation and data protection by design and by default, as set out in Union data protection law, are essential when the processing of data involves significant risks to the fundamental rights of individuals. Providers and users of AI systems should implement state-of-the-art technical and organisational measures in order to protect those rights. Such measures should include not only anonymisation and encryption, but also the use of increasingly available technology that permits algorithms to be brought to the data and allows valuable insights to be derived without the transmission between parties or unnecessary copying of the raw or structured data themselves. Or. en Justification alignment with the changes in article 10."
    },
    {
        "uuid":"c52cdc53-6b48-410d-9d32-90b30f61a083",
        "amendment_number":31,
        "author":"IMCO-LIBE",
        "title":"Recital 45 b (new)",
        "text":"(45b) Providers may not always be able to access the datasets needed to develop high-risk AI systems, such as when the datasets are in the exclusive possession of the user while the provider only provides the tools and the techniques to the user in order to develop the AI system. In such circumstances, the provider cannot objectively comply with the requirements and obligations on the quality of datasets laid down in this Regulation. Such obligations should therefore be fulfilled by the user, on the basis of an agreement between the provider and the user. Or. en Justification Alignment with the changes in article 10."
    },
    {
        "uuid":"d4912e86-7cc8-43de-a0ec-99c85bc5e593",
        "amendment_number":32,
        "author":"IMCO-LIBE",
        "title":"Recital 56",
        "text":"(56) To enable enforcement of this Regulation and create a level-playing field for operators, and taking into account the different forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative established in the Union. (56) To enable enforcement of this Regulation and create a level-playing field for operators, and taking into account the different forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, providers established outside the Union shall, by written mandate, appoint an authorised representative established in the Union. Or. en Justification Alignment with article 25.1."
    },
    {
        "uuid":"f641617a-4092-40f1-8472-7cc73026a50d",
        "amendment_number":33,
        "author":"IMCO-LIBE",
        "title":"Recital 61",
        "text":"(61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025\/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. However, the Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient. (61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025\/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. To ensure the effectiveness of standards and standardisation as policy tools for the Union, and considering the importance of standards for the competitiveness of undertakings and for ensuring conformity with the requirements of this Regulation, it is necessary to ensure a balanced representation of interests by encouraging the participation of all relevant stakeholders in the development of standards. In areas where no harmonised standards exist or where the standards are insufficient, the Commission could adopt common technical specifications. __________________ __________________ 54 Regulation (EU) No 1025\/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89\/686\/EEC and 93\/15\/EEC and Directives 94\/9\/EC, 94\/25\/EC, 95\/16\/EC, 97\/23\/EC, 98\/34\/EC, 2004\/22\/EC, 2007\/23\/EC, 2009\/23\/EC and 2009\/105\/EC of the European Parliament and of the Council and repealing Council Decision 87\/95\/EEC and Decision No 1673\/2006\/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12). 54 Regulation (EU) No 1025\/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89\/686\/EEC and 93\/15\/EEC and Directives 94\/9\/EC, 94\/25\/EC, 95\/16\/EC, 97\/23\/EC, 98\/34\/EC, 2004\/22\/EC, 2007\/23\/EC, 2009\/23\/EC and 2009\/105\/EC of the European Parliament and of the Council and repealing Council Decision 87\/95\/EEC and Decision No 1673\/2006\/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12). Or. en Justification Alignment with the changes in article 40."
    },
    {
        "uuid":"8cbe44fd-a608-4cfe-9571-d01d58eb4dc7",
        "amendment_number":34,
        "author":"IMCO-LIBE",
        "title":"Recital 68",
        "text":"(68) Under certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons and for society as a whole. It is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment. deleted Or. en Justification alignment with the deletion of article 47."
    },
    {
        "uuid":"270432fd-5498-405d-81bc-13a5df6777ec",
        "amendment_number":35,
        "author":"IMCO-LIBE",
        "title":"Recital 69",
        "text":"(69) In order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, should be required to register their high-risk AI system in a EU database, to be established and managed by the Commission. The Commission should be the controller of that database, in accordance with (69) In order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, should be required to register their high-risk AI system in a EU database, to be established and managed by the Commission. Users who are public authorities or Union institutions, bodies, offices and agencies Regulation (EU) 2018\/1725 of the European Parliament and of the Council55. In order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report. or users acting on their behalf should also register in the EU database before putting into service or using a high-risk AI system. The Commission should be the controller of that database, in accordance with Regulation (EU) 2018\/1725 of the European Parliament and of the Council55. In order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report. __________________ __________________ 55 Regulation (EU) 2016\/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95\/46\/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). 55 Regulation (EU) 2016\/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95\/46\/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1). Or. en Justification To match changes in article 51."
    },
    {
        "uuid":"30589afe-e0c4-4053-aa40-2930e76b0f12",
        "amendment_number":36,
        "author":"IMCO-LIBE",
        "title":"Recital 76",
        "text":"(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or (76) In order to facilitate a smooth, effective and consistent implementation of this Regulation and to prevent the fragmentation of the internal market, a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence. this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence, including on possible amendments of the annexes, in particular the annex listing high-risk AI systems. To contribute to the effective and harmonised enforcement of this Regulation, the Board should also be able to issue recommendations to the relevant national supervisory authorities in order to provide assistance for the settlement of cases involving two or more Member States in which the national competent authorities are in disagreement. Or. en Justification Alingment with the changes on the governance chapter."
    },
    {
        "uuid":"f3960a7b-a003-40e0-ab45-d1b7aff7f214",
        "amendment_number":37,
        "author":"IMCO-LIBE",
        "title":"Recital 76 a (new)",
        "text":"(76a) To ensure a common and consistent approach regarding the development and use of AI systems in the various areas and sectors concerned and to ensure synergies and complementarities, the Board should cooperate closely with other relevant institutions, bodies, offices, agencies and boards established at Union level, including the European Data Protection Supervisor, the European Data Protection Board, Data innovation Board set up by... [Data Governance Act] and the European Board for Digital Services established by... [Digital Services Act]. In addition, the Board should regularly consult and work with representatives from industry, SMEs and start-ups and relevant civil society organisations, such as non- governmental organisations (NGOs), consumer associations, the social partners and academia. Or. en"
    },
    {
        "uuid":"d4276e3a-b8d2-4ef3-9997-808f3cbffbeb",
        "amendment_number":38,
        "author":"IMCO-LIBE",
        "title":"Recital 77",
        "text":"(77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority. (77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one single national authority should be designated as national supervisory authority. That national supervisory authority should act as lead authority and should also represent its Member State on the Board. Where the designated national supervisory authority is not the national data protection authority, the national supervisory authority should act in close cooperation with the national data protection authority, in order to ensure a consistent and efficient implementation of data protection rights and obligations. Or. en Justification alignment with changes in article 59."
    },
    {
        "uuid":"5d9d442b-6eb8-4b0b-87c0-0b6e32089bec",
        "amendment_number":39,
        "author":"IMCO-LIBE",
        "title":"Recital 78",
        "text":"(78) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place. This system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems. (78) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place. This system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities, or where relevant, to the Commission, any serious incidents, malfunctioning or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems and take appropriate corrective actions. Users should also report to the relevant authorities or, where relevant, to the Commission, any serious incidents or breaches to national and Union law protecting fundamental rights resulting from the use of their AI system when they become aware of such serious incidents or breaches. Or. en Justification alignment with article 62."
    },
    {
        "uuid":"78a1afce-6676-4832-83bc-109cbfee8c65",
        "amendment_number":40,
        "author":"IMCO-LIBE",
        "title":"Recital 80 a (new)",
        "text":"(80a) Given the objectives of this Regulation, namely to ensure an equivalent level of protection of health, safety and fundamental rights of natural persons, to ensure the protection of the Union values enshrined in Article 2 TEU, and to ensure the free movement of AI systems throughout the Union, and taking into account that the mitigation of the risks of AI system against such rights may not be sufficiently achieved at national level or may be subject to diverging interpretation which could ultimately lead to an uneven level of protection of natural persons and create market fragmentation, the Commission should be empowered, on its own initiative or upon recommendation from the Board, to initiate proceedings. Such proceedings should be initiated where the Commission or the Board have sufficient reasons to believe that an infringement of this Regulation amount to a widespread infringement or a widespread infringement with a Union dimension, or where the AI system presents a risk which affects or is likely to affect at least 45 million individuals within the Union, or where the infringement affects natural persons in at least two Member States and the Member States responsible for enforcing this Regulation have not taken any action. Or. en Justification to match the new article 68a."
    },
    {
        "uuid":"6d9c837a-561f-4916-a671-595c34ba1ba7",
        "amendment_number":41,
        "author":"IMCO-LIBE",
        "title":"Recital 80 b (new)",
        "text":"(80b) Once the Commission initiates proceedings, the national supervisory authorities of the Member States concerned should be precluded from exercising their investigatory and enforcement powers in respect of the relevant operator or operators, so as to avoid duplication, inconsistencies and risks from the viewpoint of the principle of ne bis in idem. However, in the interest of effectiveness, national supervisory authorities should not be precluded from exercising their power to assist the Commission, on its request in the performance of its tasks, or in respect of other conduct, including conduct by the same operator that is suspected to constitute a new infringement. National supervisory authorities, as well as the Board and other national competent authorities where relevant, should provide the Commission with all necessary information and assistance to allow it to perform its tasks effectively. The Commission should keep national supervisory authorities informed with regard to the exercise of its powers as appropriate. In that regard, the Commission should, where appropriate, take account of any relevant assessments carried out by the Board or by the national supervisory authorities concerned and of any relevant evidence and information gathered by them, without prejudice to the Commission’s powers and responsibility to carry out additional investigations as necessary. Or. en Justification to match new article 68a."
    },
    {
        "uuid":"9ddb34b9-58f6-4485-911b-1d441bfb0afe",
        "amendment_number":42,
        "author":"IMCO-LIBE",
        "title":"Recital 80 c (new)",
        "text":"(80c) In view of both the particular challenges that may arise in seeking to ensure compliance by the relevant operators and the importance of doing so effectively, considering the impact and the harms that their AI systems may cause, the Commission should have strong investigative and enforcement powers to allow it to investigate, enforce and monitor certain of the rules laid down in this Regulation, in full respect of the principle of proportionality and the rights and interests of the affected parties. Or. en Justification to match new article 68b."
    },
    {
        "uuid":"525fc88d-9ced-45f1-a056-febae311e624",
        "amendment_number":43,
        "author":"IMCO-LIBE",
        "title":"Recital 80 d (new)",
        "text":"(80d) The Commission should have access to any relevant documents, information and data necessary to open and conduct investigations and to monitor the compliance with this Regulation, regardless of their form or format or the manner or location of their storage. The Commission should be able to directly require the operators concerned to provide any relevant evidence, information and data. In addition, the Commission should be able to request any relevant information from any public authority, body or agency within the Member States, or from any natural or legal person for the purpose of this Regulation. The Commission should be empowered to require access to, and explanations relating to, databases, algorithms and source codes, to interview, upon their consent, any persons who may be in possession of useful information and to record the statements made. The Commission should be able to carry out the necessary remote and on-site inspections, and should have the power to enter any premises, land or means of transport that the economic operator uses for purposes relating to its trade, business, craft or profession. The Commission should also be empowered to undertake such inspections as are necessary to enforce this Regulation. Where the Commission finds out that the operator or operators concerned do not comply with this Regulation, it should be empowered to adopt decisions and impose fines. Where there is a risk of serious and irreparable harm to natural persons due to non-compliance, the Commission should be able to take measures, where duly justified and proportionate and where there are no other means available to prevent or mitigate such harm. Those investigatory and enforcement powers aim to complement the Commission’s possibility to ask the national supervisory authority and other Member States’ authorities for assistance, for instance by providing information or in the exercise of those powers. Or. en Justification to match new article 68b."
    },
    {
        "uuid":"9ee9725a-882f-4350-a227-f9b43fecb75c",
        "amendment_number":44,
        "author":"IMCO-LIBE",
        "title":"Recital 83",
        "text":"(83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks. (83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks. The relevant competent authorities should put in place adequate cybersecurity and organisational measures to protect the security and confidentiality of the information and data obtained in carrying out their tasks and activities. Or. en Justification to match the changes in article 70."
    },
    {
        "uuid":"4248b677-ac9c-447f-8f15-5162b009b1a8",
        "amendment_number":45,
        "author":"IMCO-LIBE",
        "title":"Recital 84 a (new)",
        "text":"(84a) Compliance with this Regulation should be enforceable by means of the imposition of fines by the Commission when carrying out proceedings under the procedure laid down in this Regulation. To that end, appropriate levels of fines should also be laid down for non- compliance with the obligations and for breaches of the procedural rules, subject to appropriate limitation periods. Or. en Justification to match the new enforcement powers given to the Commission in the enforcement chapter."
    },
    {
        "uuid":"f48ef3ac-fbf9-48af-a7d8-335d05fc204d",
        "amendment_number":46,
        "author":"IMCO-LIBE",
        "title":"Recital 84 b (new)",
        "text":"(84b) Natural and legal persons and groups of natural or legal persons should be entitled to access proportionate and effective remedies. They should in particular have the right to lodge a complaint against the providers or users of AI systems and receive compensation against any direct damage or loss they have with regard to their health, safety, or fundamental rights, due to an infringement of this Regulation by the provider or the user. Without prejudice to any other administrative or non-judicial remedy, natural and legal persons and groups of natural or legal persons should also have the right to an effective judicial remedy with regard to a legally binding decision of a national supervisory authority or of the Commission concerning them or, where the national supervisory authority does not handle a complaint, does not inform the complainant of the progress or preliminary outcome of the complaint lodged or does not comply with its obligation to reach a final decision, with regard to the complaint. Or. en Justification to match the new provisions on redress."
    },
    {
        "uuid":"2578df24-1f38-476b-a0cb-89540c4618c4",
        "amendment_number":47,
        "author":"IMCO-LIBE",
        "title":"Article 1 – paragraph 1 – point a",
        "text":"(a) harmonised rules for the placing on the market, the putting into service and the use of artificial intelligence systems (‘AI systems’) in the Union; (a) harmonised rules for the development, the placing on the market, the putting into service and the use of artificial intelligence systems (‘AI systems’) in the Union; Or. en"
    },
    {
        "uuid":"18ff90f7-5efc-4620-8cba-9fb82228a5ba",
        "amendment_number":48,
        "author":"IMCO-LIBE",
        "title":"Article 1 – paragraph 1 – point c a (new)",
        "text":"(ca) harmonised rules on high-risk AI systems to ensure a high level of trustworthiness and of protection of health, safety, fundamental rights and the Union values enshrined in Article 2 TEU; Or. en"
    },
    {
        "uuid":"6819a2de-607e-4c6d-9a2c-2dfc3cdd9d50",
        "amendment_number":49,
        "author":"IMCO-LIBE",
        "title":"Article 1 – paragraph 1 – point d",
        "text":"(d) harmonised transparency rules for AI systems intended to interact with natural persons, emotion recognition systems and biometric categorisation systems, and AI systems used to generate or manipulate image, audio or video content; (d) harmonised transparency rules for AI systems; Or. en"
    },
    {
        "uuid":"7c200552-6156-44e5-80b4-2568a1ac81b4",
        "amendment_number":50,
        "author":"IMCO-LIBE",
        "title":"Article 2 – paragraph 1 – point a",
        "text":"(a) providers placing on the market or putting into service AI systems in the Union, irrespective of whether those providers are established within the Union or in a third country; (a) operators placing on the market or putting into service AI systems in the Union, irrespective of whether those operators are established within the Union or in a third country; Or. en"
    },
    {
        "uuid":"7cd9a625-9039-4415-9492-2d848f0bada3",
        "amendment_number":51,
        "author":"IMCO-LIBE",
        "title":"Article 2 – paragraph 1 – point c",
        "text":"(c) providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union; (c) providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union or affects natural persons within the Union; Or. en"
    },
    {
        "uuid":"4ec8c1b5-c5ce-4245-ac45-4d9abfd94b43",
        "amendment_number":52,
        "author":"IMCO-LIBE",
        "title":"Article 2 – paragraph 1 – point c a (new)",
        "text":"(ca) natural persons affected by the use of an AI system. Or. en"
    },
    {
        "uuid":"af224565-a1cd-4092-a400-c58ffd24145c",
        "amendment_number":53,
        "author":"IMCO-LIBE",
        "title":"Article 2 – paragraph 3 a (new)",
        "text":"3a. This Regulation shall also apply to Union institutions, offices, bodies and agencies when acting as a provider or user of an AI system. Or. en"
    },
    {
        "uuid":"e83e6880-096c-47ed-bcca-7b9759ea148e",
        "amendment_number":54,
        "author":"IMCO-LIBE",
        "title":"Article 2 – paragraph 5 a (new)",
        "text":"5a. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal data processed in connection with the rights and obligations laid down in this Regulation. This Regulation shall not affect Regulation (EU) 2016\/679, Regulation (EU) 2018\/1725, Directive 2002\/58\/EC or Directive (EU) 2016\/680. Or. en"
    },
    {
        "uuid":"cb5f344c-8245-4aa3-8f4d-8f0eff7b859a",
        "amendment_number":55,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 1",
        "text":"(1) ‘artificial intelligence system’ (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a (1) ‘artificial intelligence system’ (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with; given set of objectives, generate outputs such as content, predictions, hypotheses, recommendations, or decisions influencing the environments they interact with; Or. en"
    },
    {
        "uuid":"27b92dc3-2245-45f7-9dbe-7f7516471872",
        "amendment_number":56,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 4",
        "text":"(4) ‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non- professional activity; (4) ‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority and in the course of its professional activity; Or. en"
    },
    {
        "uuid":"f5eff728-d224-446b-be06-6ca29fa1eeb7",
        "amendment_number":57,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 14",
        "text":"(14) ‘safety component of a product or system’ means a component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property; (14) ‘safety component of a product or system’ means a component of a product or of a system which fulfils a safety or security function for that product or system or the failure or malfunctioning of which endangers the health and safety or the fundamental rights of natural persons or which damages property; Or. en"
    },
    {
        "uuid":"abc08611-5424-46b3-a347-61316c760966",
        "amendment_number":58,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 15",
        "text":"(15) ‘instructions for use’ means the information provided by the provider to inform the user of in particular an AI system’s intended purpose and proper use, inclusive of the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used; (15) ‘instructions for use’ means the information provided by the provider, on a durable medium, to inform the user of in particular an AI system’s intended purpose and proper use, as well as information on any precautions to be taken, inclusive of the specific geographical, behavioural or functional setting within which the high- risk AI system is intended to be used; Or. en"
    },
    {
        "uuid":"6b65898c-d2de-4939-ba22-357c234486b7",
        "amendment_number":59,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 16",
        "text":"(16) ‘recall of an AI system’ means any measure aimed at achieving the return to the provider of an AI system made available to users; (16) ‘recall of an AI system’ means any measure aimed at achieving the return to the provider of an AI system that has been made available to users; Or. en"
    },
    {
        "uuid":"fb8fb268-1738-46b1-bdc5-3f63ba59792d",
        "amendment_number":60,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 22",
        "text":"(22) ‘notified body’ means a conformity assessment body designated in accordance with this Regulation and other relevant Union harmonisation legislation; (22) ‘notified body’ means a conformity assessment body notified in accordance with Article 32 of this Regulation and with other relevant Union harmonisation legislation; Or. en"
    },
    {
        "uuid":"8a7587f3-cd00-4475-84ad-922e09937d36",
        "amendment_number":61,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 23",
        "text":"(23) ‘substantial modification’ means a change to the AI system following its placing on the market or putting into service which affects the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation or results in a modification to the intended purpose for which the AI system has been assessed; (23) ‘substantial modification’ means a change or a series of changes to the AI system following its placing on the market or putting into service which affects the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation or results in a modification to the intended purpose for which the AI system has been assessed or to its performance; Or. en"
    },
    {
        "uuid":"a2546410-4cc8-4c6c-9799-1398528b97a5",
        "amendment_number":62,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 30",
        "text":"(30) ‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process, among other things, in order to prevent overfitting; whereas the validation dataset can be a separate dataset or part of the training dataset, either as a fixed or variable split; (30) ‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process, among other things, in order to prevent underfitting or overfitting; whereas the validation dataset can be a separate dataset or part of the training dataset, either as a fixed or variable split; Or. en"
    },
    {
        "uuid":"38c646ab-1c39-4f9d-a9ce-afbc1cd61e20",
        "amendment_number":63,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 33",
        "text":"(33) ‘biometric data’ means personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data; (33) ‘biometric data’ means biometric data as defined in Article 4, point (14) of Regulation (EU) 2016\/679; Or. en"
    },
    {
        "uuid":"769c0abb-0b06-4204-8106-61c013f88256",
        "amendment_number":64,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 33 a (new)",
        "text":"(33a) ‘biometrics-based data’ means data resulting from specific technical processing relating to physical, physiological or behavioural signals of a natural person, such as facial expressions, movements, pulse frequency, voice, key strikes or gait, which may or may not allow or confirm the unique identification of a natural person; Or. en"
    },
    {
        "uuid":"e135c5ad-bdf7-475d-875e-c70a8318afd6",
        "amendment_number":65,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 33 b (new)",
        "text":"(33b) ‘subliminal techniques’ means techniques that use sensorial stimuli such as images, text, or sounds, that are below or above the threshold of conscious human perception; Or. en"
    },
    {
        "uuid":"805e0b9a-980c-4fd6-9d9d-73b9decb93c5",
        "amendment_number":66,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 33 c (new)",
        "text":"(33c) ‘special categories of personal data’ means the categories of personal data referred to in Article 9(1) of Regulation (EU)2016\/679; Or. en"
    },
    {
        "uuid":"3efe3dff-97b0-4a4e-a02c-f442edc77446",
        "amendment_number":67,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 34",
        "text":"(34) ‘emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data; (34) ‘emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions, thoughts, states of mind or intentions of natural persons on the basis of their biometric and biometric-based data; Or. en"
    },
    {
        "uuid":"212e3c70-6f6e-4eb5-b47e-4dc5bacac647",
        "amendment_number":68,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 35",
        "text":"(35) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data; (35) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories, such as gender, sex, age, hair colour, eye colour, tattoos, ethnic origin, health, mental or physical ability, behavioural or personality traits or sexual or political orientation, on the basis of their biometric and biometric-based data; Or. en"
    },
    {
        "uuid":"9a51bd8a-2989-4c6f-a050-8bc398a9cfc4",
        "amendment_number":69,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 42",
        "text":"(42) ‘national supervisory authority’ means the authority to which a Member State assigns the responsibility for the implementation and application of this Regulation, for coordinating the activities entrusted to that Member State, for acting as the single contact point for the Commission, and for representing the Member State at the European Artificial Intelligence Board; (42) ‘national supervisory authority’ means a public authority to which a Member State assigns the responsibility for the implementation and application of this Regulation, for coordinating the activities entrusted to that Member State, for acting as the single contact point for the Commission, and for representing the Member State at the European Artificial Intelligence Board; Or. en"
    },
    {
        "uuid":"968cd895-df58-4c5b-a32b-ebe74b8e458f",
        "amendment_number":70,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 – introductory part",
        "text":"(44) ‘serious incident’ means any incident that directly or indirectly leads, might have led or might lead to any of the (44) ‘serious incident’ means any incident or malfunctioning that directly or indirectly leads, might have led or might following: lead to any of the following: Or. en"
    },
    {
        "uuid":"932b1d99-4bd5-4420-bb91-a81c42e9e4a9",
        "amendment_number":71,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 a (new)",
        "text":"(44a) 'personal data' means personal data as defined in Article 4, point (1) of Regulation (EU)2016\/679; Or. en"
    },
    {
        "uuid":"3401d16f-bc06-4fe8-b6ee-b155868d3755",
        "amendment_number":72,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 b (new)",
        "text":"(44b) ‘non-personal data’ means data other than personal data; Or. en"
    },
    {
        "uuid":"d7504e3c-31c0-407d-a83f-192d66f2f53c",
        "amendment_number":73,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 c (new)",
        "text":"(44c) ‘risk’ means the combination of the probability of an occurrence of a hazard causing harm and the degree of severity of that harm; Or. en"
    },
    {
        "uuid":"e035be92-509e-48df-903b-62b4bf6da021",
        "amendment_number":74,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 d (new)",
        "text":"(44d) ‘widespread infringement’ means: (a) any act or omission contrary to Union law that protects the interests of individuals, that has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other than the Member State, in which: (i) the act or omission originated or took place; (ii) the provider concerned, or, where applicable, its authorised representative is established; or, (iii) the user is established, when the infringement is committed by the user; (b) any acts or omissions contrary to Union law that protects the interests of individuals, that have done, do or are likely to do harm to the collective interests of individuals and that have common features, including the same unlawful practice, the same interest being infringed and that are occurring concurrently, committed by the same operator, in at least three Member States; Or. en Justification A definition of widespread infringement has been introduced to clarify the conditions triggering the Commission’s intervention in the enforcement chapter. The concept is taken from Regulation (EU) 2017\/2394 of the European Parliament and of the Council of 12 December 2017 on cooperation between national authorities responsible for the enforcement of consumer protection laws and adapted to this Regulation."
    },
    {
        "uuid":"681e006a-0aa0-466c-97ef-7026bc37a1bb",
        "amendment_number":75,
        "author":"IMCO-LIBE",
        "title":"Article 3 – paragraph 1 – point 44 e (new)",
        "text":"(44e) ‘widespread infringement with a Union dimension’ means a widespread infringement that has harmed or is likely to harm the collective interests of individuals in at least two-thirds of the Member States, accounting, together, for at least two-thirds of the population of the Union. Or. en"
    },
    {
        "uuid":"6947bcc5-9b80-4008-98c4-bfee4abe772e",
        "amendment_number":76,
        "author":"IMCO-LIBE",
        "title":"Article 5 – paragraph 1 – point c a (new)",
        "text":"(ca) the placing on the market, putting into service or use of an AI system for making individual risk assessments of natural persons in order to assess the risk of a natural person for offending or reoffending or for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of a natural person or on assessing personality traits and characteristics or past criminal behaviour of natural persons or groups of natural persons; Or. en Justification Predictive policing violates human dignity and the presumption of innocence, and it holds a particular risk of discrimination. It is therefore inserted among the prohibited practices."
    },
    {
        "uuid":"74368146-3786-4726-8f02-cf5eba376916",
        "amendment_number":77,
        "author":"IMCO-LIBE",
        "title":"Article 5 – paragraph 4 a (new)",
        "text":"4a. This Article shall not affect the prohibitions that apply where an artificial intelligence practice infringes other law, including data protection law, non- discrimination law, consumer protection law or competition law. Or. en"
    },
    {
        "uuid":"9b008ce2-2fbe-4a5e-8a09-bd1a926f9773",
        "amendment_number":78,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 1 – point a",
        "text":"(a) the AI systems are intended to be used in any of the areas listed in points 1 to 8 of Annex III; deleted Or. en"
    },
    {
        "uuid":"fb220a71-fa44-4722-b019-c0b3b76e390c",
        "amendment_number":79,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 1 – point b",
        "text":"(b) the AI systems pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights, that is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high-risk AI systems already referred to in Annex III. deleted Or. en"
    },
    {
        "uuid":"32754719-70f6-4bc2-b7f7-8a535e50a89c",
        "amendment_number":80,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 1 a (new)",
        "text":"1a. The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex III by adding areas of high-risk AI systems, where a type of AI system poses a risk of harm to health and safety, a risk of adverse impact on fundamental rights, or a risk of contravention of the Union values enshrined in Article 2 TEU and that risk is, in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by high-risk AI systems in use in the areas listed in Annex III. Or. en"
    },
    {
        "uuid":"cbc3de55-6a81-4761-a70f-7a778e00b08e",
        "amendment_number":81,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – introductory part",
        "text":"2. When assessing for the purposes of paragraph 1 whether an AI system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high-risk AI systems already referred to in Annex III, the Commission shall take into account the following criteria: 2. When assessing an AI system for the purposes of paragraph 1, the Commission shall take into account the following criteria: Or. en"
    },
    {
        "uuid":"7d1ae186-8d47-4b3b-b69a-0f2f99222fe1",
        "amendment_number":82,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – point b",
        "text":"(b) the extent to which an AI system has been used or is likely to be used; (b) the extent to which an AI system has been used or is likely to be used, including its reasonably foreseeable misuse; Or. en"
    },
    {
        "uuid":"70f0e1ca-d41e-4421-91b0-96630f878afd",
        "amendment_number":83,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – point b a (new)",
        "text":"(ba) the type and nature of the data processed and used by the AI system; Or. en"
    },
    {
        "uuid":"3e754dab-4571-4b2f-a83e-b08d172f258c",
        "amendment_number":84,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – point c",
        "text":"(c) the extent to which the use of an AI system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities; (c) the extent to which the use of an AI system has already caused harm to natural persons, has contravened the Union values enshrined in Article 2 TEU, has caused harm to health and safety, has had an adverse impact on fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities, to the Commission, to the Board, to the EDPS or to the European Union Agency for Fundamental Rights (FRA); Or. en"
    },
    {
        "uuid":"76d10a95-f38f-4bb9-a2ae-24f3c8ef8378",
        "amendment_number":85,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – point d",
        "text":"(d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons; (d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons or to disproportionately affect a particular group of persons; Or. en"
    },
    {
        "uuid":"14646538-fe34-45f8-a37a-25537ba1d39f",
        "amendment_number":86,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 – point g",
        "text":"(g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible; (g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on health, safety, fundamental rights of persons, or on the Union values enshrined in Article 2 TEU shall not be considered as easily reversible; Or. en"
    },
    {
        "uuid":"181ba45c-1625-4297-8199-3e0faedb07ae",
        "amendment_number":87,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 a (new)",
        "text":"2a. When assessing whether an AI system poses a risk of harm to the health and safety or risk of adverse impact on fundamental rights that is equivalent or greater than the risk of harm posed by the high-risk AI system, the Commission shall consult, where relevant, representatives of groups on which an AI system has an impact, industry, independent experts and civil society organisations. The Commission shall organise public consultations in this regard. Or. en"
    },
    {
        "uuid":"0c941c2c-8be8-4cc7-a797-5a25e215c2fc",
        "amendment_number":88,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 b (new)",
        "text":"2b. The Commission shall publish a detailed report on the assessment referred to in paragraph 2. Or. en"
    },
    {
        "uuid":"e06e1f7d-1650-43ae-9904-07e15b2d87e2",
        "amendment_number":89,
        "author":"IMCO-LIBE",
        "title":"Article 7 – paragraph 2 c (new)",
        "text":"2c. The Commission shall consult the Board before drafting delegated acts pursuant to paragraph 1. Or. en"
    },
    {
        "uuid":"29262266-02f7-49c2-a4b7-3f58d835a659",
        "amendment_number":90,
        "author":"IMCO-LIBE",
        "title":"Article 9 – paragraph 2 – point a",
        "text":"(a) identification and analysis of the known and foreseeable risks associated with each high-risk AI system; (a) identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to: (i) the health or safety of natural persons; (ii) the legal rights or legal status of natural persons; (iii) the fundamental rights of natural persons; (iv) the equal access to services and opportunities of natural persons; (v) the Union values enshrined in Article 2 TEU. Or. en"
    },
    {
        "uuid":"44ca4a18-5dcc-4778-a51d-2c334651aaf2",
        "amendment_number":91,
        "author":"IMCO-LIBE",
        "title":"Article 9 – paragraph 3",
        "text":"3. The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2. They shall take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications. 3. The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2. They shall take into account the state of the art, including as reflected in relevant harmonised standards or common specifications. Or. en"
    },
    {
        "uuid":"45dc7166-e479-4126-ae68-c889e040a17d",
        "amendment_number":92,
        "author":"IMCO-LIBE",
        "title":"Article 9 – paragraph 4 – subparagraph 3",
        "text":"In eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment in which the system is intended to be used. In eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the user and the environment, including possible context, in which the system is intended to be used. Or. en"
    },
    {
        "uuid":"e61119bf-7c0c-4e29-a3e6-50a1d93b71cf",
        "amendment_number":93,
        "author":"IMCO-LIBE",
        "title":"Article 9 – paragraph 6",
        "text":"6. Testing procedures shall be suitable to achieve the intended purpose of the AI system and do not need to go beyond what is necessary to achieve that purpose. 6. Testing procedures shall be suitable to achieve the intended purpose of the AI system. Or. en"
    },
    {
        "uuid":"181d9588-5841-4f11-a351-a9d63ab5c923",
        "amendment_number":94,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 2 – point c",
        "text":"(c) relevant data preparation processing operations, such as annotation, labelling, (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and cleaning, enrichment and aggregation; aggregation; Or. en"
    },
    {
        "uuid":"fd9ae582-186e-41ae-aec9-5aea2a65ddfa",
        "amendment_number":95,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 2 – point f a (new)",
        "text":"(fa) appropriate measures to detect, prevent and mitigate possible biases; Or. en"
    },
    {
        "uuid":"6c7a634e-e8fd-4cdf-bc8d-eeabc935c668",
        "amendment_number":96,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 3",
        "text":"3. Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the data sets may be met at the level of individual data sets or a combination thereof. 3. Training, validation and testing datasets shall be relevant, representative, up-to-date, and to the best extent possible, taking into account the state of the art, free of errors and be as complete as possible. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used. These characteristics of the datasets may be met at the level of individual data sets or a combination thereof. Or. en"
    },
    {
        "uuid":"7d20966b-fc4b-4f8b-889e-361105f1920d",
        "amendment_number":97,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 4",
        "text":"4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used. 4. Training, validation and testing data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, cultural, behavioural or functional setting within which the high-risk AI system is intended to be used. Or. en"
    },
    {
        "uuid":"7ace8dc6-166c-4f6b-b99b-b3f55ff22dbb",
        "amendment_number":98,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 5",
        "text":"5. To the extent that it is strictly necessary for the purposes of ensuring bias monitoring, detection and correction in relation to the high-risk AI systems, the providers of such systems may process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016\/679, Article 10 of Directive (EU) 2016\/680 and Article 10(1) of Regulation (EU) 2018\/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons, including technical limitations on the re-use and use of state- of-the-art security and privacy-preserving measures, such as pseudonymisation, or encryption where anonymisation may significantly affect the purpose pursued. deleted Or. en Justification This Regulation should not constitute a separate legal basis for processing personal data."
    },
    {
        "uuid":"2a45c0b5-6ea4-422a-9d49-8ff07c6d262d",
        "amendment_number":99,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 6 a (new)",
        "text":"6a. Where the provider cannot comply with the obligations laid down in this Article because it does not have access to the data and the data is held exclusively by the user, the user may, on the basis of a contract, be made responsible for any infringement of this Article. Or. en"
    },
    {
        "uuid":"638f7288-e126-4f3f-b49b-f550b08dd980",
        "amendment_number":100,
        "author":"IMCO-LIBE",
        "title":"Article 10 – paragraph 6 b (new)",
        "text":"6b. The principles of data minimisation and of data protection by design and by default, as referred to, respectively, in Article 5(1), point (c) and in Article 25 of Regulation (EU) 2016\/679 shall be applied when developing and using high-risk AI systems and during the entire lifecycle of those systems. Or. en"
    },
    {
        "uuid":"14ab71d3-76c6-4c86-a22d-1b64015f56b0",
        "amendment_number":101,
        "author":"IMCO-LIBE",
        "title":"Article 11 – paragraph 2",
        "text":"2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is 2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in Annex IV as well as the information required under those legal acts. placed on the market or put into service, technical documentation shall be drawn up containing all the information set out in Annex IV as well as the information required under those legal acts. Or. en"
    },
    {
        "uuid":"ff35422b-afe7-46ab-b982-063d8842e982",
        "amendment_number":102,
        "author":"IMCO-LIBE",
        "title":"Article 12 – paragraph 2",
        "text":"2. The logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to the intended purpose of the system. 2. The logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its entire lifecycle that is appropriate to the intended purpose of the system. Or. en"
    },
    {
        "uuid":"eb614709-c41d-403a-857e-72bff67e4438",
        "amendment_number":103,
        "author":"IMCO-LIBE",
        "title":"Article 12 – paragraph 3",
        "text":"3. In particular, logging capabilities shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification, and facilitate the post-market monitoring referred to in Article 61. 3. In particular, logging capabilities shall enable the monitoring of the operation of the high-risk AI system with respect to the identification of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification, and facilitate the monitoring of operations as referred in Article 29(4) as well as the post-market monitoring referred to in Article 61. Or. en"
    },
    {
        "uuid":"01606203-82d5-43e5-b3ce-ea32689169d4",
        "amendment_number":104,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point a a (new)",
        "text":"(aa) ensure that natural persons to whom human oversight of high-risk AI systems is assigned are specifically made aware and remain aware of the risk of automation bias; Or. en"
    },
    {
        "uuid":"812ed31d-bbc1-4667-bd81-781506e6e9cf",
        "amendment_number":106,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point d",
        "text":"(d) when under their control, keep the logs automatically generated by their high- risk AI systems; (d) when under their control, keep the logs automatically generated by their high- risk AI systems that are required for ensuring and demonstrating compliance with this Regulation, for ex-post audits of any reasonably foreseeable malfunction or misuses of the system, or for ensuring and monitoring for the proper functioning of the system throughout its entire lifecycle; Or. en"
    },
    {
        "uuid":"4f9cd44f-9c62-4188-b7b4-26143dd3db7b",
        "amendment_number":107,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point e",
        "text":"(e) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service; (e) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure, prior to its placing on the market or putting into service, in accordance with Article 43; Or. en"
    },
    {
        "uuid":"6bcb0e7c-0314-4f5e-a081-bf79d0e4ab48",
        "amendment_number":108,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point e a (new)",
        "text":"(ea) draw up an EU declaration of conformity in accordance with Article 48; Or. en"
    },
    {
        "uuid":"18ab80ad-d387-4936-adae-d2adecb219d5",
        "amendment_number":109,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point e b (new)",
        "text":"(eb) affix the CE marking to their high-risk AI systems to indicate conformity with this Regulation in accordance with Article 49; Or. en Justification moved up from letter i"
    },
    {
        "uuid":"aef1b378-e510-4df6-a2d5-97c33e8b261e",
        "amendment_number":110,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point g",
        "text":"(g) take the necessary corrective actions, if the high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title; (g) take the necessary corrective actions as referred to in Article 21 and provide information in that regard; Or. en"
    },
    {
        "uuid":"99cacbe9-45cc-4d8e-8e0d-8b414af2e604",
        "amendment_number":111,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point h",
        "text":"(h) inform the national competent authorities of the Member States in which they made the AI system available or put it into service and, where applicable, the notified body of the non-compliance and of any corrective actions taken; deleted Or. en"
    },
    {
        "uuid":"00f01325-97d2-433d-94b0-210568c898dd",
        "amendment_number":112,
        "author":"IMCO-LIBE",
        "title":"Article 16 – paragraph 1 – point i",
        "text":"(i) to affix the CE marking to their high-risk AI systems to indicate the conformity with this Regulation in deleted accordance with Article 49; Or. en"
    },
    {
        "uuid":"ee50cc14-c3b8-462f-943a-955df03cdd27",
        "amendment_number":113,
        "author":"IMCO-LIBE",
        "title":"Article 17 – paragraph 1 – introductory part",
        "text":"1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects: 1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. It shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects: Or. en"
    },
    {
        "uuid":"48b49389-846f-44d6-8126-0d1d54899705",
        "amendment_number":114,
        "author":"IMCO-LIBE",
        "title":"Article 17 – paragraph 1 – point j",
        "text":"(j) the handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties; (j) the handling of communication with relevant competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties; Or. en"
    },
    {
        "uuid":"88374080-da7f-446d-80ce-5f4b55946758",
        "amendment_number":115,
        "author":"IMCO-LIBE",
        "title":"Article 17 – paragraph 2",
        "text":"2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the size of the provider’s organisation. 2. The implementation of aspects referred to in paragraph 1 shall be proportionate to the size of the provider’s organisation. Providers shall in any event respect the degree of rigour and the level of protection required to ensure compliance of their AI systems with this Regulation. Or. en Justification The size of the company needs to be taken into account but should not justify less rigour for compliance."
    },
    {
        "uuid":"0fe25ec3-919e-4ea8-b0ed-abda42ef7bd9",
        "amendment_number":116,
        "author":"IMCO-LIBE",
        "title":"Article 19",
        "text":"Article 19 deleted Conformity assessment 1. Providers of high-risk AI systems shall ensure that their systems undergo the relevant conformity assessment procedure in accordance with Article 43, prior to their placing on the market or putting into service. Where the compliance of the AI systems with the requirements set out in Chapter 2 of this Title has been demonstrated following that conformity assessment, the providers shall draw up an EU declaration of conformity in accordance with Article 48 and affix the CE marking of conformity in accordance with Article 49. 2. For high-risk AI systems referred to in point 5(b) of Annex III that are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013\/36\/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to101 of that Directive. Or. en Justification Paragraph 1 of this article has been moved up to art. 16(e) and (ea), while paragraph 2 is already in article 43.2."
    },
    {
        "uuid":"63d12384-aae4-4ded-9a4c-73dcdc48dcf3",
        "amendment_number":117,
        "author":"IMCO-LIBE",
        "title":"Article 21 – paragraph 1",
        "text":"Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system in question and, where applicable, the authorised representative and importers accordingly. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately and without delay take the necessary corrective actions to bring that system into conformity, to withdraw it or to recall it, as appropriate. Or. en"
    },
    {
        "uuid":"25b1e3b0-12cd-4b11-9f6e-8e8b2ac0a235",
        "amendment_number":118,
        "author":"IMCO-LIBE",
        "title":"Article 21 – paragraph 1 a (new)",
        "text":"In the cases referred to in paragraph 1, providers shall immediately inform the distributors of the high-risk AI system and, where applicable, the authorised representative, importers and users accordingly. They shall also immediately inform the national competent authorities of the Member States in which they made the AI system available or put it into service, and where applicable, the notified body of the non-compliance and of any corrective actions taken. Or. en"
    },
    {
        "uuid":"273ed5e5-b99c-47cb-a038-1d31a61f95a2",
        "amendment_number":119,
        "author":"IMCO-LIBE",
        "title":"Article 22 – paragraph 1",
        "text":"Where the high-risk AI system presents a risk within the meaning of Article 65(1) and that risk is known to the provider of the system, that provider shall immediately inform the national competent authorities of the Member States in which it made the system available and, where applicable, the notified body that issued a certificate for the high-risk AI system, in particular of the non-compliance and of any corrective actions taken. Where the high-risk AI system presents a risk and that risk is known to the provider of the system, that provider shall immediately inform the national competent authorities of the Member States in which it made the system available and, where applicable, the notified body that issued a certificate for the high-risk AI system, in particular of the non-compliance and of any corrective actions taken. Where applicable, the provider shall also inform the users of the high-risk AI system. Or. en"
    },
    {
        "uuid":"468621c5-daaf-4196-9124-03542411ac11",
        "amendment_number":120,
        "author":"IMCO-LIBE",
        "title":"Article 23 – title",
        "text":"Cooperation with competent authorities Cooperation with competent authorities, the Board and the Commission Or. en"
    },
    {
        "uuid":"5cc08ff6-c3ec-40de-a641-185ce4aba4d9",
        "amendment_number":121,
        "author":"IMCO-LIBE",
        "title":"Article 23 – paragraph 1",
        "text":"Providers of high-risk AI systems shall, upon request by a national competent authority, provide that authority with all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title, in an official Union language determined by the Member State concerned. Upon a reasoned request from a national competent authority, providers shall also give that authority access to the logs automatically generated by the high- risk AI system, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law. Providers and where applicable, users of high-risk AI systems shall, upon request by a national competent authority or where applicable, by the Board or the Commission, provide them with all the information and documentation necessary to demonstrate the conformity of the high- risk AI system with the requirements set out in Chapter 2 of this Title, in an official Union language determined by the Member State concerned. Or. en"
    },
    {
        "uuid":"fb247a32-8530-4de8-9f72-8e7063a20789",
        "amendment_number":122,
        "author":"IMCO-LIBE",
        "title":"Article 23 – paragraph 1 a (new)",
        "text":"Upon a reasoned request by a national competent authority or, where applicable, by the Commission, providers and, where applicable, users shall also give the requesting national competent authority or the Commission, as applicable, access to the logs automatically generated by the high-risk AI system, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law. Or. en"
    },
    {
        "uuid":"ceaaba1a-f247-4861-8d76-65253be3be1c",
        "amendment_number":123,
        "author":"IMCO-LIBE",
        "title":"Article 25 – paragraph 1",
        "text":"1. Prior to making their systems available on the Union market, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative which is established in the Union. 1. Prior to making their systems available on the Union market, providers established outside the Union shall, by written mandate, appoint an authorised representative which is established in the Union. Or. en"
    },
    {
        "uuid":"cc9591a7-7bf0-44ba-bd2e-0495b5f1103b",
        "amendment_number":124,
        "author":"IMCO-LIBE",
        "title":"Article 25 – paragraph 2 – introductory part",
        "text":"2. The authorised representative shall perform the tasks specified in the mandate received from the provider. The mandate shall empower the authorised representative to carry out the following tasks: 2. The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the market surveillance authorities upon request, in an official Union language determined by the national competent authority. The mandate shall empower the authorised representative to carry out the following tasks: Or. en"
    },
    {
        "uuid":"d09b9171-8d4b-4d77-a55d-083919613922",
        "amendment_number":125,
        "author":"IMCO-LIBE",
        "title":"Article 25 – paragraph 2 – point c",
        "text":"(c) cooperate with competent national authorities, upon a reasoned request, on any action the latter takes in relation to the high-risk AI system. (c) cooperate with national competent authorities, upon a reasoned request, on any action the latter takes in relation to the high-risk AI system. Or. en"
    },
    {
        "uuid":"37104143-fc3b-4d27-8468-830a1ef0ded0",
        "amendment_number":126,
        "author":"IMCO-LIBE",
        "title":"Article 25 – paragraph 2 – point c a (new)",
        "text":"(ca) where applicable, comply with the registration obligations referred in Article 51. Or. en"
    },
    {
        "uuid":"08194546-60d8-4fef-9bf4-b5f27b6827ff",
        "amendment_number":127,
        "author":"IMCO-LIBE",
        "title":"Article 26 – paragraph 1 – point b",
        "text":"(b) the provider has drawn up the technical documentation in accordance with Annex IV; (b) the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV; Or. en"
    },
    {
        "uuid":"ebf28b65-8afc-4521-8d64-aa85716d7932",
        "amendment_number":128,
        "author":"IMCO-LIBE",
        "title":"Article 26 – paragraph 1 – point c a (new)",
        "text":"(ca) where applicable, the provider has appointed an authorised representative in accordance with Article 25(1). Or. en"
    },
    {
        "uuid":"0cb70437-f6e5-43c3-b654-7589b62a8dd3",
        "amendment_number":129,
        "author":"IMCO-LIBE",
        "title":"Article 27 – paragraph 2",
        "text":"2. Where a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements. Furthermore, where the system presents a risk within the meaning of Article 65(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect. 2. Where a distributor considers or has reason to consider that a high-risk AI system is not in conformity with the requirements set out in Chapter 2 of this Title, it shall not make the high-risk AI system available on the market until that system has been brought into conformity with those requirements. Furthermore, where the system presents a risk within the meaning of Article 65(1), the distributor shall inform the provider or the importer of the system and the relevant national competent authority, as applicable, to that effect. Or. en"
    },
    {
        "uuid":"a10332c5-6997-4041-9766-327eb19bc596",
        "amendment_number":130,
        "author":"IMCO-LIBE",
        "title":"Article 27 – paragraph 5",
        "text":"5. Upon a reasoned request from a national competent authority, distributors of high-risk AI systems shall provide that authority with all the information and documentation necessary to demonstrate 5. Upon a reasoned request from a national competent authority, distributors of high-risk AI systems shall provide that authority with all the information and documentation necessary to demonstrate the conformity of a high-risk system with the requirements set out in Chapter 2 of this Title. Distributors shall also cooperate with that national competent authority on any action taken by that authority. the conformity of the high-risk AI system with the requirements set out in Chapter 2 of this Title. Distributors shall also cooperate with that national competent authority on any action taken by that authority. Or. en"
    },
    {
        "uuid":"87071a7c-9f33-41c0-89ed-fc504cb1a744",
        "amendment_number":131,
        "author":"IMCO-LIBE",
        "title":"Article 28 – paragraph 1 – point a",
        "text":"(a) they place on the market or put into service a high-risk AI system under their name or trademark; (a) they place on the market or put into service a high-risk AI system under their name or trademark unless a contractual arrangement provides otherwise with regard to the allocation of obligations, where applicable; Or. en"
    },
    {
        "uuid":"84806c74-22cc-4380-b1b4-001bdaf5bb98",
        "amendment_number":132,
        "author":"IMCO-LIBE",
        "title":"Article 28 – paragraph 1 – point b a (new)",
        "text":"(ba) they modify the intended purpose of an AI system placed on the market or put into service in such manner that the AI system becomes a high risk AI system in accordance with Article 6; Or. en"
    },
    {
        "uuid":"8aa6d36a-db93-4b30-bf6a-37dba3cbb121",
        "amendment_number":133,
        "author":"IMCO-LIBE",
        "title":"Article 28 – paragraph 1 – point c",
        "text":"(c) they make a substantial modification to the high-risk AI system. (c) they make a substantial modification to a high-risk AI system. Or. en"
    },
    {
        "uuid":"6b37cac2-85af-4b56-bf59-abaec58cd3a1",
        "amendment_number":134,
        "author":"IMCO-LIBE",
        "title":"Article 28 – paragraph 1 – point c a (new)",
        "text":"(ca) they make a substantial modification to an AI system in such manner that the AI system becomes a high risk AI system; Or. en"
    },
    {
        "uuid":"551c7aae-e87a-426c-9af1-f567be0fc9c5",
        "amendment_number":135,
        "author":"IMCO-LIBE",
        "title":"Article 28 – paragraph 2",
        "text":"2. Where the circumstances referred to in paragraph 1, point (b) or (c), occur, the provider that initially placed the high- risk AI system on the market or put it into service shall no longer be considered a provider for the purposes of this Regulation. 2. Where the circumstances referred to in paragraph 1, point (b), (ba), (c) or (ca), occur, the provider that initially placed the high-risk AI system on the market or put it into service shall no longer be considered a provider for the purposes of this Regulation. Or. en"
    },
    {
        "uuid":"6fd6bdff-7949-455f-a7b8-19f0f9d2bb9a",
        "amendment_number":136,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 1 a (new)",
        "text":"1a. Where relevant, users of high-risk AI systems shall comply with the human oversight requirements laid down in this Regulation. Or. en"
    },
    {
        "uuid":"dfef472a-d990-494b-994d-7d62175016ac",
        "amendment_number":137,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 1 b (new)",
        "text":"1b. Users of high-risk AI systems shall ensure that natural persons assigned to ensure human oversight for high-risk AI systems are competent, properly qualified and trained and have the necessary resources in order to ensure the effective supervision of the system in accordance with Article 14; Or. en"
    },
    {
        "uuid":"2d8f3eef-d7aa-4d78-b31d-1698b50fdea4",
        "amendment_number":138,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 1 c (new)",
        "text":"1c. Users of high-risk AI systems shall ensure that the natural persons entrusted with the human oversight of the high-risk AI are competent, properly qualified and trained and have the necessary resources in order to ensure the effective supervision of the AI system in accordance with Article 14. Or. en"
    },
    {
        "uuid":"bb9135be-9a71-4192-b0ec-deb69a6bb5eb",
        "amendment_number":139,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 2",
        "text":"2. The obligations in paragraph 1 are without prejudice to other user obligations under Union or national law and to the user’s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. 2. The obligations in paragraphs 1, 1a and 1b are without prejudice to other user obligations under Union or national law and to the user’s discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider. Or. en"
    },
    {
        "uuid":"cdacf578-8160-47bc-9d92-4ae53b8fb6b5",
        "amendment_number":140,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 3",
        "text":"3. Without prejudice to paragraph 1, to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system. 3. Without prejudice to paragraph 1, 1a and 1b to the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system. Or. en"
    },
    {
        "uuid":"d2276efe-6bf0-4243-9c06-5de3b727e3df",
        "amendment_number":141,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 4 – subparagraph 1",
        "text":"Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use. When they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall inform the provider or distributor and suspend the use of the system. They shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system. In case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis. Users shall monitor the operation of the high-risk AI system on the basis of the instructions of use. When they have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1) they shall immediately inform the provider or distributor and suspend the use of the system. They shall also immediately inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system. In case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis. Or. en"
    },
    {
        "uuid":"638a92c5-3963-435f-b118-a20d7c8dfde5",
        "amendment_number":142,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 5 – subparagraph 1",
        "text":"Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control. The logs shall be kept for a period that is appropriate in the light of the intended purpose of the high- risk AI system and applicable legal obligations under Union or national law. Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent that such logs are under their control and are required for ensuring and demonstrating compliance with this Regulation, forex- post audits of any reasonably foreseeable malfunction, incidents or misuses of the system, or for ensuring and monitoring for the proper functioning of the system throughout its lifecycle. The logs shall be kept for a period that is appropriate in light of the intended purpose of the high-risk AI system and applicable legal obligations under Union or national law. Or. en"
    },
    {
        "uuid":"bc209a6e-6d4f-4c29-90fa-6c49591cb562",
        "amendment_number":143,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 5 a (new)",
        "text":"5a. Users of high-risk AI systems that are public authorities or Union institutions, bodies, offices and agencies shall comply with the registration obligations referred to in Article 51. Or. en"
    },
    {
        "uuid":"7504cd51-c435-4900-9519-7d4d40eb1e9e",
        "amendment_number":144,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 6",
        "text":"6. Users of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016\/679 or Article 27 of Directive (EU) 2016\/680, where applicable. 6. Where applicable, users of high- risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016\/679 or Article 27 of Directive (EU) 2016\/680 having regard to the technical characteristic of the system, the specific use and the specific context in which the AI system is intended to operate. Or. en"
    },
    {
        "uuid":"24a26f42-317f-4d99-9107-cbda9cb844d1",
        "amendment_number":145,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 6 a (new)",
        "text":"6a. Users of high-risk AI systems referred to in Annex III, which make decisions or assist in making decisions related to natural persons, shall inform the natural persons that they are subject to the use of the high-risk AI system. Or. en"
    },
    {
        "uuid":"5964da1a-dc99-43b6-ae1e-27bb75801838",
        "amendment_number":146,
        "author":"IMCO-LIBE",
        "title":"Article 29 – paragraph 6 b (new)",
        "text":"6b. Users of AI systems that generate, on the basis of limited human input, complex text content, such as news articles, opinion articles, novels, scripts, and scientific articles, shall disclose that the text content has been artificially generated or manipulated, including to the natural persons who are exposed to the content, each time they are exposed, in a clear and intelligible manner. Or. en"
    },
    {
        "uuid":"37a48028-1ef2-4c7d-a7fb-9d75af532aad",
        "amendment_number":147,
        "author":"IMCO-LIBE",
        "title":"Article 29 a (new)",
        "text":"Article 29a Notification Member States shall notify the Commission and the other Member States of conformity assessment bodies. Or. en (Article 29a is inserted in Chapter 4 before Article 30)"
    },
    {
        "uuid":"774b5087-26f5-435f-a78d-680fe31c1991",
        "amendment_number":148,
        "author":"IMCO-LIBE",
        "title":"Article 32 – paragraph 1",
        "text":"1. Notifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33. 1. Notifying authorities shall notify only conformity assessment bodies which have satisfied the requirements laid down in Article 33. Or. en"
    },
    {
        "uuid":"f98c4eba-d821-485a-bdab-dc4f66a1ffed",
        "amendment_number":149,
        "author":"IMCO-LIBE",
        "title":"Article 32 – paragraph 2",
        "text":"2. Notifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission. 2. Notifying authorities shall notify the Commission and the other Member States using the electronic notification tool developed and managed by the Commission of each conformity assessment body referred to in paragraph 1. Or. en"
    },
    {
        "uuid":"63b9deb4-a225-463b-bee0-f64d8d2f3cdd",
        "amendment_number":150,
        "author":"IMCO-LIBE",
        "title":"Article 32 – paragraph 3",
        "text":"3. The notification shall include full details of the conformity assessment 3. The notification referred to in paragraph 2 shall include full details of the activities, the conformity assessment module or modules and the artificial intelligence technologies concerned. conformity assessment activities, the conformity assessment module or modules and the artificial intelligence technologies concerned. Or. en"
    },
    {
        "uuid":"c167a34d-2449-4eb1-a47a-198b80b8b8cf",
        "amendment_number":151,
        "author":"IMCO-LIBE",
        "title":"Article 33 – paragraph 7",
        "text":"7. Notified bodies shall have procedures for the performance of activities which take due account of the size of an undertaking, the sector in which it operates, its structure, the degree of complexity of the AI system in question. 7. Notified bodies shall have procedures for the performance of activities which take due account of the size of an undertaking, the sector in which it operates, its structure, the degree of complexity of the AI system in question. Those procedures shall nevertheless respect the degree of rigour and ensure the level of protection required for the compliance of the AI system with the requirements laid down in this Regulation. Or. en Justification Reflecting Decision No 768\/2008\/EC of the European Parliament and of the Council on a common framework for the marketing of products, Article R27."
    },
    {
        "uuid":"f0d71589-374a-442b-bc3a-639cf197a737",
        "amendment_number":152,
        "author":"IMCO-LIBE",
        "title":"Article 34 – paragraph 3",
        "text":"3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. 3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified bodies shall make a list of their subsidiaries publicly available. Or. en"
    },
    {
        "uuid":"1dc540f2-df5a-4765-87bc-f73da1e1414b",
        "amendment_number":153,
        "author":"IMCO-LIBE",
        "title":"Article 34 – paragraph 4",
        "text":"4. Notified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation. 4. Notified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the verification of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation. Or. en"
    },
    {
        "uuid":"75f92854-29f6-40d3-9eaa-b106b0be13a5",
        "amendment_number":154,
        "author":"IMCO-LIBE",
        "title":"Article 35 – title",
        "text":"Identification numbers and lists of notified bodies designated under this Regulation Identification numbers and lists of notified bodies Or. en"
    },
    {
        "uuid":"501ff2ce-8f3b-4ff9-90a9-54dd4ef2d8d4",
        "amendment_number":155,
        "author":"IMCO-LIBE",
        "title":"Article 36 – paragraph 2",
        "text":"2. In the event of restriction, suspension or withdrawal of notification, or where the notified body has ceased its 2. In the event of restriction, suspension or withdrawal of notification, or where the notified body has ceased its activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities at their request. activity, the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities, and market surveillance authority at their request. Or. en"
    },
    {
        "uuid":"18208050-65cb-4e1d-8695-adbe5e557566",
        "amendment_number":156,
        "author":"IMCO-LIBE",
        "title":"Article 37 – paragraph 1",
        "text":"1. The Commission shall, where necessary, investigate all cases where there are reasons to doubt whether a notified body complies with the requirements laid down in Article 33. 1. The Commission shall investigate all cases where there are reasons to doubt the competence of a notified body or the continued fulfilment by a notified body of the applicable requirements and responsibilities. Or. en"
    },
    {
        "uuid":"a809ab95-48dd-491f-adb7-49156b5472c9",
        "amendment_number":157,
        "author":"IMCO-LIBE",
        "title":"Article 37 – paragraph 2",
        "text":"2. The Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification of the notified body concerned. 2. The Notifying authority shall provide the Commission, on request, with all relevant information relating to the notification or the maintenance of the notified body concerned. Or. en"
    },
    {
        "uuid":"35ef88d8-2763-4aa5-a3d1-176c6199d322",
        "amendment_number":158,
        "author":"IMCO-LIBE",
        "title":"Article 37 – paragraph 4",
        "text":"4. Where the Commission ascertains that a notified body does not meet or no longer meets the requirements laid down in Article 33, it shall adopt a reasoned decision requesting the notifying Member State to take the necessary corrective measures, including withdrawal of notification if necessary. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 74(2). 4. Where the Commission ascertains that a notified body does not meet or no longer meets the requirements for its notification, it shall adopt an implementing act requesting the notifying Member State to take the necessary corrective measures, including withdrawal of notification if necessary. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 74(2). Or. en"
    },
    {
        "uuid":"202722a8-5a64-433f-9caf-0029701b17d9",
        "amendment_number":159,
        "author":"IMCO-LIBE",
        "title":"Article 39 a (new)",
        "text":"Article 39a Exchange of knowhow and best practices The Commission shall provide for the exchange of knowhow and best practices between the Member States' national authorities responsible for notification policy. Or. en"
    },
    {
        "uuid":"0eaae4d9-0326-4429-936d-7ce4a200621d",
        "amendment_number":160,
        "author":"IMCO-LIBE",
        "title":"Article 40 – paragraph 1 a (new)",
        "text":"The standardisation process shall ensure a balanced representation of interests and effective participation of all relevant stakeholders in accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025\/2012. Or. en"
    },
    {
        "uuid":"8df2196d-e80d-4f87-ab44-a4ca0f638241",
        "amendment_number":161,
        "author":"IMCO-LIBE",
        "title":"Article 41 – paragraph 2",
        "text":"2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law. 2. The Commission, when preparing the common specifications referred to in paragraph 1, shall gather the views of relevant bodies or expert groups established under relevant sectorial Union law, as well as other relevant stakeholders. Or. en"
    },
    {
        "uuid":"a066ea72-4c0a-4bac-aa86-7e21c8977901",
        "amendment_number":162,
        "author":"IMCO-LIBE",
        "title":"Article 44 – paragraph 2",
        "text":"2. Certificates shall be valid for the period they indicate, which shall not exceed five years. On application by the provider, the validity of a certificate may be extended for further periods, each not exceeding five years, based on a re- assessment in accordance with the applicable conformity assessment procedures. 2. Certificates shall be valid for the period they indicate, which shall not exceed four years. On application by the provider, the validity of a certificate may be extended for further periods, each not exceeding four years, based on a re- assessment in accordance with the applicable conformity assessment procedures. Any supplementary certificate shall remain valid for the same duration as the certificate which it supplements. Or. en"
    },
    {
        "uuid":"a86aad5e-1e33-4e58-8d10-1934e81bb628",
        "amendment_number":163,
        "author":"IMCO-LIBE",
        "title":"Article 45 – paragraph 1",
        "text":"Member States shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision. Member States shall ensure that an appeal procedure against decisions of the notified bodies, including on issued conformity certificates, is available to parties having a legitimate interest in that decision Or. en"
    },
    {
        "uuid":"cc7d7698-b17a-49e1-b42b-e25be3ff4f29",
        "amendment_number":164,
        "author":"IMCO-LIBE",
        "title":"Article 47",
        "text":"[...] deleted Or. en Justification The public interest reasons indicated in the Article do not justify a derogation from the conformity assessment for urgency reasons. To the contrary, putting into service a high-risk AI system intended to address the indicated concerns without performing a conformity assessment risks aggravating those concerns, if the system is biased, inaccurate, or exposed to vulnerabilities."
    },
    {
        "uuid":"1a08a9b7-41c0-47d9-806a-e2331f85662c",
        "amendment_number":165,
        "author":"IMCO-LIBE",
        "title":"Article 48 – paragraph 1",
        "text":"1. The provider shall draw up a written EU declaration of conformity for each AI system and keep it at the disposal of the national competent authorities for 10 years after the AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the AI system for which it has been drawn up. A copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request. 1. The provider shall draw up a written EU declaration of conformity for each high-risk AI system and keep it at the disposal of the national competent authorities for 10 years after the high-risk AI system has been placed on the market or put into service. A copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request. Or. en Justification The declaration of conformity already contains the identification of the high-risk AI system pursuant to Annex V."
    },
    {
        "uuid":"e2d17e4e-ca9e-49c0-bddb-82ed9a4f14d1",
        "amendment_number":166,
        "author":"IMCO-LIBE",
        "title":"Article 48 – paragraph 2",
        "text":"2. The EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title. The EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which the high-risk AI system is made available. 2. The EU declaration of conformity shall state that the high-risk AI system in question meets the requirements set out in Chapter 2 of this Title, including the requirements related to the respect of the Union data protection law. The EU declaration of conformity shall contain the information set out in Annex V and shall be translated into an official Union language or languages required by the Member State(s) in which the high-risk AI system is placed on the market or made available. Or. en"
    },
    {
        "uuid":"10b7e4dd-1cc8-4551-bd0f-6d0c1bdedcf2",
        "amendment_number":167,
        "author":"IMCO-LIBE",
        "title":"Article 48 – paragraph 3",
        "text":"3. Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union legislations applicable to the high-risk AI system. The declaration shall contain all the information required for identification of the Union harmonisation legislation to which the declaration relates. 3. Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity can be drawn up in respect of all Union legislations applicable to the high-risk AI system. The declaration shall contain all the information required for identification of the Union harmonisation legislation to which the declaration relates. Or. en"
    },
    {
        "uuid":"15e66244-4ca8-47da-996b-44d8b51357c3",
        "amendment_number":168,
        "author":"IMCO-LIBE",
        "title":"Article 49 – paragraph 1",
        "text":"1. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate. 1. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems before the high-risk AI system is placed on the market. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate. Or. en"
    },
    {
        "uuid":"285c5216-73c1-4a2a-a13f-0fa68a8af955",
        "amendment_number":169,
        "author":"IMCO-LIBE",
        "title":"Article 49 – paragraph 3",
        "text":"3. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking. 3. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider or the provider’s authorised representative. The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking. Or. en"
    },
    {
        "uuid":"5558f17d-6cb7-44ce-88f5-d4c1ed5f2895",
        "amendment_number":170,
        "author":"IMCO-LIBE",
        "title":"Article 49 – paragraph 3 a (new)",
        "text":"3a. The CE marking shall be affixed only after assessment of the compliance with Union data protection law. Or. en"
    },
    {
        "uuid":"d43f6924-b956-4168-8bfc-98c128e6939f",
        "amendment_number":171,
        "author":"IMCO-LIBE",
        "title":"Article 51 – paragraph 1",
        "text":"Before placing on the market or putting into service a high-risk AI system referred to in Article 6(2), the provider or, where applicable, the authorised representative shall register that system in the EU Before placing on the market or putting into service a high-risk AI system referred to in Article 6(2), the provider or, where applicable, the authorised representative shall register that system in the EU database referred to in Article 60. database referred to in Article 60, in accordance with Article 60(2). Or. en"
    },
    {
        "uuid":"48a9c1a9-3e24-4378-899a-506e8610cc31",
        "amendment_number":172,
        "author":"IMCO-LIBE",
        "title":"Article 51 – paragraph 1 a (new)",
        "text":"Before putting into service or using a high-risk AI system in accordance with Article 6(2), users who are public authorities or Union institutions, bodies, offices or agencies or users acting on their behalf shall register in the EU database referred to in Article 60. Or. en"
    },
    {
        "uuid":"8254d17d-be19-415a-9435-341c67a1b21f",
        "amendment_number":173,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 1",
        "text":"1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox. 1. AI regulatory sandboxes established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan. This shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised. Or. en"
    },
    {
        "uuid":"62f70cc3-852a-4ff0-8310-094e6fc3767b",
        "amendment_number":174,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 1 a (new)",
        "text":"1a. The AI regulatory sandbox shall allow and facilitate the involvement of notified bodies, standardisation bodies, and other relevant stakeholders when relevant. Or. en"
    },
    {
        "uuid":"aeeb48da-72d4-42d4-8209-9745b9521aaa",
        "amendment_number":175,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 2",
        "text":"2. Member States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox. 2. Member States shall ensure that to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to personal data, the national data protection authorities and those other national authorities are associated to the operation of the AI regulatory sandbox. Or. en"
    },
    {
        "uuid":"4ba1d5ef-e6c7-4ba0-901a-21506a8a67a3",
        "amendment_number":176,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 3",
        "text":"3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place. 3. The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate and adequate mitigation. Where such mitigation proves to be ineffective, the development and testing process shall be suspended without delay until such mitigation takes place. Or. en"
    },
    {
        "uuid":"cd330a5a-2c99-47cc-9a5a-0e5bd3d8d285",
        "amendment_number":177,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 4",
        "text":"4. Participants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox. 4. Participants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox. Or. en"
    },
    {
        "uuid":"940acb37-305e-4333-9fe3-86ec88f4f7e2",
        "amendment_number":178,
        "author":"IMCO-LIBE",
        "title":"Article 53 – paragraph 5",
        "text":"5. Member States’ competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the 5. The national competent authorities shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board on framework of the European Artificial Intelligence Board. They shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. AI regulatory sandboxes. They shall submit annual reports to the Board and the Commission on the results of the implementation of those scheme, including good practices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox. Those reports or abstracts thereof shall be made available to the public in order to further enable innovation in the Union. Or. en"
    },
    {
        "uuid":"2c73663b-fe86-4466-bb9a-62addae8aad1",
        "amendment_number":179,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 1",
        "text":"1. A ‘European Artificial Intelligence Board’ (the ‘Board’) is established. 1. An independent ‘European Artificial Intelligence Board’ (the ‘Board’) shall be established. Or. en"
    },
    {
        "uuid":"41e0674e-dbc8-4768-93b2-9329e9c90c0c",
        "amendment_number":180,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 2 – introductory part",
        "text":"2. The Board shall provide advice and assistance to the Commission in order to: 2. The Board shall provide advice and assistance to the Commission and to the national supervisory authorities in order to: Or. en"
    },
    {
        "uuid":"dc379c10-af5c-4712-9184-ccfeea2f0688",
        "amendment_number":181,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 2 – point b",
        "text":"(b) coordinate and contribute to guidance and analysis by the Commission and the national supervisory authorities and other competent authorities on emerging issues across the internal market with regard to matters covered by this Regulation; (b) coordinate and provide guidance and analysis to the Commission and to the national supervisory authorities and other competent authorities on emerging issues across the internal market with regard to matters covered by this Regulation; Or. en"
    },
    {
        "uuid":"2623692b-7841-4b11-826b-d73d350ee6f0",
        "amendment_number":182,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 2 – point c",
        "text":"(c) assist the national supervisory authorities and the Commission in ensuring the consistent application of this Regulation. (c) contribute to the effective and consistent application of this Regulation and assist the national supervisory authorities and the Commission in this regard. Or. en"
    },
    {
        "uuid":"08793178-c85b-49c0-8a62-f6d83f935b00",
        "amendment_number":183,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 2 – point c a (new)",
        "text":"(ca) contribute to the effective cooperation with the competent authorities of third countries and with international organisations. Or. en"
    },
    {
        "uuid":"5747f220-b7c8-44dd-ad67-db96282d5d7e",
        "amendment_number":184,
        "author":"IMCO-LIBE",
        "title":"Article 56 – paragraph 2 a (new)",
        "text":"2a. The Board shall contribute to the effective and consistent enforcement of this Regulation throughout the Union, including with regard to cases involving two or more Member States as set out in Article 59b. Or. en"
    },
    {
        "uuid":"50e436ef-b1a1-4e12-8386-cbef95a4eb93",
        "amendment_number":185,
        "author":"IMCO-LIBE",
        "title":"Article 57 – title",
        "text":"Structure of the Board Structure and independence of the Board Or. en"
    },
    {
        "uuid":"4705f407-a0e1-4415-8916-1ef3b629e313",
        "amendment_number":186,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 1",
        "text":"1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, and the European Data Protection Supervisor. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. 1. The Board shall be composed of the national supervisory authorities, who shall be represented by the head or equivalent high-level official of that authority, the European Data Protection Supervisor and the FRA. Other national authorities may be invited to the meetings, where the issues discussed are of relevance for them. Each Member State shall have one vote. The EDPS and FRA shall not have voting rights. Or. en"
    },
    {
        "uuid":"0f04d11f-71e3-4db6-b2d7-cc2a322145aa",
        "amendment_number":187,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 1 a (new)",
        "text":"1a. The Board shall act independently when performing its tasks or exercising its powers. Or. en"
    },
    {
        "uuid":"c953c973-0f08-43ab-ab19-4fae87ef7008",
        "amendment_number":188,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 1 b (new)",
        "text":"1b. The Board shall take decisions by a simple majority of its voting members, unless otherwise provided for in this Regulation. Or. en"
    },
    {
        "uuid":"db55c516-6003-4bfb-a594-b596f5647233",
        "amendment_number":189,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 2",
        "text":"2. The Board shall adopt its rules of procedure by a simple majority of its members, following the consent of the Commission. The rules of procedure shall also contain the operational aspects related 2. The Board shall adopt its rules of procedure by a two-thirds majority of its voting members. The rules of procedure shall also contain the operational aspects related to the execution of the Board’s to the execution of the Board’s tasks as listed in Article 58. The Board may establish sub-groups as appropriate for the purpose of examining specific questions. tasks as listed in Article 58. Or. en"
    },
    {
        "uuid":"35b0f2ca-d2af-4127-8bf5-8a5d2a6f364e",
        "amendment_number":190,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 2 a (new)",
        "text":"2a. The Board may establish sub- groups as appropriate for the purpose of examining specific questions. Or. en"
    },
    {
        "uuid":"42c55b07-bd28-4f69-a99c-6ad7e80688ef",
        "amendment_number":191,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 2 b (new)",
        "text":"2b. The Board shall be represented by its Chair. Or. en"
    },
    {
        "uuid":"96e36797-2fe1-4077-8e44-bdac60c6d128",
        "amendment_number":192,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 2 c (new)",
        "text":"2c. The Board shall elect a Chair and two deputy Chairs from among its voting members by simple majority. The term of office of the Chair and of the deputy Chairs shall be three years. The terms of the Chair and of the deputy Chairs may be renewed once. Or. en"
    },
    {
        "uuid":"c14253ba-8440-4d6c-87da-04ac04120b9f",
        "amendment_number":193,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 3",
        "text":"3. The Board shall be chaired by the Commission. The Commission shall convene the meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation. 3. The Chair shall have the following tasks: - convene the meetings of the Board; - prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and with its rules of procedure; - ensure the timely performance of the tasks of the Board; - notify Member States and the Commission of recommendations adopted by the Board. The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation. Or. en"
    },
    {
        "uuid":"e67ac55e-8aaa-4233-a862-c6b6494355d4",
        "amendment_number":194,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 3 a (new)",
        "text":"3a. The meetings of the Board shall be considered to be quorate where at least two-thirds of its members are present. Or. en"
    },
    {
        "uuid":"49e514be-585d-4e32-941c-d8a3588be0d9",
        "amendment_number":195,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 3 b (new)",
        "text":"3b. The secretariat of the Board shall have the necessary human and financial resources to be able to perform its tasks pursuant to this Regulation. Or. en"
    },
    {
        "uuid":"692696f7-0a10-4431-8f0d-e28861f45cdf",
        "amendment_number":196,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 3 c (new)",
        "text":"3c. The Board shall organise consultations with stakeholders twice a year. Such stakeholders shall include representatives from industry, start-ups and SMEs, civil society organisations, such as NGOs, consumer associations, the social partners and academia, to assess the evolution of trends in technology, issues related to the implementation and the effectiveness of this Regulation, regulatory gaps or loopholes observed in practice. Or. en"
    },
    {
        "uuid":"035c0e76-d2fe-4361-9982-ec29cc61a694",
        "amendment_number":197,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 4",
        "text":"4. The Board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent. To that end the Commission may facilitate exchanges between the Board and other Union bodies, offices, agencies and advisory groups. 4. The Board may invite national authorities, such as national equality bodies, to its meetings, where the issues discussed are of relevance for them. The Board may also invite, where appropriate, external experts, observers and interested third parties, including stakeholders such as those referred to in paragraph 3c, to attend its meetings and hold exchanges with them. Or. en"
    },
    {
        "uuid":"a975bb2f-7af5-434f-92db-f27b0ddd110e",
        "amendment_number":198,
        "author":"IMCO-LIBE",
        "title":"Article 57 – paragraph 4 a (new)",
        "text":"4a. The Board shall cooperate with Union institutions, bodies, offices, agencies and advisory groups and shall make the results of that cooperation publicly available. Or. en"
    },
    {
        "uuid":"1c9163c2-1770-408a-b200-bce9529f05b9",
        "amendment_number":199,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – introductory part",
        "text":"When providing advice and assistance to the Commission in the context of Article When providing advice and assistance to the Commission and the national 56(2), the Board shall in particular: supervisory authorities in the context of Article 56(2), the Board shall in particular: Or. en"
    },
    {
        "uuid":"66aa2d21-6cd7-4652-b3d9-221fe7bda4bb",
        "amendment_number":200,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point a a (new)",
        "text":"(aa) issue opinions, recommendations or written contributions with a view to ensuring the consistent implementation of this Regulation; Or. en"
    },
    {
        "uuid":"3bbe6cbf-05bd-46dd-9c4f-573586e5582b",
        "amendment_number":201,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point a b (new)",
        "text":"(ab) examine, on its own initiative or on request of one of its members, any question covering the application of this Regulation and issue guidelines, recommendations and best practices with a view to ensuring the consistent implementation of this Regulation; Or. en"
    },
    {
        "uuid":"7a43140e-3708-4a8b-a0cf-c2c591103e5d",
        "amendment_number":202,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c a (new)",
        "text":"(ca) encourage, facilitate and support the drawing up of codes of conduct intended to foster the voluntary application to AI systems of those codes of conduct in close cooperation with relevant stakeholders in accordance with Article 69; Or. en"
    },
    {
        "uuid":"2bec2277-243d-41c5-83b8-0a6430ba4958",
        "amendment_number":203,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c b (new)",
        "text":"(cb) cooperate with the European Data Protection Board and with the FRA to provide guidance in relation to the respect of fundamental rights, in particular the right to non-discrimination and to equal treatment, the right to privacy and the protection of personal data; Or. en"
    },
    {
        "uuid":"bc203340-42fa-47e5-85c3-56fe2fc3119b",
        "amendment_number":204,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c c (new)",
        "text":"(cc) promote public awareness and understanding of the benefits, risks, rules and safeguards and rights in relation to the use of AI systems; Or. en"
    },
    {
        "uuid":"5aaee44e-02b3-45ce-9b0c-1e7c43a6a29b",
        "amendment_number":205,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c d (new)",
        "text":"(cd) promote the cooperation and effective bilateral and multilateral exchange of information and best practices between the national supervisory authorities; Or. en"
    },
    {
        "uuid":"a2395f03-b025-4883-a04d-6c7bde21fb5d",
        "amendment_number":206,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c e (new)",
        "text":"(ce) advise the Commission on the possible amendment of the Annexes by means of delegated act in accordance with Article 73, in particular the annex listing high-risk AI systems; Or. en"
    },
    {
        "uuid":"4583fefc-0485-4600-93f8-204cea4e50f3",
        "amendment_number":207,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c f (new)",
        "text":"(cf) ensure that the national supervisory authorities actively cooperate in the implementation of this Regulation; Or. en"
    },
    {
        "uuid":"8337d475-1f72-41b3-a7b7-e31b6e9e2c39",
        "amendment_number":208,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 – point c g (new)",
        "text":"(cg) provide guidance in relation to children’s rights, applicable law and minimum standards to meet the objectives of this Regulation that pertain to children. Or. en"
    },
    {
        "uuid":"4a20cb79-ade4-4413-8c2a-43367d809475",
        "amendment_number":209,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 a (new)",
        "text":"1a. When acting in the context of Article 59a on cases involving two or more Member States, the Board shall adopt recommendations for national supervisory authorities. Or. en"
    },
    {
        "uuid":"49f7314a-6806-4ca2-bb15-0df09582131c",
        "amendment_number":210,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 b (new)",
        "text":"1b. The Board shall refer to the Commission any cases referred to in Article 68a of which it becomes aware. Or. en"
    },
    {
        "uuid":"559a912a-1ec1-4f35-980e-a74920c9d89c",
        "amendment_number":211,
        "author":"IMCO-LIBE",
        "title":"Article 58 – paragraph 1 c (new)",
        "text":"1c. The Board shall draw up an annual report regarding its activities. The report shall be made public and be transmitted to the European Parliament, to the Council and to the Commission in all official languages of the Union. In particular, the annual report shall include information with regard to: (a) serious incidents and malfunctioning reported in accordance with Article 62; (b) serious cases of misuse of high- risk AI systems or cases of use of prohibited practices in accordance with Article 64; (c) the fines issued pursuant to this Regulation in accordance with Articles 71 and 72; (d) the possible cases involving two or more Member States and any recommendations issued pursuant with Article 59a; (e) the practical application of and possible follow-up to the opinions, guidelines, recommendations, advice and other measures taken under paragraph 1. Or. en"
    },
    {
        "uuid":"9130e2be-511a-49a5-a7bd-ceaa4acd8dca",
        "amendment_number":212,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 2",
        "text":"2. Each Member State shall designate a national supervisory authority among the 2. Each Member State shall designate a single national supervisory authority national competent authorities. The national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority. among the national competent authorities by... [3 months after the entry into force of this Regulation]. The national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority, in which case, it shall provide reasons to the Commission and the Board for doing so. Where it is not the designated supervisory authority, the national supervisory authority shall act in close cooperation with the national data protection authority. Or. en"
    },
    {
        "uuid":"9dc52b2b-a7db-45f3-b5bb-7079526b9807",
        "amendment_number":213,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 2 a (new)",
        "text":"2a. The national supervisory authority shall act as lead authority and be responsible for ensuring the effective coordination between national competent authorities regarding the implementation of this Regulation and shall contribute to the effective and consistent application and enforcement of this Regulation. It shall represent its Member State on the Board, in accordance with Article 57. Or. en"
    },
    {
        "uuid":"96b9921c-4cc2-4930-9635-19ae0e5343d0",
        "amendment_number":214,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 2 b (new)",
        "text":"2b. Each national supervisory authority shall act independently in performing its tasks and exercising its powers in accordance with this Regulation. The member or members of each national supervisory authority shall, in the performance of their tasks and exercise of their powers in accordance with this Regulation, remain free from external influence, whether direct or indirect, and shall not seek or take instructions from any other body in relation to the exercise of the tasks assigned to them. Or. en"
    },
    {
        "uuid":"ca3fecd1-06a4-4610-81f4-2dc5174c8dba",
        "amendment_number":215,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 3",
        "text":"3. Member States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority. 3. Member States shall make publicly available and communicate to the Commission and the Board the name of their national competent authority which has been designated as national supervisory Authority and information on how it can be contacted, by ...[three months after the entry into force of this Regulation]. Or. en"
    },
    {
        "uuid":"1eeea5ec-f88d-4826-b417-d836467bf7d3",
        "amendment_number":216,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 4",
        "text":"4. Member States shall ensure that national competent authorities are provided with adequate financial and human 4. Member States shall ensure that national competent authorities are provided with adequate technical, financial and resources to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. human resources, premises and infrastructure necessary to fulfil their tasks under this Regulation. In particular, national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, personal data protection, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. Member States shall assess and update competence and resource requirements referred to in this paragraph on an annual basis. Or. en"
    },
    {
        "uuid":"e6439144-2120-4ad3-aa2b-f8d783b79e08",
        "amendment_number":217,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 6",
        "text":"6. The Commission shall facilitate the exchange of experience between national competent authorities. 6. The Commission and the Board shall facilitate the exchange of experience between national competent authorities. Or. en"
    },
    {
        "uuid":"f9cca84f-770f-419f-bc67-3175c8d8c377",
        "amendment_number":218,
        "author":"IMCO-LIBE",
        "title":"Article 59 – paragraph 7",
        "text":"7. National competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers. Whenever national competent authorities intend to provide guidance and advice with 7. National competent authorities may provide guidance and advice on the implementation of this Regulation, including to small-scale providers. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate. Member States may also establish one central contact point for communication with operators. regard to an AI system in areas covered by other Union legislation, the competent national authorities under that Union legislation shall be consulted, as appropriate. Member States shall also establish one central contact point for communication with operators and other stakeholders. Or. en"
    },
    {
        "uuid":"691fc7c8-ed9e-481f-8ee1-c472d00931be",
        "amendment_number":219,
        "author":"IMCO-LIBE",
        "title":"Article 59 a (new)",
        "text":"Article 59a Cooperation mechanism between national supervisory authorities in cases involving two or more Member States 1. Each national supervisory authority shall perform its tasks and powers conferred on in accordance with this Regulation on the territory of its own Member State. 2. In the event of a case involving two or more national supervisory authorities, the national supervisory authority of the Member State where the provider or the user of the concerned AI system is established or where the authorised representative is appointed shall be considered to be the lead national supervisory authority. 3. In the cases referred to in paragraph 2, the relevant national supervisory authorities shall cooperate and exchange all relevant information in due time. National supervisory authorities shall cooperate in order to reach a consensus. 4. In the case of a serious disagreement between two or more national supervisory authorities, the national supervisory authorities shall notify the Board and communicate without delay all relevant information related to the case to the Board. 5. The Board shall, within three months of receipt of the notification referred to in paragraph 4, issue a recommendation to the national supervisory authorities. Or. en"
    },
    {
        "uuid":"26536fff-23e9-47a3-82b9-f4d1c4c483a3",
        "amendment_number":220,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 1",
        "text":"1. The Commission shall, in collaboration with the Member States, set up and maintain a EU database containing information referred to in paragraph 2 concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Article 51. 1. The Commission shall, in collaboration with the Member States, set up and maintain a EU database containing information referred to in paragraphs 2 and 2a concerning high-risk AI systems referred to in Article 6(2) which are registered in accordance with Article 51 and users of such systems by public authorities and Union institutions, bodies, offices or agencies. Or. en"
    },
    {
        "uuid":"2c73f183-7e72-4076-912b-ceab09c986c7",
        "amendment_number":221,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 2",
        "text":"2. The data listed in Annex VIII shall be entered into the EU database by the providers. The Commission shall provide them with technical and administrative 2. The data listed in Annex VIII, point (1), shall be entered into the EU database by the providers. The Commission shall provide them with technical and support. administrative support. Or. en"
    },
    {
        "uuid":"bd1ce6b4-5f8f-4da8-a912-eac12b2feb2f",
        "amendment_number":222,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 2 a (new)",
        "text":"2a. The data listed in Annex VIII, point (2), shall be entered into the EU database by the users who are or who act on behalf of public authorities or Union institutions, bodies, offices or agencies. The Commission shall provide them with technical and administrative support. Or. en"
    },
    {
        "uuid":"db41a188-4349-46bc-b8cb-ff985bdd4336",
        "amendment_number":223,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 3",
        "text":"3. Information contained in the EU database shall be accessible to the public. 3. Information contained in the EU database shall be accessible to the public, user-friendly and accessible, easily navigable and machine-readable. Or. en"
    },
    {
        "uuid":"f1421b65-a9cc-4f3c-a191-b3c115b822e8",
        "amendment_number":224,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 4",
        "text":"4. The EU database shall contain 4. The EU database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this Regulation. That information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider. personal data only insofar as necessary for collecting and processing information in accordance with this Regulation. That information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider or the user which is a public authority or Union institutions, bodies, offices or agencies. Or. en"
    },
    {
        "uuid":"033e0dad-531d-4b9c-9fe4-7def58dbff06",
        "amendment_number":225,
        "author":"IMCO-LIBE",
        "title":"Article 60 – paragraph 5",
        "text":"5. The Commission shall be the controller of the EU database. It shall also ensure to providers adequate technical and administrative support. 5. The Commission shall be the controller of the EU database. Or. en"
    },
    {
        "uuid":"f38ddba4-c93a-4c5b-97a5-42f24767afc9",
        "amendment_number":226,
        "author":"IMCO-LIBE",
        "title":"Article 61 – paragraph 3",
        "text":"3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan. 3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to be included in the plan by ... [12 months following the entry into force of this Regulation]. Or. en"
    },
    {
        "uuid":"67aa1efd-3409-4b79-af76-ce09744761ac",
        "amendment_number":227,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 1 – subparagraph 1",
        "text":"Providers of high-risk AI systems placed on the Union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to the market surveillance authorities of the Member States where that incident or breach occurred. Providers and, where users have identified a serious incident or malfunctioning, users of high-risk AI systems placed on the Union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to the market surveillance authorities of the Member States where that incident or breach occurred and, where relevant, to the Commission. Or. en"
    },
    {
        "uuid":"3b76f093-7d6d-4a31-aa0b-3be746e5bcb6",
        "amendment_number":228,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 1 – subparagraph 2",
        "text":"Such notification shall be made immediately after the provider has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the providers becomes aware of the serious incident or of the malfunctioning. Such notification shall be made immediately after the provider or where applicable the user has established a causal link between the AI system and the incident or malfunctioning or the reasonable likelihood of such a link, and, in any event, not later than 72 hours after the provider or, where applicable, the user becomes aware of the serious incident or of the malfunctioning. Or. en Justification the deadline has been shortened to 3 days to match the corresponding reporting obligations deadlines in the GDPR."
    },
    {
        "uuid":"6944890a-fca5-4d07-b287-901cc34d2def",
        "amendment_number":229,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 1 – subparagraph 2 a (new)",
        "text":"Upon establishing a causal link between the AI system and the serious incident or malfunctioning or the reasonable likelihood of such a link, providers shall take appropriate corrective actions pursuant to Article 21. Or. en"
    },
    {
        "uuid":"4c4ec9f7-0bbc-41bd-9b62-f8b61a608fb1",
        "amendment_number":230,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 2",
        "text":"2. Upon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the market surveillance authority shall inform the national public authorities or bodies referred to in Article 64(3). The Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1. That guidance shall be issued 12 months after the entry into force of this Regulation, at the latest. 2. Upon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, the market surveillance authority shall inform the national public authorities or bodies referred to in Article 64(3). The Commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1. That guidance shall be issued by ... [the entry into force of this Regulation] and shall be reviewed every year. Or. en"
    },
    {
        "uuid":"c60afd41-52c9-4d4d-9854-9d48d22b77b6",
        "amendment_number":231,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 3",
        "text":"3. For high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013\/36\/EU and for high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulation (EU) 2017\/745 and Regulation (EU) 2017\/746, the notification of serious incidents or malfunctioning shall be limited to those that that constitute a breach of obligations under Union law intended to protect fundamental rights. 3. For high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013\/36\/EU and for high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulation (EU) 2017\/745 and Regulation (EU) 2017\/746, the notification of serious incidents or malfunctioning for the purposes of this Regulation shall be limited to those that that constitute a breach of obligations under Union law intended to protect fundamental rights. Or. en"
    },
    {
        "uuid":"850d051c-85c7-4e7d-b52c-a43b9afe36e4",
        "amendment_number":232,
        "author":"IMCO-LIBE",
        "title":"Article 62 – paragraph 3 a (new)",
        "text":"3a. National supervisory authorities shall on an annual basis notify the Board of the serious incidents and malfunctioning reported to them in accordance with this Article. Or. en"
    },
    {
        "uuid":"5953a1da-a551-4a29-a0ec-a0647810ed2f",
        "amendment_number":233,
        "author":"IMCO-LIBE",
        "title":"Article 63 – paragraph 2",
        "text":"2. The national supervisory authority shall report to the Commission on a regular basis the outcomes of relevant market surveillance activities. The national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules. 2. The national supervisory authority shall report to the Commission annually the outcomes of relevant market surveillance activities. The national supervisory authority shall report, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules. Or. en"
    },
    {
        "uuid":"c3e96a04-fa31-44d9-9a83-67c71b535dd5",
        "amendment_number":234,
        "author":"IMCO-LIBE",
        "title":"Article 64 – paragraph 1",
        "text":"1. Access to data and documentation in the context of their activities, the market surveillance authorities shall be granted full access to the training, validation and testing datasets used by the provider, including through application programming interfaces (‘API’) or other appropriate technical means and tools enabling remote access. 1. In the context of their activities, the market surveillance authorities, or the Commission when acting pursuant to Article 68a shall be granted full access to the training, validation and testing datasets used by the provider or, where relevant, the user, including through application programming interfaces (‘API’) or other appropriate technical means and tools enabling remote access. Or. en"
    },
    {
        "uuid":"36eafd46-4f18-4732-9638-183e5013b66c",
        "amendment_number":235,
        "author":"IMCO-LIBE",
        "title":"Article 64 – paragraph 2",
        "text":"2. Where necessary to assess the 2. Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the market surveillance authorities shall be granted access to the source code of the AI system. conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2 and upon a reasoned request, the market surveillance authorities or, where applicable, the Commission shall be granted access to the source code of the AI system. Or. en"
    },
    {
        "uuid":"15251f4b-f685-468b-b821-f95211de49bf",
        "amendment_number":236,
        "author":"IMCO-LIBE",
        "title":"Article 64 – paragraph 3",
        "text":"3. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request. 3. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned or, where applicable, the Commission of any such request. Or. en"
    },
    {
        "uuid":"0f3ba068-0cc8-4185-8334-b9562245741a",
        "amendment_number":237,
        "author":"IMCO-LIBE",
        "title":"Article 64 – paragraph 5",
        "text":"5. Where the documentation referred 5. Where the documentation referred to in paragraph 3 is insufficient to ascertain whether a breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to paragraph 3 may make a reasoned request to the market surveillance authority to organise testing of the high- risk AI system through technical means. The market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request. to in paragraph 3 is insufficient to ascertain whether a breach of obligations under Union law intended to protect fundamental rights has occurred, the public authority or body referred to paragraph 3 may make a reasoned request to the market surveillance authority or, where applicable, to the Commission to organise testing of the high-risk AI system through technical means. The market surveillance authority or, where applicable, the Commission shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request. Or. en"
    },
    {
        "uuid":"1f6af2fc-b151-4e60-a7a8-7453e8dcef5e",
        "amendment_number":238,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 1",
        "text":"1. AI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019\/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned. 1. AI systems presenting a risk shall be understood as a product presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019\/1020 insofar as risks to the health or safety or to the protection of fundamental rights of persons or the Union values enshrined in Article 2 TEU are concerned. Or. en"
    },
    {
        "uuid":"19f121fb-3520-4702-8c9c-419a0ca6fc37",
        "amendment_number":239,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 2 – subparagraph 1",
        "text":"Where the market surveillance authority of a Member State has sufficient reasons to Where the market surveillance authority of a Member State has sufficient reasons to consider that an AI system presents a risk as referred to in paragraph 1, they shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation. When risks to the protection of fundamental rights are present, the market surveillance authority shall also inform the relevant national public authorities or bodies referred to in Article 64(3). The relevant operators shall cooperate as necessary with the market surveillance authorities and the other national public authorities or bodies referred to in Article 64(3). consider that an AI system presents a risk as referred to in paragraph 1, they shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation. When risks to the protection of fundamental rights are present, the market surveillance authority shall also immediately inform and fully cooperate with the relevant national public authorities or bodies referred to in Article 64(3). The relevant operators shall cooperate as necessary with the market surveillance authorities and the other national public authorities or bodies referred to in Article 64(3). Or. en"
    },
    {
        "uuid":"6ee56a9c-5851-4bcc-8165-6af7b397fcfa",
        "amendment_number":240,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 2 – subparagraph 1",
        "text":"Where, in the course of that evaluation, the market surveillance authority finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. Where, in the course of that evaluation, the market surveillance authority or, where relevant, the national public authority referred to in Article 64(3) finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. Or. en"
    },
    {
        "uuid":"3447065c-2367-4217-a8d1-c207d059b1af",
        "amendment_number":241,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 3",
        "text":"3. Where the market surveillance authority considers that non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States of the results of the evaluation and of the actions which it has required the operator to take. 3. Where the market surveillance authority considers that non-compliance is not restricted to its national territory, it shall inform the Board, the Commission and the other Member States of the results of the evaluation and of the actions which it has required the operator to take. Or. en"
    },
    {
        "uuid":"cf382a7b-ee8c-4850-ac00-833815db2e5d",
        "amendment_number":242,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 5",
        "text":"5. Where the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market, to withdraw the product from that market or to recall it. That authority shall inform the Commission and the other Member States, without delay, of those measures. 5. Where the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market or put into service, to withdraw the AI system from that market or to recall it. That authority shall inform the Commission, the Board and the other Member States, without delay, of those measures. Or. en"
    },
    {
        "uuid":"6019dacf-e4cb-4382-a37a-1cf88e30da43",
        "amendment_number":243,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 6 – introductory part",
        "text":"6. The information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator. In particular, the market surveillance authorities shall indicate whether the non-compliance is due to one or more of the following: 6. The information referred to in paragraph 5 shall include all available details, in particular the data necessary for the identification of the non-compliant AI system, the origin of the AI system and the supply chain, the nature of the non- compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator. In particular, the market surveillance authorities shall indicate whether the non- compliance is due to one or more of the following: Or. en"
    },
    {
        "uuid":"9da6a7ca-d886-443d-820a-21ddc124b7c2",
        "amendment_number":244,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 6 – point a",
        "text":"(a) a failure of the AI system to meet requirements set out in Title III, Chapter 2; (a) a failure of the AI system to meet requirements set out in this Regulation; Or. en"
    },
    {
        "uuid":"8773d17e-c931-422d-b864-69edeb0baf53",
        "amendment_number":245,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 7",
        "text":"7. The market surveillance authorities of the Member States other than the market surveillance authority of the Member State initiating the procedure shall without delay inform the Commission and the other 7. The market surveillance authorities of the Member States other than the market surveillance authority of the Member State initiating the procedure shall without delay inform the Commission, the Board and the Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections. other Member States of any measures adopted and of any additional information at their disposal relating to the non- compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections. Or. en"
    },
    {
        "uuid":"c2463b5f-7963-4f55-9245-406ea303e486",
        "amendment_number":246,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 9",
        "text":"9. The market surveillance authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the product concerned, such as withdrawal of the product from their market, without delay. 9. The market surveillance authorities of all Member States shall ensure that appropriate restrictive measures are taken in respect of the AI system concerned, such as withdrawal of the AI system from their market, without delay. Or. en"
    },
    {
        "uuid":"45d44ab3-4efb-41fd-be65-cdb9a6d56896",
        "amendment_number":247,
        "author":"IMCO-LIBE",
        "title":"Article 65 – paragraph 9 a (new)",
        "text":"9a. National supervisory authorities shall annually report to the Board about the possible use of prohibited practices and serious cases of misuse of high-risk AI systems that occurred during that year and about the measures taken to eliminate or mitigate the risks in accordance with this Article. Or. en"
    },
    {
        "uuid":"0b795eb3-bce0-4fa8-857e-215127ef1d4f",
        "amendment_number":248,
        "author":"IMCO-LIBE",
        "title":"Article 66 – paragraph 1",
        "text":"1. Where, within three months of receipt of the notification referred to in Article 65(5), objections are raised by a Member State against a measure taken by another Member State, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the relevant Member State and operator or operators and shall evaluate the national measure. On the basis of the results of that evaluation, the Commission shall decide whether the national measure is justified or not within 9 months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned. 1. Where, within three months of receipt of the notification referred to in Article 65(5), objections are raised by a Member State against a measure taken by another Member State, or where the Commission considers the measure to be contrary to Union law, the Commission shall without delay enter into consultation with the relevant Member State and operator or operators and shall evaluate the national measure. On the basis of the results of that evaluation, the Commission shall decide whether the national measure is justified or not within three months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned. Or. en"
    },
    {
        "uuid":"10d6cd0b-927e-48f1-aa29-08d8cae8a4f6",
        "amendment_number":249,
        "author":"IMCO-LIBE",
        "title":"Article 66 – paragraph 2",
        "text":"2. If the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market, and shall inform the Commission accordingly. If the national measure is considered unjustified, the Member State concerned shall withdraw the measure. 2. If the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market without delay, and shall inform the Commission and the Board accordingly. If the national measure is considered unjustified, the Member State concerned shall withdraw the measure. Or. en"
    },
    {
        "uuid":"26e8c317-fc3c-41af-a85c-c9e15f406c17",
        "amendment_number":250,
        "author":"IMCO-LIBE",
        "title":"Article 67 – paragraph 1",
        "text":"1. Where, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. 1. Where, having performed an evaluation under Article 65, in full cooperation with the relevant national public authority referred to in Article 64(3), the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or the Union values enshrined in Article 2 TEU or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk, to withdraw the AI system from the market or to recall it within a reasonable period, commensurate with the nature of the risk, as it may prescribe. Or. en"
    },
    {
        "uuid":"92ec432d-bc35-4c7e-adde-5f659a547938",
        "amendment_number":251,
        "author":"IMCO-LIBE",
        "title":"Article 67 – paragraph 3",
        "text":"3. The Member State shall immediately inform the Commission and the other Member States. That information shall include all available details, in particular the data necessary for the identification of the AI system concerned, 3. The Member State shall immediately inform the Commission, the Board and the other Member States. That information shall include all available details, in particular the data necessary for the identification of the AI system the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken. concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken. Or. en"
    },
    {
        "uuid":"9f9538d9-5512-4edb-80b3-750512d92358",
        "amendment_number":252,
        "author":"IMCO-LIBE",
        "title":"Article 67 – paragraph 5",
        "text":"5. The Commission shall address its decision to the Member States. 5. The Commission shall address its decision to the Member States. It shall immediately communicate the decision to the Member States and to the relevant operators. Or. en"
    },
    {
        "uuid":"061f80c0-0d15-47d6-8e74-8f439a65724b",
        "amendment_number":253,
        "author":"IMCO-LIBE",
        "title":"Article 68 – paragraph 1 – point a",
        "text":"(a) the conformity marking has been affixed in violation of Article 49; (a) the CE marking has been affixed in violation of Article 49; Or. en"
    },
    {
        "uuid":"7ca279a9-dfc7-4519-aed3-157d546faf65",
        "amendment_number":254,
        "author":"IMCO-LIBE",
        "title":"Article 68 – paragraph 1 – point b",
        "text":"(b) the conformity marking has not been affixed; (b) the CE marking has not been affixed; Or. en"
    },
    {
        "uuid":"01bbfaae-ffcd-4c9d-92de-cef1d670e8fc",
        "amendment_number":255,
        "author":"IMCO-LIBE",
        "title":"Article 68 – paragraph 1 – point e a (new)",
        "text":"(ea) the technical documentation is not available; Or. en"
    },
    {
        "uuid":"8e8c24e4-97c4-4e1c-97b7-11290b3fcf3e",
        "amendment_number":256,
        "author":"IMCO-LIBE",
        "title":"Article 68 – paragraph 1 – point e b (new)",
        "text":"(eb) the registration in the EU database has not been carried out. Or. en"
    },
    {
        "uuid":"b4a3fb90-1ee2-4102-957e-ce1c0063de5c",
        "amendment_number":257,
        "author":"IMCO-LIBE",
        "title":"Article 68 – paragraph 2",
        "text":"2. Where the non-compliance referred to in paragraph 1 persists, the Member State concerned shall take all appropriate measures to restrict or prohibit the high- risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market. 2. Where the non-compliance referred to in paragraph 1 persists, the Member State concerned shall take all appropriate measures to restrict or prohibit the high- risk AI system being made available on the market or ensure that it is recalled or withdrawn from the market without delay. The Member State concerned shall immediately inform the Board of the non- compliance and the measures taken. Or. en"
    },
    {
        "uuid":"e9e0cb4c-26a8-4b00-891b-6402963246d0",
        "amendment_number":258,
        "author":"IMCO-LIBE",
        "title":"Article 68 a (new)",
        "text":"Article 68a Commission's intervention and opening of proceedings 1. The Commission, acting upon the Board’s recommendation or on its own initiative, may initiate proceedings in view of the possible adoption of decisions pursuant to Articles 67e and 71 in any of the following cases: (a) the Commission or the Board have sufficient reasons to believe that an AI system infringes this Regulation in such a way to amount to a widespread infringement or a widespread infringement with a Union dimension; (b) the Commission or the Board have sufficient reasons to believe that an AI system concerned presents a risk which affects or is likely to affect at least 45 million citizens within the Union; (c) an AI system is suspected of having infringed any of the provisions of this Regulation in two or more Member States and the relevant national supervisory authorities of the Member States concerned have not taken any action. 2. Where the Commission decides to initiate proceedings pursuant to paragraph 1, it shall notify the Board and the operators concerned. Pursuant to that notification, the national supervisory authority of the Member State concerned shall no longer be entitled to take any investigatory or enforcement measures against the operator concerned, without prejudice to any other measures that it takes at the request of the Commission. 3. The national supervisory authority, shall, without delay upon being informed, transmit to the Commission: (a) any information that that national supervisory authority exchanged relating to the infringement or the suspected infringement, as applicable, with the Board and with the operator concerned; (b) where applicable, the case file of that national supervisory authority relating to the infringement or the suspected infringement, as applicable; (c) any other information in the possession of that national supervisory authority that may be relevant to the proceedings initiated by the Commission. 4. For the purpose of carrying out its tasks under this Article, the Commission may invite independent external experts and auditors to assist it in its tasks and to provide it with specific expertise or knowledge. Or. en"
    },
    {
        "uuid":"1a654421-64b1-4cd5-ba45-5a782761c23a",
        "amendment_number":260,
        "author":"IMCO-LIBE",
        "title":"Article 68 b (new)",
        "text":"Article 68b Commission's investigation and enforcement power 1. In order to fulfil its tasks under Article 68a, the Commission shall have the following investigation and enforcement powers: (a) to require providers or users of an AI system to provide relevant documents, technical specifications, data and information with regard to the compliance and technical aspects of the AI system, in any form or format and irrespective of the medium of storage or the place where such documents, technical specifications, data or information are stored, and to take or obtain copies thereof; (b) to access data and documentation related to an AI system and its functioning, in accordance with Article 64; (c) to require providers or users of an AI system to provide relevant information, in accordance with Article 68d; (d) to carry out unannounced on-site and remote inspections as well as physical checks in accordance with Article 68f; (e) to conduct interviews in accordance with Article 68e; (f) to start investigations on its own initiative in order to identify non- compliances and bring them to an end; (g) to order providers and users of an AI system to take appropriate action to bring an instance of non-compliance to an end or to eliminate the risk in accordance with Article 68h and to adopt interim measures in that regard, in accordance with Article 68i; (h) to take appropriate measures in accordance with Article 68h; (i) to impose penalties in accordance with Article 71. 2. In order to fulfil its tasks under paragraph 1, the Commission may use any information, document, finding, statement or intelligence as evidence for the purpose of their investigations, irrespective of the format in which and medium on which they are stored. 3. In fulfilling its task, the Commission shall take into account the procedural rights of the concerned operator in accordance with Article 18 of Regulation (EU) 2019\/1020. Or. en"
    },
    {
        "uuid":"097eb0b7-6e83-44d3-90db-e4ae73fdf529",
        "amendment_number":261,
        "author":"IMCO-LIBE",
        "title":"Article 68 c (new)",
        "text":"Article 68c Cooperation and information exchange between the Commission and the national competent authorities 1. The Commission and the national supervisory authority shall work in close cooperation. 2. The Commission shall act in close and constant cooperation with the national competent authorities and the national supervisory authority of the Member States from which it obtains comments and information 3. The Commission and the national supervisory authority of the Member States shall have the power to provide one another with and use in evidence any matter of fact or of law, including confidential information. In doing so, they shall respect the confidentiality of information in accordance with Article 70. Or. en"
    },
    {
        "uuid":"7aa1643d-74e5-4f65-b082-03ba19d84dae",
        "amendment_number":262,
        "author":"IMCO-LIBE",
        "title":"Article 68 d (new)",
        "text":"Article 68d Commission’s power to request information 1. In order to carry out the duties assigned to it under this Chapter, the Commission may require an operator concerned that is aware or can be assumed to be aware of information relating to the suspected infringement or the infringement, as applicable, including organisations performing the conformity assessment referred to in Article 43, to provide such information within a reasonable time period of no more than 15 days. 2. When sending a simple request for information to the operator concerned, the Commission shall state the legal basis and the purpose of the request, specify what information is required and set the time period within which the information isto be provided, and the penalties provided for in Article 71 for not supplying information or supplying incorrect or misleading information. 3. Where the Commission adopts a decision requiring the operator concerned to supply information, it shall state the legal basis and the purpose of the request, specify what information is required and set the time limit within which such information is to be provided. It shall also indicate the penalties provided for in Article 71. It shall further indicate the right to have the decision reviewed by the Court of Justice of the European Union. 4. The operator concerned and the persons authorised to represent them by law shall supply the information requested. 5. The Commission shall, without delay, forward a copy of the simple request or of the decision to the national supervisory authority of the Member State in which the operator concerned has its main establishment or legal representative. Or. en"
    },
    {
        "uuid":"33a47e2a-8c3d-43e4-a049-8855f84f33cc",
        "amendment_number":263,
        "author":"IMCO-LIBE",
        "title":"Article 68 e (new)",
        "text":"Article 68e Commission's power to take interviews and statements In order to carry out the tasks assigned to it under Article 68a, the Commission may, subject to their consent, interview any natural or legal person for the purpose of collecting information, relating to the subject-matter of an investigation, in relation to the suspected infringement or infringement, as applicable. Or. en"
    },
    {
        "uuid":"b83c740f-6d50-4de4-bc68-5582f51738f1",
        "amendment_number":264,
        "author":"IMCO-LIBE",
        "title":"Article 68 f (new)",
        "text":"Article 68f Commission’s power of inspection 1. In order to carry out the duties assigned to it by this Regulation, the Commission may conduct all necessary inspections. In particular, the Commission may acquire samples related to AI systems, including through remote inspections, reverse engineer the AI systems, access and test the datasets and algorithms used for and by the AI system and request access the source code if needed. 2. The officials and other accompanying persons authorised by the Commission to conduct an inspection shall be empowered to: (a) enter any premises that the provider or user uses for purposes related to an AI system; (b) examine any document or record related to an AI system irrespective of the medium on which such a document or record is stored; (c) take or obtain in any form copies of or extracts from such a document or record; (d) ask any representative of the provider or user for explanations on facts or document relating to the subject matter and purpose of the inspection and to record the answers. 3. The officials and other accompanying persons authorised by the Commission to conduct an inspection shall exercise their powers upon production of a written authorisation specifying the subject matter and purpose of the inspection and the penalties provided for in Article 71 where the production of the required documents is incomplete or where the answers to questions asked are incorrect or misleading. 4. Providers and users of AI systems shall submit to any inspection ordered by a Commission decision. 5. The Commission shall inform the national supervisory authority or authorities of the Member State or Member States concerned. Or. en"
    },
    {
        "uuid":"fbcdf8bc-1294-4613-b84d-7972afc08fcf",
        "amendment_number":265,
        "author":"IMCO-LIBE",
        "title":"Article 68 g (new)",
        "text":"Article 68g Non-compliance 1. The Commission shall adopt anon- compliance decision where it finds that the operator concerned does not comply with one or more of the relevant provisions of this Regulation. 2. Before adopting a decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings to the operator concerned. In the preliminary findings, the Commission shall explain the measures that it considers taking, or that it considers that the operator concerned should take, in order to effectively address the preliminary findings. 3. In a decision adopted pursuant to paragraph 1 the Commission shall order the operator concerned to take the necessary measures to ensure compliance with that decision within a reasonable time period and to provide information on the measures that the operator intends to take to comply with the decision. 4. Upon the implementation of a decision pursuant to paragraph 1, the operator concerned shall provide the Commission with a description of the measures that it has taken to ensure compliance with that decision. 5. Where the operator fails to take the measures referred to in paragraph 3 or where the non-compliance or the risk persists, the Commission shall take appropriate measures, including prohibiting or restricting the placing on the market, the putting into service or the use of the AI system concerned. 6. Where the Commission finds that the conditions of paragraph 1 are not met, it shall adopt a decision closing the investigation. Or. en"
    },
    {
        "uuid":"cf7b9a83-d779-49c8-aedd-80f17f48423b",
        "amendment_number":266,
        "author":"IMCO-LIBE",
        "title":"Article 68 h (new)",
        "text":"Article 68h Interim measures 1. In the context of proceedings which may lead to the adoption of a decision of non-compliance pursuant to Article 68h,where there is an urgency due to the risk of serious harm to individuals, the Commission may adopt a decision ordering interim measures to be imposed on the operator concerned on the basis of the prima facie finding of an infringement. 2. A Commission decision under paragraph 1 shall apply for a specified period of time and may be renewed where this is necessary and appropriate. Or. en"
    },
    {
        "uuid":"1bc2c0e8-fac1-480c-aec8-263dac06c56b",
        "amendment_number":267,
        "author":"IMCO-LIBE",
        "title":"Article 68 i (new)",
        "text":"Article 68i Publication of decisions 1. The Commission shall publish the decisions it adopts pursuant to Articles 68h and 68i, stating the names of the addressees and the main content of the decisions, including any penalties imposed pursuant to Article 71. 2. The publication shall have regard to the rights and legitimate interests of the operator concerned. Or. en"
    },
    {
        "uuid":"a4e66e81-3b27-470a-9c94-57ccb0f1ce83",
        "amendment_number":268,
        "author":"IMCO-LIBE",
        "title":"Article 68 j (new)",
        "text":"Article 68j Right to lodge a complaint 1. Natural persons or groups of natural persons affected by an AI system falling within the scope of this Regulation shall have the right to lodge a complaint against the providers or users of such AI system with the national supervisory authority of the Member State where they have their habitual place of residence or place of work or where the alleged infringement took place, if they consider that their health, safety, or fundamental rights have been breached. 2. Natural persons or groups of natural persons shall have a right to be heard in the complaint handling procedure and in the context of any investigations conducted by the national supervisory authority as a result of their complaint. 3. The national supervisory authority with which the complaint has been lodged shall inform the complainants about the progress and outcome of their complaint. In particular, the national supervisory authority shall take all the necessary actions to follow up on the complaints it receives and, within three months of the reception of a complaint, give the complainant a preliminary response indicating the measures it intends to take and the next steps in the procedure, if any. 4. The national supervisory authority shall take a decision on the complaint, including the possibility of a judicial remedy pursuant to Article 68k, without delay and no later than six months after the date on which the complaint was lodged. Or. en"
    },
    {
        "uuid":"31f6bb5a-f630-4b0b-89c7-3b8ea341ede9",
        "amendment_number":270,
        "author":"IMCO-LIBE",
        "title":"Article 68 k (new)",
        "text":"Article 68k Right to an effective judicial remedy against a national supervisory authority 1. Without prejudice to any other administrative or non-judicial remedy, natural persons and legal persons or groups of natural or legal persons shall have the right to an effective judicial remedy against a legally binding decision of a national supervisory authority concerning them. 2. Without prejudice to any other administrative or non-judicial remedy, natural persons and legal persons and groups of natural or legal persons shall have the right to an effective judicial remedy where the national supervisory authority does not handle a complaint, does not inform the complainant on the progress or preliminary outcome of the complaint lodged within three months pursuant to Article 68a(3) or does not comply with its obligation to reach a final decision on the complaint within six months pursuant to Article 68a(4) or its obligations under Article 65. 3. Proceedings against a national supervisory authority shall be brought before the courts of the Member State where that authority is established. Or. en"
    },
    {
        "uuid":"4d231904-6648-4529-a23b-577afa78f165",
        "amendment_number":271,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 1 – point b",
        "text":"(b) the effective implementation of this Regulation, in particular for the purpose of inspections, investigations or audits;(c) public and national security interests; (b) the effective implementation of this Regulation, in particular for the purpose of inspections, investigations or audits; Or. en"
    },
    {
        "uuid":"e7e36a1e-d2d9-4025-92a6-b8e5b9612184",
        "amendment_number":272,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 1 – point b a (new)",
        "text":"(ba) public and national security interests; Or. en"
    },
    {
        "uuid":"1ae3823a-bde4-48fe-b6f8-b7e1f4147a59",
        "amendment_number":273,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 1 a (new)",
        "text":"1a. The Commission, the Board, national competent authorities and notified bodies involved in the application of this Regulation shall put in place adequate cybersecurity and organisational measures to protect the security and confidentiality of the information and data obtained in carrying out their tasks and activities. Or. en"
    },
    {
        "uuid":"fff4c74b-cd4a-4785-85fe-64e9b3e974c4",
        "amendment_number":274,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 2 – subparagraph 1",
        "text":"Without prejudice to paragraph 1, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high-risk AI systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, immigration or asylum authorities, when such disclosure would jeopardise public and national security interests. Without prejudice to paragraphs 1 and 1a, information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the Commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high-risk AI systems referred to in points 1, 6 and 7 of Annex III are used by law enforcement, immigration or asylum authorities, when such disclosure would jeopardise public and national security interests. Or. en"
    },
    {
        "uuid":"d1561c6a-819b-459b-b49b-ecfd27243d63",
        "amendment_number":275,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 3",
        "text":"3. Paragraphs 1 and 2 shall not affect the rights and obligations of the Commission, Member States and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under criminal law of the Member States. 3. Paragraphs 1, 1a and 2 shall not affect the rights and obligations of the Commission, Member States and notified bodies with regard to the exchange of information and the dissemination of warnings, nor the obligations of the parties concerned to provide information under criminal law of the Member States. Or. en"
    },
    {
        "uuid":"d47e5d67-8b75-42bf-9a3c-60669547898e",
        "amendment_number":276,
        "author":"IMCO-LIBE",
        "title":"Article 70 – paragraph 4",
        "text":"4. The Commission and Member States may exchange, where necessary, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality. 4. The Commission and Member States may exchange, where strictly necessary, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality. Or. en"
    },
    {
        "uuid":"64475249-4011-4dc5-86ac-9656f0ab56fe",
        "amendment_number":277,
        "author":"IMCO-LIBE",
        "title":"Article 71 – title",
        "text":"Penalties Penalties and fines Or. en"
    },
    {
        "uuid":"6803e73a-34b2-4d57-8b5c-ef9611a41ef3",
        "amendment_number":278,
        "author":"IMCO-LIBE",
        "title":"Article 71 – paragraph 2",
        "text":"2. The Member States shall notify the Commission of those rules and of those measures and shall notify it, without delay, of any subsequent amendment affecting them. 2. The Member States shall notify the Commission and the Board of those rules and of those measures and shall notify it, without delay, of any subsequent amendment affecting them. Or. en"
    },
    {
        "uuid":"4e3bfc26-3aba-44d5-aaf4-89469af0b437",
        "amendment_number":279,
        "author":"IMCO-LIBE",
        "title":"Article 71 – paragraph 8 a (new)",
        "text":"8a. In accordance with Chapter 4 of Title VIII, the Commission may adopt a decision imposing fines pursuant to paragraphs 3 to 6 on providers and users of high-risk AI systems. Or. en"
    },
    {
        "uuid":"fa3c50f4-aca4-440c-9759-ab01605174aa",
        "amendment_number":280,
        "author":"IMCO-LIBE",
        "title":"Article 71 – paragraph 8 b (new)",
        "text":"8b. In addition to paragraph 8a, the Commission may adopt a decision imposing on the operator concerned fines not exceeding 2 % of the total turnover in the preceding financial year, where the operator intentionally or negligently: (a) fails to provide information to the Commission by the deadline set in a Commission decision; (b) fails to rectify by the deadline set in a Commission decision, incorrect, incomplete or misleading information given by a member of staff, or fails or refuses to provide complete information; (c) refuses to submit to a remote or on-site inspection pursuant to Article 68f. Or. en"
    },
    {
        "uuid":"9f9e5413-722b-45e2-acc9-24541eade4c0",
        "amendment_number":281,
        "author":"IMCO-LIBE",
        "title":"Article 71 – paragraph 8 c (new)",
        "text":"8c. The Commission and national supervisory authorities shall, on an annual basis, report to the Board about the fines they have issued during that year, in accordance with this Article. Or. en"
    },
    {
        "uuid":"45e4705d-c853-4b91-9d25-e73d66b94cd3",
        "amendment_number":282,
        "author":"IMCO-LIBE",
        "title":"Article 72 – paragraph 6",
        "text":"6. Funds collected by imposition of fines in this Article shall be the income of the general budget of the Union. 6. Funds collected by imposition of fines in this Article shall contribute to the general budget of the Union. Or. en"
    },
    {
        "uuid":"f3320f30-99c2-4ae5-b55f-1d22f286f6a2",
        "amendment_number":283,
        "author":"IMCO-LIBE",
        "title":"Article 72 – paragraph 6 a (new)",
        "text":"6a. The European Data Protection Supervisor shall, on an annual basis, notify the Board of the fines it has imposed pursuant to this Article. Or. en"
    },
    {
        "uuid":"13b2165d-1c7d-4dde-a3ec-d040191fdbeb",
        "amendment_number":284,
        "author":"IMCO-LIBE",
        "title":"Article 84 – paragraph 1",
        "text":"1. The Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation. 1. The Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation and on a regular basis following a recommendation of the Board. Or. en"
    },
    {
        "uuid":"37ae8691-472e-4188-8303-7df25e87fb8d",
        "amendment_number":285,
        "author":"IMCO-LIBE",
        "title":"Article 84 – paragraph 2",
        "text":"2. By [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The reports shall be made public. 2. By [two years after the date of application of this Regulation referred to in Article 85(2)] and every two years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council. The reports shall be made public. Or. en"
    },
    {
        "uuid":"3879b417-d9b0-4e9d-a220-a23b3e1c4070",
        "amendment_number":286,
        "author":"IMCO-LIBE",
        "title":"Article 84 – paragraph 6",
        "text":"6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the Board, of the 6. In carrying out the evaluations and reviews referred to in paragraphs 1 to 4 the Commission shall take into account the positions and findings of the Board, of the European Parliament, of the Council, and of other relevant bodies or sources. European Parliament, of the Council, of equality bodies and of other relevant bodies or sources and shall consult relevant external stakeholders, in particular those potentially affected by the AI systems, organisations representing their interests, academia, the social partners and civil society. Or. en"
    },
    {
        "uuid":"9b83ddd8-ac17-4cc5-b3ce-31f6a5bba133",
        "amendment_number":287,
        "author":"IMCO-LIBE",
        "title":"Annex III – paragraph 1 – point 8 – point a a (new)",
        "text":"(aa) AI systems intended to be used by political parties, political candidates, public authorities, or on their behalf for influencing natural persons in the exercise of their vote in local, national, or European Parliament elections; Or. en"
    },
    {
        "uuid":"da70fdf1-faa6-4b34-b78b-7a3875f7d33a",
        "amendment_number":296,
        "author":"IMCO-LIBE",
        "title":"Annex IV – paragraph 1 – point 1",
        "text":"(ga) a description of how the AI system works and examples of representative use cases for which the AI system is intended; Or. en"
    }
]